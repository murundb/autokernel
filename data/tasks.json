{
  "tasks": [
    {
      "kernel_name": "simpleMLP",
      "task": "Write {gpu_software} kernel that implements a simple MLP: a linear layer (matrix-vector multiplication), followed by ReLU activation, followed by average pooling. The input is a vector of size 2048, the weight matrix is 2048x2048, and the output is a single float (the average of the ReLU outputs). The signature of the kernel should strictly be __kernel void simpleMLP(__global const float* restrict input, __global const float* restrict weights, __global float* restrict result, const int dim).",
      "input_args": [
        {"name": "input", "type": "matrix", "shape": [2048], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "weights", "type": "matrix", "shape": [2048, 2048], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "result", "type": "float", "dtype": "float32", "init": "zero", "role": "output"},
        {"name": "matrix_dim", "type": "int", "value": 2048, "role": "input"}
      ],
      "verification": {
        "type": "scalar_equals",
        "expected_value": 2048,
        "tolerance": 1e-5
      },
      "disabled" :false
    },
    {
      "kernel_name": "matrixMultiply",
      "task": "Write {gpu_software} kernel that performs 4096x4096 matrix multiplication optimized for {gpu_manufacturer} {gpu_hardware} architecture. The signature of the kernel should be __kernel void matrixMultiply(__global const float* restrict A, __global const float* restrict B, __global float* restrict Result, const int matrix_dim).",
      "input_args": [
        {"name": "A", "type": "matrix", "shape": [4096, 4096], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "B", "type": "matrix", "shape": [4096, 4096], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "Result", "type": "matrix", "shape": [4096, 4096], "dtype": "float32", "init": "zeros", "role": "output"},
        {"name": "matrix_dim", "type": "int", "value": 4096, "role": "input"}
      ],
      "verification": {
        "type": "matrix_equals",
        "expected_value": 4096,
        "sample_indices": [[0,0], [0,1], [1,0], [2048,2048], [4095,4095]],
        "tolerance": 1e-5
      },
      "disabled" :false
    },
    {
      "kernel_name": "matrixReduce",
      "task": "Write {gpu_software} kernel that reduces an 2048x2048 matrix to a single float by summing all its elements, optimized for {gpu_manufacturer} {gpu_hardware} architecture. The signature of the kernel should be __kernel void matrixReduce(__global const float* restrict A, __global float* restrict result, const int matrix_dim).",
      "input_args": [
        {"name": "A", "type": "matrix", "shape": [2048, 2048], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "result", "type": "float", "dtype": "float32", "init": "zero", "role": "output"},
        {"name": "matrix_dim", "type": "int", "value": 2048, "role": "input"}
      ],
      "verification": {
        "type": "scalar_equals",
        "expected_value": 4194304,
        "tolerance": 1e-5
      },
      "disabled" :false
    },
    {
      "kernel_name": "residualBlock",
      "task": "Write {gpu_software} kernel that implements a residual block as used in DNNs: given an input vector x of size 1024, two weight matrices W1 and W2 of size 1024x1024, and two bias vectors b1 and b2 of size 1024, compute result = ReLU(W2 * ReLU(W1 * x + b1) + b2 + x). The signature of the kernel should be __kernel void residualBlock(__global const float* restrict x, __global const float* restrict W1, __global const float* restrict b1, __global const float* restrict W2, __global const float* restrict b2, __global float* restrict result, const int dim).",
      "input_args": [
        {"name": "x", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "W1", "type": "matrix", "shape": [1024, 1024], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "b1", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "zeros", "role": "input"},
        {"name": "W2", "type": "matrix", "shape": [1024, 1024], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "b2", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "zeros", "role": "input"},
        {"name": "result", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "zeros", "role": "output"},
        {"name": "dim", "type": "int", "value": 1024, "role": "input"}
      ],
      "verification": {
        "type": "vector_equals",
        "expected_value": [1048577, 1048577, 1048577, 1048577],
        "sample_indices": [0, 1, 512, 1023],
        "tolerance": 1e-5
      },
      "disabled": false
    },
    {
      "kernel_name": "CrossKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "#define TORCH_ASSERT_ONLY_METHOD_OPERATORS\n#include <ATen/native/Cross.h>\n#include <ATen/cuda/detail/KernelUtils.h>\n#include <ATen/native/cuda/Loops.cuh>\n#include <ATen/Dispatch.h>\n#include <ATen/core/Tensor.h>\nnamespace at::native {\ntemplate <typename T, typename OffsetCalc, typename StrideType>\n__global__ void cross_kernel(\n    int numel, T* out, const T* x1, const T* x2, OffsetCalc offset_calculator,\n    StrideType ostride, StrideType x1stride, StrideType x2stride) {\n  CUDA_KERNEL_LOOP(i, numel) {\n    const auto offsets = offset_calculator.get(i);\n    auto* out_row = out + offsets[0];\n    const auto* x1_row = x1 + offsets[1];\n    const auto* x2_row = x2 + offsets[2];\n    const T val0 = (x1_row[1 * x1stride] * x2_row[2 * x2stride] -\n                    x1_row[2 * x1stride] * x2_row[1 * x2stride]);\n    const T val1 = (x1_row[2 * x1stride] * x2_row[0 * x2stride] -\n                    x1_row[0 * x1stride] * x2_row[2 * x2stride]);\n    const T val2 = (x1_row[0 * x1stride] * x2_row[1 * x2stride] -\n                    x1_row[1 * x1stride] * x2_row[0 * x2stride]);\n    out_row[0 * ostride] = val0;\n    out_row[1 * ostride] = val1;\n    out_row[2 * ostride] = val2;\n  }\n}\nvoid launch_cross_kernel(const TensorIteratorBase& iter, int64_t ostride,\n                        int64_t x1stride, int64_t x2stride) {\n  const auto N = iter.numel();\n  auto offset_calculator = make_element_offset_calculator<3>(iter);\n  TORCH_INTERNAL_ASSERT_DEBUG_ONLY(N > 0 && N <= std::numeric_limits<int32_t>::max());\n  int64_t grid = (N + num_threads() - 1) / num_threads();\n  auto stream = at::cuda::getCurrentCUDAStream();\n  AT_DISPATCH_ALL_TYPES_AND_COMPLEX_AND2(kHalf, kBFloat16, iter.common_dtype(), \"cross_cuda\", [&] {\n    auto out = static_cast<scalar_t*>(iter.data_ptr(0));\n    auto x1 = static_cast<const scalar_t*>(iter.data_ptr(1));\n    auto x2 = static_cast<const scalar_t*>(iter.data_ptr(2));\n    constexpr int64_t int_max = std::numeric_limits<int>::max();\n    if (ostride * 2 > int_max || x1stride * 2 > int_max || x2stride * 2 > int_max) {\n      cross_kernel<<<grid, num_threads(), 0, stream>>>(\n          N, out, x1, x2, offset_calculator, ostride, x1stride, x2stride);\n      C10_CUDA_KERNEL_LAUNCH_CHECK();\n    } else {\n      cross_kernel<<<grid, num_threads(), 0, stream>>>(\n          N, out, x1, x2, offset_calculator,\n          static_cast<int>(ostride),\n          static_cast<int>(x1stride),\n          static_cast<int>(x2stride));\n      C10_CUDA_KERNEL_LAUNCH_CHECK();\n    }\n  });\n}\nvoid cross_impl(const Tensor& result, const Tensor& x1, const Tensor& x2, int64_t dim) {\n  const int64_t ostride = result.stride(dim);\n  const int64_t x1stride = x1.stride(dim);\n  const int64_t x2stride = x2.stride(dim);\n  auto iter = TensorIteratorConfig()\n      .add_output(result)\n      .add_const_input(x1)\n      .add_const_input(x2)\n      .resize_outputs(false)\n      .declare_static_shape(result.sizes(), /*squash_dims=*/dim)\n      .build();\n  if (iter.numel() == 0) {\n    return;\n  }\n  if (iter.can_use_32bit_indexing()) {\n    launch_cross_kernel(iter, ostride, x1stride, x2stride);\n  } else {\n    for (auto&& sub_iter: iter.with_32bit_indexing()) {\n      launch_cross_kernel(sub_iter, ostride, x1stride, x2stride);\n    }\n  }\n}\nREGISTER_DISPATCH(cross_stub, &cross_impl)\n} // namespace at::native",
      "input_args": [
        {"name": "x", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "W1", "type": "matrix", "shape": [1024, 1024], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "b1", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "zeros", "role": "input"},
        {"name": "W2", "type": "matrix", "shape": [1024, 1024], "dtype": "float32", "init": "ones", "role": "input"},
        {"name": "b2", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "zeros", "role": "input"},
        {"name": "result", "type": "matrix", "shape": [1024], "dtype": "float32", "init": "zeros", "role": "output"},
        {"name": "dim", "type": "int", "value": 1024, "role": "input"}
      ],
      "verification": {
        "type": "vector_equals",
        "expected_value": [1048577, 1048577, 1048577, 1048577],
        "sample_indices": [0, 1, 512, 1023],
        "tolerance": 1e-5
      },
      "disabled": false
    },
    {
      "kernel_name": "_unfold_backward_elementwise_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _unfold_backward_elementwise_kernel(int total_n_elems, func_t f) {\n  constexpr int total_work_block = n_threads * n_elems_per_thread;\n  int idx = total_work_block * blockIdx.x + threadIdx.x;\n\n  #pragma unroll\n  for (int i = 0; i < n_elems_per_thread; ++i) {\n    if (idx < total_n_elems) {\n      f(idx);\n      idx += n_threads;\n    }\n  }\n}",
      "disabled": false
    },
    {
      "kernel_name": "conv_depthwise3d_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void conv_depthwise3d_cuda_kernel(\n    const PackedTensorAccessor32<const scalar_t, 5> input,\n    PackedTensorAccessor32<scalar_t, 5> output,\n    const PackedTensorAccessor32<const scalar_t, 5> kernel,\n    const scalar_t* bias,\n    int strideT, int strideH, int strideW,\n    int paddingT, int paddingH, int paddingW,\n    int dilationT_, int dilationH_, int dilationW_)\n{\n  const int kT = kKnownKernelT > 0 ? kKnownKernelT : kernel.size(2);\n  const int kH = kKnownKernelH > 0 ? kKnownKernelH : kernel.size(3);\n  const int kW = kKnownKernelW > 0 ? kKnownKernelW : kernel.size(4);\n  const int oC = output.size(1);\n  const int oT = output.size(2);\n  const int oH = output.size(3);\n  const int oW = output.size(4);\n  const int iC = input.size(1);\n  const int iT = input.size(2);\n  const int iH = input.size(3);\n  const int iW = input.size(4);\n  const int channel_multiplier = oC / iC;\n  const int dilationT = kKnownDilationT > 0 ? kKnownDilationT : dilationT_;\n  const int dilationH = kKnownDilationH > 0 ? kKnownDilationH : dilationH_;\n  const int dilationW = kKnownDilationW > 0 ? kKnownDilationW : dilationW_;\n  const int num_output = output.size(0) * output.stride(0);\n\n  CUDA_KERNEL_LOOP(index, num_output) {\n    const int out_col = index % oW;\n    const int out_row = (index / oW) % oH;\n    const int out_frame = (index / oW / oH) % oT;\n    const int out_channel = (index / oW / oH / oT) % oC;\n    const int batch = index / oW / oH / oT / oC;\n\n    const int in_channel = out_channel / channel_multiplier;\n\n    const int in_col_start = out_col * strideW - paddingW;\n    const int in_row_start = out_row * strideH - paddingH;\n    const int in_frame_start = out_frame * strideT - paddingT;\n\n    accscalar_t sum = 0;\n    const scalar_t *kernel_ptr = kernel[out_channel].data();\n    const scalar_t *input_ptr =\n        &input[batch][in_channel][in_frame_start][in_row_start][in_col_start];\n    for (int k_frame = 0; k_frame < kT; ++k_frame) {\n      const int in_frame = in_frame_start + k_frame * dilationT;\n      for (int k_row = 0; k_row < kH; ++k_row) {\n        const int in_row = in_row_start + k_row * dilationH;\n        for (int k_col = 0; k_col < kW; ++k_col) {\n          const accscalar_t op1 = *(kernel_ptr++);\n          const int in_col = in_col_start + k_col * dilationW;\n          if (in_frame >= 0 && in_row >= 0 && in_col >= 0 &&\n              in_frame < iT && in_row < iH && in_col < iW) {\n            sum += op1 * *(input_ptr);\n          }\n          input_ptr += dilationW;\n        }\n        input_ptr += iW * dilationH - kW * dilationW;\n      }\n      input_ptr += iW * (iH * dilationT - kH * dilationH);\n    }\n    if (bias != NULL) {\n      sum += bias[out_channel];\n    }\n\n    output[batch][out_channel][out_frame][out_row][out_col] = sum;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "conv_depthwise3d_cuda_backward_input_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\nconv_depthwise3d_cuda_backward_input_kernel(\n    const PackedTensorAccessor32<const scalar_t, 5> grad_output,\n    PackedTensorAccessor32<scalar_t, 5> grad_input,\n    const PackedTensorAccessor32<const scalar_t, 5> kernel,\n    int strideT_, int strideH_, int strideW_,\n    int paddingT, int paddingH, int paddingW,\n    int dilationT_, int dilationH_, int dilationW_) {\n  const int kT = kKnownKernelT > 0 ? kKnownKernelT : kernel.size(2);\n  const int kH = kKnownKernelH > 0 ? kKnownKernelH : kernel.size(3);\n  const int kW = kKnownKernelW > 0 ? kKnownKernelW : kernel.size(4);\n  const int oC = grad_output.size(1);\n  const int oT = grad_output.size(2);\n  const int oH = grad_output.size(3);\n  const int oW = grad_output.size(4);\n  const int iC = grad_input.size(1);\n  const int iT = grad_input.size(2);\n  const int iH = grad_input.size(3);\n  const int iW = grad_input.size(4);\n  const int channel_multiplier = oC / iC;\n  const int dilationT = kKnownDilationT > 0 ? kKnownDilationT : dilationT_;\n  const int dilationH = kKnownDilationH > 0 ? kKnownDilationH : dilationH_;\n  const int dilationW = kKnownDilationW > 0 ? kKnownDilationW : dilationW_;\n  const int strideT = kKnownStrideT > 0 ? kKnownStrideT : strideT_;\n  const int strideH = kKnownStrideH > 0 ? kKnownStrideH : strideH_;\n  const int strideW = kKnownStrideW > 0 ? kKnownStrideW : strideW_;\n  const int num_input = grad_input.size(0) * grad_input.stride(0);\n\n  CUDA_KERNEL_LOOP(index, num_input) {\n    const int in_col = index % iW;\n    const int in_row = (index / iW) % iH;\n    const int in_frame = (index / iW / iH) % iT;\n    const int in_channel = (index / iW / iH / iT) % iC;\n    const int batch = index / iW / iH / iT / iC;\n\n    const int out_col_end = in_col + paddingW;\n    const int out_row_end = in_row + paddingH;\n    const int out_frame_end = in_frame + paddingT;\n\n    const scalar_t* kernel_ptr = kernel[in_channel * channel_multiplier].data();\n    accscalar_t sum = 0;\n\n    for (int k_chn = in_channel * channel_multiplier;\n         k_chn < (in_channel + 1) * channel_multiplier;\n         ++k_chn) {\n      const scalar_t* gout_ptr = grad_output[batch][k_chn].data();\n\n      for (int k_frame = 0; k_frame < kT; ++k_frame) {\n        const int out_frame_raw = out_frame_end - k_frame * dilationT;\n        const int out_frame = out_frame_raw / strideT;\n        for (int k_row = 0; k_row < kH; ++k_row) {\n          const int out_row_raw = out_row_end - k_row * dilationH;\n          const int out_row = out_row_raw / strideH;\n          for (int k_col = 0; k_col < kW; ++k_col) {\n            const accscalar_t op1 = *(kernel_ptr++);\n            const int out_col_raw = out_col_end - k_col * dilationW;\n            const int out_col = out_col_raw / strideW;\n\n            const int out_offs = (out_frame * oH + out_row) * oW + out_col;\n\n            accscalar_t op2 = (accscalar_t)0;\n            if (out_col >= 0 && out_row >= 0 && out_frame >= 0 &&\n                out_col < oW && out_row < oH && out_frame < oT) {\n              op2 = *(gout_ptr + out_offs);\n            }\n            if (out_frame * strideT == out_frame_raw &&\n                out_row * strideH == out_row_raw &&\n                out_col * strideW == out_col_raw) {\n              sum += op1 * op2;\n            }\n          }\n        }\n      }\n    }\n\n    grad_input[batch][in_channel][in_frame][in_row][in_col] = sum;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "conv_depthwise3d_cuda_backward_weight_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\nconv_depthwise3d_cuda_backward_weight_kernel(\n    const PackedTensorAccessor32<const scalar_t, 5> grad_output,\n    const PackedTensorAccessor32<const scalar_t, 5> input,\n    PackedTensorAccessor32<scalar_t, 5> grad_kernel,\n    int strideT, int strideH_, int strideW_,\n    int paddingT, int paddingH, int paddingW,\n    int dilationT, int dilationH, int dilationW) {\n  const int kC = grad_kernel.size(0);\n  const int kT = grad_kernel.size(2);\n  const int kH = grad_kernel.size(3);\n  const int kW = grad_kernel.size(4);\n\n  const int strideH = kKnownStrideH > 0 ? kKnownStrideH : strideH_;\n  const int strideW = kKnownStrideW > 0 ? kKnownStrideW : strideW_;\n\n  const int k_col = blockIdx.x % kW;\n  const int k_row = (blockIdx.x / kW) % kH;\n  const int k_frame = (blockIdx.x / kW / kH) % kT;\n  const int k_channel = blockIdx.x / kW / kH / kT;\n  scalar_t *result = &grad_kernel[k_channel][0][k_frame][k_row][k_col];\n\n  const int oT = grad_output.size(2);\n  const int oH = grad_output.size(3);\n  const int oW = grad_output.size(4);\n  const int iT = input.size(2);\n  const int iH = input.size(3);\n  const int iW = input.size(4);\n  const int channel_multiplier = grad_output.size(1) / input.size(1);\n  const int in_channel = k_channel / channel_multiplier;\n\n  extern __shared__ int sdata_raw[];\n  scalar_t* sdata = reinterpret_cast<scalar_t*>(sdata_raw);\n\n  if (k_channel >= kC) {\n    return;\n  }\n\n  const int laneid = threadIdx.x % C10_WARP_SIZE;\n  const int warpid = threadIdx.x / C10_WARP_SIZE;\n  const int nwarps = blockDim.x / C10_WARP_SIZE;\n\n  accscalar_t grad = 0;\n  int batch = warpid / oT;\n  int gout_frame = warpid - batch * oT;\n  for (int outer_pos = warpid; outer_pos < input.size(0) * oT;\n       outer_pos += nwarps, gout_frame += nwarps) {\n    while (gout_frame >= oT) { gout_frame -= oT; batch ++; }\n\n    const int in_frame = (gout_frame * strideT) + (k_frame * dilationT) - paddingT;\n\n    if (in_frame < 0 || in_frame >= iT) {\n      continue;\n    }\n\n    const scalar_t* gout_ptr = grad_output[batch][k_channel][gout_frame].data() + laneid;\n    const scalar_t* input_ptr = input[batch][in_channel][in_frame].data();\n\n    int gout_row = laneid / oW;\n    int gout_col = laneid - gout_row * oW;\n\n    for (; gout_row < oH; ) {\n      const accscalar_t op1 = *(gout_ptr);\n      gout_ptr += C10_WARP_SIZE;\n\n      const int in_col = (gout_col * strideW) + (k_col * dilationW) - paddingW;\n      const int in_row = (gout_row * strideH) + (k_row * dilationH) - paddingH;\n      const int in_pos = in_row * iW + in_col;\n\n      accscalar_t op2 = (accscalar_t)0;\n      if (in_col >= 0 && in_col < iW && in_row >= 0 && in_row < iH) {\n        op2 = *(input_ptr + in_pos);\n      }\n\n      gout_col += C10_WARP_SIZE;\n      while (gout_col >= oW) {\n        gout_col -= oW; gout_row ++;\n      }\n\n      grad += op1 * op2;\n    }\n  }\n\n  sdata[threadIdx.x] = grad;\n  __syncthreads();\n\n  CUDA_KERNEL_ASSERT(__popc(blockDim.x) == 1);\n#pragma unroll\n  for (int i = blockDim.x / 2; i >= 1; i >>= 1) {\n    if (threadIdx.x < i) {\n      sdata[threadIdx.x] += sdata[threadIdx.x + i];\n    }\n    __syncthreads();\n  }\n\n  if (threadIdx.x == 0) {\n    *result = sdata[0];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "grid_sampler_2d_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void grid_sampler_2d_kernel(\n      const index_t nthreads,\n      TensorInfo<const scalar_t, index_t> input,\n      TensorInfo<const scalar_t, index_t> grid,\n      TensorInfo<scalar_t, index_t> output,\n      const GridSamplerInterpolation interpolation_mode,\n      const GridSamplerPadding padding_mode,\n      bool align_corners) {\n\n    using opmath_t = at::opmath_type<scalar_t>;\n    index_t C = input.sizes[1];\n    index_t inp_H = input.sizes[2];\n    index_t inp_W = input.sizes[3];\n    index_t out_H = grid.sizes[1];\n    index_t out_W = grid.sizes[2];\n    index_t inp_sN = input.strides[0];\n    index_t inp_sC = input.strides[1];\n    index_t inp_sH = input.strides[2];\n    index_t inp_sW = input.strides[3];\n    index_t grid_sN = grid.strides[0];\n    index_t grid_sH = grid.strides[1];\n    index_t grid_sW = grid.strides[2];\n    index_t grid_sCoor = grid.strides[3];\n    index_t out_sN = output.strides[0];\n    index_t out_sC = output.strides[1];\n    index_t out_sH = output.strides[2];\n    index_t out_sW = output.strides[3];\n\n    CUDA_KERNEL_LOOP_TYPE(index, nthreads, index_t) {\n      const index_t w = index % out_W;\n      const index_t h = (index / out_W) % out_H;\n      const index_t n = index / (out_H * out_W);\n      const index_t grid_offset = n * grid_sN + h * grid_sH + w * grid_sW;\n\n      // get the corresponding input x, y co-ordinates from grid\n      opmath_t x = grid.data[grid_offset];\n      opmath_t y = grid.data[grid_offset + grid_sCoor];\n\n      opmath_t ix = grid_sampler_compute_source_index(x, inp_W, padding_mode, align_corners);\n      opmath_t iy = grid_sampler_compute_source_index(y, inp_H, padding_mode, align_corners);\n\n      if (interpolation_mode == GridSamplerInterpolation::Bilinear) {\n        // get NE, NW, SE, SW pixel values from (x, y)\n        index_t ix_nw = static_cast<index_t>(::floor(ix));\n        index_t iy_nw = static_cast<index_t>(::floor(iy));\n        index_t ix_ne = ix_nw + 1;\n        index_t iy_ne = iy_nw;\n        index_t ix_sw = ix_nw;\n        index_t iy_sw = iy_nw + 1;\n        index_t ix_se = ix_nw + 1;\n        index_t iy_se = iy_nw + 1;\n\n        // get surfaces to each neighbor:\n        opmath_t nw = (ix_se - ix)    * (iy_se - iy);\n        opmath_t ne = (ix    - ix_sw) * (iy_sw - iy);\n        opmath_t sw = (ix_ne - ix)    * (iy    - iy_ne);\n        opmath_t se = (ix    - ix_nw) * (iy    - iy_nw);\n\n        // calculate bilinear weighted pixel value and set output pixel\n        auto inp_ptr_NC = input.data + n * inp_sN;\n        auto out_ptr_NCHW = output.data + n * out_sN + h * out_sH + w * out_sW;\n        for (index_t c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCHW += out_sC) {\n          opmath_t out_acc = 0;\n          if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW] * nw;\n          }\n          if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW] * ne;\n          }\n          if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW] * sw;\n          }\n          if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW] * se;\n          }\n          *out_ptr_NCHW = out_acc;\n        }\n      } else if (interpolation_mode == GridSamplerInterpolation::Nearest) {\n        index_t ix_nearest = static_cast<index_t>(std::nearbyint(ix));\n        index_t iy_nearest = static_cast<index_t>(std::nearbyint(iy));\n\n        // assign nearest neighbour pixel value to output pixel\n        auto inp_ptr_NC = input.data + n * inp_sN;\n        auto out_ptr_NCHW = output.data + n * out_sN + h * out_sH + w * out_sW;\n        for (index_t c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCHW += out_sC) {\n          if (within_bounds_2d(iy_nearest, ix_nearest, inp_H, inp_W)) {\n            *out_ptr_NCHW = inp_ptr_NC[iy_nearest * inp_sH + ix_nearest * inp_sW];\n          } else {\n            *out_ptr_NCHW = static_cast<scalar_t>(0);\n          }\n        }\n      } else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {\n\n        ix = grid_sampler_unnormalize(x, inp_W, align_corners);\n        iy = grid_sampler_unnormalize(y, inp_H, align_corners);\n\n        opmath_t ix_nw = std::floor(ix);\n        opmath_t iy_nw = std::floor(iy);\n\n        const opmath_t tx = ix - ix_nw;\n        const opmath_t ty = iy - iy_nw;\n\n        auto inp_ptr_NC = input.data + n * inp_sN;\n        auto out_ptr_NCHW = output.data + n * out_sN + h * out_sH + w * out_sW;\n        for (index_t c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCHW += out_sC) {\n          opmath_t coefficients[4];\n\n          #pragma unroll 4\n          for (index_t i = 0; i < 4; ++i) {\n            coefficients[i] = cubic_interp1d(\n              get_value_bounded<scalar_t>(inp_ptr_NC, ix_nw - 1, iy_nw - 1 + i, inp_W, inp_H, inp_sW, inp_sH, padding_mode, align_corners),\n              get_value_bounded<scalar_t>(inp_ptr_NC, ix_nw + 0, iy_nw - 1 + i, inp_W, inp_H, inp_sW, inp_sH, padding_mode, align_corners),\n              get_value_bounded<scalar_t>(inp_ptr_NC, ix_nw + 1, iy_nw - 1 + i, inp_W, inp_H, inp_sW, inp_sH, padding_mode, align_corners),\n              get_value_bounded<scalar_t>(inp_ptr_NC, ix_nw + 2, iy_nw - 1 + i, inp_W, inp_H, inp_sW, inp_sH, padding_mode, align_corners),\n              tx);\n          }\n\n          *out_ptr_NCHW = cubic_interp1d(\n            coefficients[0],\n            coefficients[1],\n            coefficients[2],\n            coefficients[3],\n            ty);\n        }\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "grid_sampler_3d_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void grid_sampler_3d_kernel(\n      const index_t nthreads,\n      TensorInfo<const scalar_t, index_t> input,\n      TensorInfo<const scalar_t, index_t> grid,\n      TensorInfo<scalar_t, index_t> output,\n      const GridSamplerInterpolation interpolation_mode,\n      const GridSamplerPadding padding_mode,\n      bool align_corners) {\n\n    using opmath_t = at::opmath_type<scalar_t>;\n    index_t C = input.sizes[1];\n    index_t inp_D = input.sizes[2];\n    index_t inp_H = input.sizes[3];\n    index_t inp_W = input.sizes[4];\n    index_t out_D = grid.sizes[1];\n    index_t out_H = grid.sizes[2];\n    index_t out_W = grid.sizes[3];\n    index_t inp_sN = input.strides[0];\n    index_t inp_sC = input.strides[1];\n    index_t inp_sD = input.strides[2];\n    index_t inp_sH = input.strides[3];\n    index_t inp_sW = input.strides[4];\n    index_t grid_sN = grid.strides[0];\n    index_t grid_sD = grid.strides[1];\n    index_t grid_sH = grid.strides[2];\n    index_t grid_sW = grid.strides[3];\n    index_t grid_sCoor = grid.strides[4];\n    index_t out_sN = output.strides[0];\n    index_t out_sC = output.strides[1];\n    index_t out_sD = output.strides[2];\n    index_t out_sH = output.strides[3];\n    index_t out_sW = output.strides[4];\n\n    CUDA_KERNEL_LOOP_TYPE(index, nthreads, index_t) {\n      const index_t w = index % out_W;\n      const index_t h = (index / out_W) % out_H;\n      const index_t d = (index / (out_H * out_W)) % out_D;\n      const index_t n = index / (out_D * out_H * out_W);\n      const index_t grid_offset = n * grid_sN + d * grid_sD + h * grid_sH + w * grid_sW;\n\n      // get the corresponding input x, y, z co-ordinates from grid\n      opmath_t x = grid.data[grid_offset];\n      opmath_t y = grid.data[grid_offset + grid_sCoor];\n      opmath_t z = grid.data[grid_offset + 2 * grid_sCoor];\n\n      opmath_t ix = grid_sampler_compute_source_index(x, inp_W, padding_mode, align_corners);\n      opmath_t iy = grid_sampler_compute_source_index(y, inp_H, padding_mode, align_corners);\n      opmath_t iz = grid_sampler_compute_source_index(z, inp_D, padding_mode, align_corners);\n\n      if (interpolation_mode == GridSamplerInterpolation::Bilinear) {\n        // get corner pixel values from (x, y, z)\n        // for 4d, we used north-east-south-west\n        // for 5d, we add top-bottom\n        index_t ix_tnw = static_cast<index_t>(::floor(ix));\n        index_t iy_tnw = static_cast<index_t>(::floor(iy));\n        index_t iz_tnw = static_cast<index_t>(::floor(iz));\n\n        index_t ix_tne = ix_tnw + 1;\n        index_t iy_tne = iy_tnw;\n        index_t iz_tne = iz_tnw;\n\n        index_t ix_tsw = ix_tnw;\n        index_t iy_tsw = iy_tnw + 1;\n        index_t iz_tsw = iz_tnw;\n\n        index_t ix_tse = ix_tnw + 1;\n        index_t iy_tse = iy_tnw + 1;\n        index_t iz_tse = iz_tnw;\n\n        index_t ix_bnw = ix_tnw;\n        index_t iy_bnw = iy_tnw;\n        index_t iz_bnw = iz_tnw + 1;\n\n        index_t ix_bne = ix_tnw + 1;\n        index_t iy_bne = iy_tnw;\n        index_t iz_bne = iz_tnw + 1;\n\n        index_t ix_bsw = ix_tnw;\n        index_t iy_bsw = iy_tnw + 1;\n        index_t iz_bsw = iz_tnw + 1;\n\n        index_t ix_bse = ix_tnw + 1;\n        index_t iy_bse = iy_tnw + 1;\n        index_t iz_bse = iz_tnw + 1;\n\n        // get surfaces to each neighbor:\n        opmath_t tnw = (ix_bse - ix)    * (iy_bse - iy)    * (iz_bse - iz);\n        opmath_t tne = (ix    - ix_bsw) * (iy_bsw - iy)    * (iz_bsw - iz);\n        opmath_t tsw = (ix_bne - ix)    * (iy    - iy_bne) * (iz_bne - iz);\n        opmath_t tse = (ix    - ix_bnw) * (iy    - iy_bnw) * (iz_bnw - iz);\n        opmath_t bnw = (ix_tse - ix)    * (iy_tse - iy)    * (iz - iz_tse);\n        opmath_t bne = (ix    - ix_tsw) * (iy_tsw - iy)    * (iz - iz_tsw);\n        opmath_t bsw = (ix_tne - ix)    * (iy    - iy_tne) * (iz - iz_tne);\n        opmath_t bse = (ix    - ix_tnw) * (iy    - iy_tnw) * (iz - iz_tnw);\n\n        auto inp_ptr_NC = input.data + n * inp_sN;\n        auto out_ptr_NCDHW = output.data + n * out_sN + d * out_sD + h * out_sH + w * out_sW;\n        for (index_t c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCDHW += out_sC) {\n          //   (c, iz_tnw, iy_tnw, ix_tnw) * tnw + (c, iz_tne, iy_tne, ix_tne) * tne\n          // + (c, iz_tsw, iy_tsw, ix_tsw) * tsw + (c, iz_tse, iy_tse, ix_tse) * tse\n          // + (c, iz_bnw, iy_bnw, ix_bnw) * bnw + (c, iz_bne, iy_bne, ix_bne) * bne\n          // + (c, iz_bsw, iy_bsw, ix_bsw) * bsw + (c, iz_bse, iy_bse, ix_bse) * bse\n          opmath_t out_acc = 0;\n          if (within_bounds_3d(iz_tnw, iy_tnw, ix_tnw, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_tnw * inp_sD + iy_tnw * inp_sH + ix_tnw * inp_sW] * tnw;\n          }\n          if (within_bounds_3d(iz_tne, iy_tne, ix_tne, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_tne * inp_sD + iy_tne * inp_sH + ix_tne * inp_sW] * tne;\n          }\n          if (within_bounds_3d(iz_tsw, iy_tsw, ix_tsw, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_tsw * inp_sD + iy_tsw * inp_sH + ix_tsw * inp_sW] * tsw;\n          }\n          if (within_bounds_3d(iz_tse, iy_tse, ix_tse, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_tse * inp_sD + iy_tse * inp_sH + ix_tse * inp_sW] * tse;\n          }\n          if (within_bounds_3d(iz_bnw, iy_bnw, ix_bnw, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_bnw * inp_sD + iy_bnw * inp_sH + ix_bnw * inp_sW] * bnw;\n          }\n          if (within_bounds_3d(iz_bne, iy_bne, ix_bne, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_bne * inp_sD + iy_bne * inp_sH + ix_bne * inp_sW] * bne;\n          }\n          if (within_bounds_3d(iz_bsw, iy_bsw, ix_bsw, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_bsw * inp_sD + iy_bsw * inp_sH + ix_bsw * inp_sW] * bsw;\n          }\n          if (within_bounds_3d(iz_bse, iy_bse, ix_bse, inp_D, inp_H, inp_W)) {\n            out_acc += inp_ptr_NC[iz_bse * inp_sD + iy_bse * inp_sH + ix_bse * inp_sW] * bse;\n          }\n          *out_ptr_NCDHW = out_acc;\n        }\n      } else if (interpolation_mode == GridSamplerInterpolation::Nearest) {\n        index_t ix_nearest = static_cast<index_t>(std::nearbyint(ix));\n        index_t iy_nearest = static_cast<index_t>(std::nearbyint(iy));\n        index_t iz_nearest = static_cast<index_t>(std::nearbyint(iz));\n\n        // assign nearest neighbour pixel value to output pixel\n        auto inp_ptr_NC = input.data + n * inp_sN;\n        auto out_ptr_NCDHW = output.data + n * out_sN + d * out_sD + h * out_sH + w * out_sW;\n        for (index_t c = 0; c < C; ++c, inp_ptr_NC += inp_sC, out_ptr_NCDHW += out_sC) {\n          if (within_bounds_3d(iz_nearest, iy_nearest, ix_nearest, inp_D, inp_H, inp_W)) {\n            *out_ptr_NCDHW = inp_ptr_NC[iz_nearest * inp_sD + iy_nearest * inp_sH + ix_nearest * inp_sW];\n          } else {\n            *out_ptr_NCDHW = static_cast<scalar_t>(0);\n          }\n        }\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "grid_sampler_2d_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void grid_sampler_2d_backward_kernel(\n      const index_t nthreads,\n      TensorInfo<const scalar_t, index_t> grad_output,\n      TensorInfo<const scalar_t, index_t> input,\n      TensorInfo<const scalar_t, index_t> grid,\n      TensorInfo<scalar_t, index_t> grad_input,  // initialized to zeros (or unused if input_requires_grad is false)\n      TensorInfo<scalar_t, index_t> grad_grid,   // initialized to empty\n      const GridSamplerInterpolation interpolation_mode,\n      const GridSamplerPadding padding_mode,\n      bool align_corners,\n      const index_t grad_input_memory_span,\n      const bool input_requires_grad) {\n\n    index_t C = input.sizes[1];\n    index_t inp_H = input.sizes[2];\n    index_t inp_W = input.sizes[3];\n    index_t out_H = grid.sizes[1];\n    index_t out_W = grid.sizes[2];\n    index_t inp_sN = input.strides[0];\n    index_t inp_sC = input.strides[1];\n    index_t inp_sH = input.strides[2];\n    index_t inp_sW = input.strides[3];\n    index_t grid_sN = grid.strides[0];\n    index_t grid_sH = grid.strides[1];\n    index_t grid_sW = grid.strides[2];\n    index_t grid_sCoor = grid.strides[3];\n    index_t gOut_sN = grad_output.strides[0];\n    index_t gOut_sC = grad_output.strides[1];\n    index_t gOut_sH = grad_output.strides[2];\n    index_t gOut_sW = grad_output.strides[3];\n    // gInp_* (and NC_offset below) are not really needed if input_requires_grad is false.\n    index_t gInp_sN;\n    index_t gInp_sC;\n    index_t gInp_sH;\n    index_t gInp_sW;\n    if (input_requires_grad) {\n      gInp_sN = grad_input.strides[0];\n      gInp_sC = grad_input.strides[1];\n      gInp_sH = grad_input.strides[2];\n      gInp_sW = grad_input.strides[3];\n    }\n    index_t gGrid_sW = grad_grid.strides[2];\n\n    CUDA_KERNEL_LOOP_TYPE(index, nthreads, index_t) {\n      const index_t w = index % out_W;\n      const index_t h = (index / out_W) % out_H;\n      const index_t n = index / (out_H * out_W);\n      const auto grid_offset = n * grid_sN + h * grid_sH + w * grid_sW;\n\n      // get the corresponding input x, y co-ordinates from grid\n      scalar_t x = grid.data[grid_offset];\n      scalar_t y = grid.data[grid_offset + grid_sCoor];\n\n      // multipliers for gradients on ix and iy\n      scalar_t gix_mult, giy_mult;\n      scalar_t ix = grid_sampler_compute_source_index_set_grad(x, inp_W, padding_mode, align_corners, &gix_mult);\n      scalar_t iy = grid_sampler_compute_source_index_set_grad(y, inp_H, padding_mode, align_corners, &giy_mult);\n\n      if (interpolation_mode == GridSamplerInterpolation::Bilinear) {\n        // get NE, NW, SE, SW pixel values from (x, y)\n        index_t ix_nw = static_cast<index_t>(std::floor(ix));\n        index_t iy_nw = static_cast<index_t>(std::floor(iy));\n        index_t ix_ne = ix_nw + 1;\n        index_t iy_ne = iy_nw;\n        index_t ix_sw = ix_nw;\n        index_t iy_sw = iy_nw + 1;\n        index_t ix_se = ix_nw + 1;\n        index_t iy_se = iy_nw + 1;\n\n        // get surfaces to each neighbor:\n        scalar_t nw = (ix_se - ix)    * (iy_se - iy);\n        scalar_t ne = (ix    - ix_sw) * (iy_sw - iy);\n        scalar_t sw = (ix_ne - ix)    * (iy    - iy_ne);\n        scalar_t se = (ix    - ix_nw) * (iy    - iy_nw);\n\n        scalar_t gix = static_cast<scalar_t>(0), giy = static_cast<scalar_t>(0);\n        const scalar_t *gOut_ptr_NCHW = grad_output.data + n * gOut_sN + h * gOut_sH + w * gOut_sW;\n        index_t NC_offset = n * gInp_sN;\n        const scalar_t *inp_ptr_NC = input.data + n * inp_sN;\n        for (index_t c = 0; c < C; ++c, inp_ptr_NC += inp_sC, NC_offset += gInp_sC, gOut_ptr_NCHW += gOut_sC) {\n          const scalar_t gOut = *gOut_ptr_NCHW;\n\n          if (input_requires_grad) {\n            // calculate and set grad_input. See Note [Passing pointer and offset to fastAtomicAdd].\n            safe_add_2d(grad_input.data, iy_nw, ix_nw, gInp_sH, gInp_sW, inp_H, inp_W, nw * gOut, NC_offset, grad_input_memory_span);\n            safe_add_2d(grad_input.data, iy_ne, ix_ne, gInp_sH, gInp_sW, inp_H, inp_W, ne * gOut, NC_offset, grad_input_memory_span);\n            safe_add_2d(grad_input.data, iy_sw, ix_sw, gInp_sH, gInp_sW, inp_H, inp_W, sw * gOut, NC_offset, grad_input_memory_span);\n            safe_add_2d(grad_input.data, iy_se, ix_se, gInp_sH, gInp_sW, inp_H, inp_W, se * gOut, NC_offset, grad_input_memory_span);\n          }\n\n          // calculate grad_grid\n          if (within_bounds_2d(iy_nw, ix_nw, inp_H, inp_W)) {\n            scalar_t nw_val = inp_ptr_NC[iy_nw * inp_sH + ix_nw * inp_sW];\n            gix -= nw_val * (iy_se - iy) * gOut;\n            giy -= nw_val * (ix_se - ix) * gOut;\n          }\n          if (within_bounds_2d(iy_ne, ix_ne, inp_H, inp_W)) {\n            scalar_t ne_val = inp_ptr_NC[iy_ne * inp_sH + ix_ne * inp_sW];\n            gix += ne_val * (iy_sw - iy) * gOut;\n            giy -= ne_val * (ix - ix_sw) * gOut;\n          }\n          if (within_bounds_2d(iy_sw, ix_sw, inp_H, inp_W)) {\n            scalar_t sw_val = inp_ptr_NC[iy_sw * inp_sH + ix_sw * inp_sW];\n            gix -= sw_val * (iy - iy_ne) * gOut;\n            giy += sw_val * (ix_ne - ix) * gOut;\n          }\n          if (within_bounds_2d(iy_se, ix_se, inp_H, inp_W)) {\n            scalar_t se_val = inp_ptr_NC[iy_se * inp_sH + ix_se * inp_sW];\n            gix += se_val * (iy - iy_nw) * gOut;\n            giy += se_val * (ix - ix_nw) * gOut;\n          }\n        }\n\n        // assuming grad_grid is contiguous\n        // thus we can\n        //   1. use index with gGrid_sW to directly compute gGrid_ptr_NHW\n        //   2. directly assign to gGrid_ptr_NHW[0], gGrid_ptr_NHW[1]\n        scalar_t *gGrid_ptr_NHW = grad_grid.data + index * gGrid_sW;\n        gGrid_ptr_NHW[0] = gix_mult * gix;\n        gGrid_ptr_NHW[1] = giy_mult * giy;\n      } else if (interpolation_mode == GridSamplerInterpolation::Nearest) {\n        if (input_requires_grad) {\n          index_t ix_nearest = static_cast<index_t>(std::nearbyint(ix));\n          index_t iy_nearest = static_cast<index_t>(std::nearbyint(iy));\n\n          // assign nearest neighbour pixel value to output pixel\n          const scalar_t *gOut_ptr_NCHW = grad_output.data + n * gOut_sN + h * gOut_sH + w * gOut_sW;\n          index_t NC_offset = n * gInp_sN;\n          for (index_t c = 0; c < C; ++c, NC_offset += gInp_sC, gOut_ptr_NCHW += gOut_sC) {\n            // calculate and set grad_input. See Note [Passing pointer and offset to fastAtomicAdd].\n            safe_add_2d(grad_input.data, iy_nearest, ix_nearest, gInp_sH, gInp_sW, inp_H, inp_W, *gOut_ptr_NCHW, NC_offset, grad_input_memory_span);\n          }\n        }\n\n        // assuming grad_grid is contiguous\n        // thus we can\n        //   1. use index with gGrid_sW to directly compute gGrid_ptr_NHW\n        //   2. directly assign to gGrid_ptr_NHW[0], gGrid_ptr_NHW[1]\n        scalar_t *gGrid_ptr_NHW = grad_grid.data + index * gGrid_sW;\n        gGrid_ptr_NHW[0] = static_cast<scalar_t>(0);\n        gGrid_ptr_NHW[1] = static_cast<scalar_t>(0);\n      } else if (interpolation_mode == GridSamplerInterpolation::Bicubic) {\n\n        ix = grid_sampler_unnormalize_set_grad(x, inp_W, align_corners, &gix_mult);\n        iy = grid_sampler_unnormalize_set_grad(y, inp_H, align_corners, &giy_mult);\n\n        scalar_t ix_nw = std::floor(ix);\n        scalar_t iy_nw = std::floor(iy);\n\n        const scalar_t tx = ix - ix_nw;\n        const scalar_t ty = iy - iy_nw;\n\n        scalar_t x_coeffs[4];\n        scalar_t y_coeffs[4];\n        scalar_t x_coeffs_grad[4];\n        scalar_t y_coeffs_grad[4];\n\n        get_cubic_upsampling_coefficients<scalar_t>(x_coeffs, tx);\n        get_cubic_upsampling_coefficients<scalar_t>(y_coeffs, ty);\n        get_cubic_coefficients_grad<scalar_t>(x_coeffs_grad, tx);\n        get_cubic_coefficients_grad<scalar_t>(y_coeffs_grad, ty);\n\n        scalar_t gix = static_cast<scalar_t>(0);\n        scalar_t giy = static_cast<scalar_t>(0);\n\n        const scalar_t *gOut_ptr_NCHW = grad_output.data + n * gOut_sN + h * gOut_sH + w * gOut_sW;\n        index_t NC_offset = n * gInp_sN;\n        const scalar_t *inp_ptr_NC = input.data + n * inp_sN;\n\n        for (index_t c = 0; c < C; ++c, gOut_ptr_NCHW += gOut_sC, NC_offset += gInp_sC, inp_ptr_NC+= inp_sC) {\n          const scalar_t gOut = *gOut_ptr_NCHW;\n\n          #pragma unroll 4\n          for (index_t i = 0; i < 4; ++i) {\n            #pragma unroll 4\n            for (index_t j = 0; j < 4; ++j) {\n\n              if (input_requires_grad) {\n                // set input gradient. See Note [Passing pointer and offset to fastAtomicAdd].\n                add_value_bounded<scalar_t>(grad_input.data, ix_nw - 1 + i, iy_nw - 1 + j, inp_W, inp_H, gInp_sW, gInp_sH,\n                  gOut * x_coeffs[i] * y_coeffs[j],\n                  padding_mode,\n                  align_corners,\n                  NC_offset,\n                  grad_input_memory_span);\n              }\n\n              // set grid gradient\n              scalar_t val = get_value_bounded<scalar_t>(inp_ptr_NC, ix_nw - 1 + i, iy_nw - 1 + j,\n                inp_W, inp_H, inp_sW, inp_sH, padding_mode, align_corners);\n\n              gix -= val * x_coeffs_grad[i] * y_coeffs[j] * gOut;\n              giy -= val * y_coeffs_grad[j] * x_coeffs[i] * gOut;\n            }\n          }\n        }\n\n        scalar_t *gGrid_ptr_NHW = grad_grid.data + index * gGrid_sW;\n        gGrid_ptr_NHW[0] = gix_mult * gix;\n        gGrid_ptr_NHW[1] = giy_mult * giy;\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "grid_sampler_3d_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void grid_sampler_3d_backward_kernel(\n      const index_t nthreads,\n      TensorInfo<const scalar_t, index_t> grad_output,\n      TensorInfo<const scalar_t, index_t> input,\n      TensorInfo<const scalar_t, index_t> grid,\n      TensorInfo<scalar_t, index_t> grad_input,  // initialized to zeros (or unused if input_requires_grad is false)\n      TensorInfo<scalar_t, index_t> grad_grid,   // initialized to empty\n      const GridSamplerInterpolation interpolation_mode,\n      const GridSamplerPadding padding_mode,\n      bool align_corners,\n      const index_t grad_input_memory_span,\n      const bool input_requires_grad) {\n\n    index_t C = input.sizes[1];\n    index_t inp_D = input.sizes[2];\n    index_t inp_H = input.sizes[3];\n    index_t inp_W = input.sizes[4];\n    index_t out_D = grid.sizes[1];\n    index_t out_H = grid.sizes[2];\n    index_t out_W = grid.sizes[3];\n    index_t inp_sN = input.strides[0];\n    index_t inp_sC = input.strides[1];\n    index_t inp_sD = input.strides[2];\n    index_t inp_sH = input.strides[3];\n    index_t inp_sW = input.strides[4];\n    index_t grid_sN = grid.strides[0];\n    index_t grid_sD = grid.strides[1];\n    index_t grid_sH = grid.strides[2];\n    index_t grid_sW = grid.strides[3];\n    index_t grid_sCoor = grid.strides[4];\n    index_t gOut_sN = grad_output.strides[0];\n    index_t gOut_sC = grad_output.strides[1];\n    index_t gOut_sD = grad_output.strides[2];\n    index_t gOut_sH = grad_output.strides[3];\n    index_t gOut_sW = grad_output.strides[4];\n    // gInp_* (and NC_offset below) are not really needed if input_requires_grad is false.\n    int64_t gInp_sN = 0;\n    int64_t gInp_sC = 0;\n    int64_t gInp_sD = 0;\n    int64_t gInp_sH = 0;\n    int64_t gInp_sW = 0;\n    if (input_requires_grad) {\n      gInp_sN = grad_input.strides[0];\n      gInp_sC = grad_input.strides[1];\n      gInp_sD = grad_input.strides[2];\n      gInp_sH = grad_input.strides[3];\n      gInp_sW = grad_input.strides[4];\n    }\n    index_t gGrid_sW = grad_grid.strides[3];\n\n    CUDA_KERNEL_LOOP_TYPE(index, nthreads, index_t) {\n      const index_t w = index % out_W;\n      const index_t h = (index / out_W) % out_H;\n      const index_t d = (index / (out_H * out_W)) % out_D;\n      const index_t n = index / (out_D * out_H * out_W);\n      const auto grid_offset = n * grid_sN + d * grid_sD + h * grid_sH + w * grid_sW;\n\n      // get the corresponding input x, y, z co-ordinates from grid\n      scalar_t ix = grid.data[grid_offset];\n      scalar_t iy = grid.data[grid_offset + grid_sCoor];\n      scalar_t iz = grid.data[grid_offset + 2 * grid_sCoor];\n\n      // multipliers for gradients on ix, iy, and iz\n      scalar_t gix_mult, giy_mult, giz_mult;\n      ix = grid_sampler_compute_source_index_set_grad(ix, inp_W, padding_mode, align_corners, &gix_mult);\n      iy = grid_sampler_compute_source_index_set_grad(iy, inp_H, padding_mode, align_corners, &giy_mult);\n      iz = grid_sampler_compute_source_index_set_grad(iz, inp_D, padding_mode, align_corners, &giz_mult);\n\n      if (interpolation_mode == GridSamplerInterpolation::Bilinear) {\n        // get corner pixel values from (x, y, z)\n        // for 4d, we used north-east-south-west\n        // for 5d, we add top-bottom\n        index_t ix_tnw = static_cast<index_t>(std::floor(ix));\n        index_t iy_tnw = static_cast<index_t>(std::floor(iy));\n        index_t iz_tnw = static_cast<index_t>(std::floor(iz));\n\n        index_t ix_tne = ix_tnw + 1;\n        index_t iy_tne = iy_tnw;\n        index_t iz_tne = iz_tnw;\n\n        index_t ix_tsw = ix_tnw;\n        index_t iy_tsw = iy_tnw + 1;\n        index_t iz_tsw = iz_tnw;\n\n        index_t ix_tse = ix_tnw + 1;\n        index_t iy_tse = iy_tnw + 1;\n        index_t iz_tse = iz_tnw;\n\n        index_t ix_bnw = ix_tnw;\n        index_t iy_bnw = iy_tnw;\n        index_t iz_bnw = iz_tnw + 1;\n\n        index_t ix_bne = ix_tnw + 1;\n        index_t iy_bne = iy_tnw;\n        index_t iz_bne = iz_tnw + 1;\n\n        index_t ix_bsw = ix_tnw;\n        index_t iy_bsw = iy_tnw + 1;\n        index_t iz_bsw = iz_tnw + 1;\n\n        index_t ix_bse = ix_tnw + 1;\n        index_t iy_bse = iy_tnw + 1;\n        index_t iz_bse = iz_tnw + 1;\n\n        // get surfaces to each neighbor:\n        scalar_t tnw = (ix_bse - ix)    * (iy_bse - iy)    * (iz_bse - iz);\n        scalar_t tne = (ix    - ix_bsw) * (iy_bsw - iy)    * (iz_bsw - iz);\n        scalar_t tsw = (ix_bne - ix)    * (iy    - iy_bne) * (iz_bne - iz);\n        scalar_t tse = (ix    - ix_bnw) * (iy    - iy_bnw) * (iz_bnw - iz);\n        scalar_t bnw = (ix_tse - ix)    * (iy_tse - iy)    * (iz - iz_tse);\n        scalar_t bne = (ix    - ix_tsw) * (iy_tsw - iy)    * (iz - iz_tsw);\n        scalar_t bsw = (ix_tne - ix)    * (iy    - iy_tne) * (iz - iz_tne);\n        scalar_t bse = (ix    - ix_tnw) * (iy    - iy_tnw) * (iz - iz_tnw);\n\n        scalar_t gix = static_cast<scalar_t>(0), giy = static_cast<scalar_t>(0), giz = static_cast<scalar_t>(0);\n        const scalar_t *gOut_ptr_NCDHW = grad_output.data + n * gOut_sN + d * gOut_sD + h * gOut_sH + w * gOut_sW;\n        index_t NC_offset;\n        if (input_requires_grad) {\n          NC_offset = n * gInp_sN;\n        }\n        const scalar_t *inp_ptr_NC = input.data + n * inp_sN;\n        // calculate bilinear weighted pixel value and set output pixel\n        for (index_t c = 0; c < C; ++c, gOut_ptr_NCDHW += gOut_sC, NC_offset += gInp_sC, inp_ptr_NC += inp_sC) {\n          scalar_t gOut = *gOut_ptr_NCDHW;\n\n          // calculate and set grad_input. See Note [Passing pointer and offset to fastAtomicAdd].\n          if (input_requires_grad) {\n            safe_add_3d(grad_input.data, iz_tnw, iy_tnw, ix_tnw, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, tnw * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_tne, iy_tne, ix_tne, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, tne * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_tsw, iy_tsw, ix_tsw, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, tsw * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_tse, iy_tse, ix_tse, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, tse * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_bnw, iy_bnw, ix_bnw, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, bnw * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_bne, iy_bne, ix_bne, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, bne * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_bsw, iy_bsw, ix_bsw, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, bsw * gOut,\n                        NC_offset, grad_input_memory_span);\n            safe_add_3d(grad_input.data, iz_bse, iy_bse, ix_bse, gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, bse * gOut,\n                        NC_offset, grad_input_memory_span);\n          }\n          // calculate grad_grid\n          if (within_bounds_3d(iz_tnw, iy_tnw, ix_tnw, inp_D, inp_H, inp_W)) {\n            scalar_t tnw_val = inp_ptr_NC[iz_tnw * inp_sD + iy_tnw * inp_sH + ix_tnw * inp_sW];\n            gix -= tnw_val * (iy_bse - iy)    * (iz_bse - iz)    * gOut;\n            giy -= tnw_val * (ix_bse - ix)    * (iz_bse - iz)    * gOut;\n            giz -= tnw_val * (ix_bse - ix)    * (iy_bse - iy)    * gOut;\n          }\n          if (within_bounds_3d(iz_tne, iy_tne, ix_tne, inp_D, inp_H, inp_W)) {\n            scalar_t tne_val = inp_ptr_NC[iz_tne * inp_sD + iy_tne * inp_sH + ix_tne * inp_sW];\n            gix += tne_val * (iy_bsw - iy)    * (iz_bsw - iz)    * gOut;\n            giy -= tne_val * (ix    - ix_bsw) * (iz_bsw - iz)    * gOut;\n            giz -= tne_val * (ix    - ix_bsw) * (iy_bsw - iy)    * gOut;\n          }\n          if (within_bounds_3d(iz_tsw, iy_tsw, ix_tsw, inp_D, inp_H, inp_W)) {\n            scalar_t tsw_val = inp_ptr_NC[iz_tsw * inp_sD + iy_tsw * inp_sH + ix_tsw * inp_sW];\n            gix -= tsw_val * (iy - iy_bne)    * (iz_bne - iz)    * gOut;\n            giy += tsw_val * (ix_bne - ix)    * (iz_bne - iz)    * gOut;\n            giz -= tsw_val * (ix_bne - ix)    * (iy    - iy_bne) * gOut;\n          }\n          if (within_bounds_3d(iz_tse, iy_tse, ix_tse, inp_D, inp_H, inp_W)) {\n            scalar_t tse_val = inp_ptr_NC[iz_tse * inp_sD + iy_tse * inp_sH + ix_tse * inp_sW];\n            gix += tse_val * (iy - iy_bnw)    * (iz_bnw - iz)    * gOut;\n            giy += tse_val * (ix    - ix_bnw) * (iz_bnw - iz)    * gOut;\n            giz -= tse_val * (ix    - ix_bnw) * (iy    - iy_bnw) * gOut;\n          }\n          if (within_bounds_3d(iz_bnw, iy_bnw, ix_bnw, inp_D, inp_H, inp_W)) {\n            scalar_t bnw_val = inp_ptr_NC[iz_bnw * inp_sD + iy_bnw * inp_sH + ix_bnw * inp_sW];\n            gix -= bnw_val * (iy_tse - iy)    * (iz - iz_tse)    * gOut;\n            giy -= bnw_val * (ix_tse - ix)    * (iz - iz_tse)    * gOut;\n            giz += bnw_val * (ix_tse - ix)    * (iy_tse - iy)    * gOut;\n          }\n          if (within_bounds_3d(iz_bne, iy_bne, ix_bne, inp_D, inp_H, inp_W)) {\n            scalar_t bne_val = inp_ptr_NC[iz_bne * inp_sD + iy_bne * inp_sH + ix_bne * inp_sW];\n            gix += bne_val * (iy_tsw - iy)    * (iz - iz_tsw)    * gOut;\n            giy -= bne_val * (ix    - ix_tsw) * (iz - iz_tsw)    * gOut;\n            giz += bne_val * (ix    - ix_tsw) * (iy_tsw - iy)    * gOut;\n          }\n          if (within_bounds_3d(iz_bsw, iy_bsw, ix_bsw, inp_D, inp_H, inp_W)) {\n            scalar_t bsw_val = inp_ptr_NC[iz_bsw * inp_sD + iy_bsw * inp_sH + ix_bsw * inp_sW];\n            gix -= bsw_val * (iy - iy_tne)    * (iz - iz_tne)    * gOut;\n            giy += bsw_val * (ix_tne - ix)    * (iz - iz_tne)    * gOut;\n            giz += bsw_val * (ix_tne - ix)    * (iy    - iy_tne) * gOut;\n          }\n          if (within_bounds_3d(iz_bse, iy_bse, ix_bse, inp_D, inp_H, inp_W)) {\n            scalar_t bse_val = inp_ptr_NC[iz_bse * inp_sD + iy_bse * inp_sH + ix_bse * inp_sW];\n            gix += bse_val * (iy - iy_tnw)    * (iz - iz_tnw)    * gOut;\n            giy += bse_val * (ix    - ix_tnw) * (iz - iz_tnw)    * gOut;\n            giz += bse_val * (ix    - ix_tnw) * (iy    - iy_tnw) * gOut;\n          }\n        }\n\n        // assuming grad_grid is contiguous\n        // thus we can\n        //   1. use index with gGrid_sW to directly compute gGrid_ptr_NDHW\n        //   2. directly assign to gGrid_ptr_NDHW[0], gGrid_ptr_NDHW[1], gGrid_ptr_NDHW[2]\n        scalar_t *gGrid_ptr_NDHW = grad_grid.data + index * gGrid_sW;\n        gGrid_ptr_NDHW[0] = gix_mult * gix;\n        gGrid_ptr_NDHW[1] = giy_mult * giy;\n        gGrid_ptr_NDHW[2] = giz_mult * giz;\n      } else if (interpolation_mode == GridSamplerInterpolation::Nearest) {\n        if (input_requires_grad) {\n          auto ix_nearest = static_cast<index_t>(std::nearbyint(ix));\n          auto iy_nearest = static_cast<index_t>(std::nearbyint(iy));\n          auto iz_nearest = static_cast<index_t>(std::nearbyint(iz));\n\n          // assign nearest neighbour pixel value to output pixel\n          const scalar_t *gOut_ptr_NCDHW = grad_output.data + n * gOut_sN + d * gOut_sD + h * gOut_sH + w * gOut_sW;\n          index_t NC_offset = n * gInp_sN;\n          for (index_t c = 0; c < C; ++c, gOut_ptr_NCDHW += gOut_sC, NC_offset += gInp_sC) {\n            // calculate and set grad_input. See Note [Passing pointer and offset to fastAtomicAdd].\n            safe_add_3d(grad_input.data, iz_nearest, iy_nearest, ix_nearest,\n                        gInp_sD, gInp_sH, gInp_sW, inp_D, inp_H, inp_W, *gOut_ptr_NCDHW,\n                        NC_offset, grad_input_memory_span);\n          }\n        }\n        // assuming grad_grid is contiguous\n        // thus we can\n        //   1. use index with gGrid_sW to directly compute gGrid_ptr_NDHW\n        //   2. directly assign to gGrid_ptr_NDHW[0], gGrid_ptr_NDHW[1], gGrid_ptr_NDHW[2]\n        scalar_t *gGrid_ptr_NDHW = grad_grid.data + index * gGrid_sW;\n        gGrid_ptr_NDHW[0] = static_cast<scalar_t>(0);\n        gGrid_ptr_NDHW[1] = static_cast<scalar_t>(0);\n        gGrid_ptr_NDHW[2] = static_cast<scalar_t>(0);\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SpatialSoftMaxForward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void cunn_SpatialSoftMaxForward(\n    outscalar_t *output, const scalar_t *input,\n    index_t outer_size, index_t dim_size, index_t inner_size)\n{\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n  const index_t outer_stride = inner_size * dim_size;\n  const index_t dim_stride = inner_size;\n\n  for (index_t outer_index = blockIdx.x; outer_index < outer_size; outer_index += gridDim.x) {\n    const index_t outer_offset = outer_index * outer_stride;\n    for (index_t inner_index = blockIdx.y * blockDim.y + threadIdx.y; inner_index < inner_size; inner_index += blockDim.y * gridDim.y) {\n      const index_t data_offset = outer_offset + inner_index;\n      ////////////////////////////////////////////////////////////\n      // These two blocks are really equivalent, but specializing on\n      // blockDim.x == 1 makes the kernel faster when it's unused.\n      // I didn't want to thread an extra template parameter, and nvcc\n      // seems to be smart enough to hoist the if outside of the loops.\n      ////////////////////////////////////////////////////////////\n\n      if (blockDim.x > 1) {\n        accscalar_t max_input = at::numeric_limits<accscalar_t>::lowest();\n        for (index_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n          const accscalar_t value = static_cast<accscalar_t>(input[data_offset + d * dim_stride]);\n          max_input = Max<accscalar_t>()(max_input, value);\n        }\n        max_input = spatialBlockReduceX<accscalar_t, Max>(sdata,max_input);\n\n        accscalar_t sum = 0;\n        for (index_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n          sum += std::exp(static_cast<accscalar_t>(input[data_offset + d * dim_stride])\n                 - max_input);\n        sum = spatialBlockReduceX<accscalar_t, Add>(sdata, sum);\n\n        Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(max_input, sum);\n        for (index_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n          output[data_offset + d * dim_stride] = epilogue(input[data_offset + d * dim_stride]);\n      } else {\n        accscalar_t max_input = at::numeric_limits<accscalar_t>::lowest();\n        for (index_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n          const accscalar_t value = static_cast<accscalar_t>(input[data_offset + d * dim_stride]);\n          max_input = Max<accscalar_t>()(max_input, value);\n        }\n        accscalar_t sum = 0;\n        for (index_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n          sum += std::exp(static_cast<accscalar_t>(input[data_offset + d * dim_stride])\n                 - max_input);\n        Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(max_input, sum);\n        for (index_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n          output[data_offset + d * dim_stride] = epilogue(input[data_offset + d * dim_stride]);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SpatialSoftMaxBackward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void cunn_SpatialSoftMaxBackward(\n    scalar_t *gradInput, const outscalar_t *output, const outscalar_t *gradOutput,\n    uint32_t outer_size, uint32_t dim_size, uint32_t inner_size)\n{\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n  const uint32_t outer_stride = inner_size * dim_size;\n  const uint32_t dim_stride = inner_size;\n\n  for (uint32_t outer_index = blockIdx.x; outer_index < outer_size; outer_index += gridDim.x) {\n    const uint32_t outer_offset = outer_index * outer_stride;\n    for (uint32_t inner_index = blockIdx.y * blockDim.y + threadIdx.y; inner_index < inner_size; inner_index += blockDim.y * gridDim.y) {\n      const uint32_t data_offset = outer_offset + inner_index;\n      // See the comment in forward kernel\n      if (blockDim.x > 1) {\n        accscalar_t sum = 0;\n        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x)\n          sum += gradOutput[data_offset + d * dim_stride];\n        sum = spatialBlockReduceX<accscalar_t, Add>(sdata, sum);\n\n        Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(sum);\n        for (uint32_t d = threadIdx.x; d < dim_size; d += blockDim.x) {\n          gradInput[data_offset + d * dim_stride] =\n            epilogue(gradOutput[data_offset + d * dim_stride],\n                    output[data_offset + d * dim_stride]);\n        }\n      } else {\n        accscalar_t sum = 0;\n        for (uint32_t d = 0; d < dim_size; d++)\n          sum += gradOutput[data_offset + d * dim_stride];\n\n        Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(sum);\n        for (uint32_t d = 0; d < dim_size; d++) {\n          gradInput[data_offset + d * dim_stride] =\n            epilogue(gradOutput[data_offset + d * dim_stride],\n                    output[data_offset + d * dim_stride]);\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxForwardFast",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxForwardFast(outscalar_t *output, const scalar_t *input, int classes)\n{\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n\n  // each block handles a sample in the mini-batch\n  input += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n\n  const int shift = ((uint64_t)input) % ALIGN_BYTES / sizeof(scalar_t);\n\n  // find the max\n  accscalar_t threadMax = ilpReduce<MaxFloat, ILP, scalar_t, accscalar_t>(\n    shift, input, classes, MaxFloat<scalar_t, accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n  accscalar_t max_k = blockReduceWarp<Max, accscalar_t>(sdata, threadMax,\n    Max<accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n\n  // reduce all values\n  accscalar_t threadExp = ilpReduce<SumExpfFloat, ILP, scalar_t, accscalar_t>(\n    shift, input, classes, SumExpfFloat<scalar_t, accscalar_t>(max_k), static_cast<accscalar_t>(0));\n  accscalar_t sumAll = blockReduceWarpInverse<Add, accscalar_t>(sdata, threadExp,\n    Add<accscalar_t>(), static_cast<accscalar_t>(0));\n\n  EpilogueWithMul<scalar_t, accscalar_t, outscalar_t> epilogue(max_k, sumAll);\n\n  for (int offset = threadIdx.x; offset < classes; offset += blockDim.x) {\n    output[offset] = epilogue(input[offset]);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxForward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxForward(outscalar_t *output, const scalar_t *input, int classes)\n{\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n\n  // forward pointers to batch[blockIdx.x]\n  // each block handles a sample in the mini-batch\n  input += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n\n  const int shift = ((uint64_t)input) % ALIGN_BYTES / sizeof(scalar_t);\n  const int output_shift = ((uint64_t)output) % ALIGN_BYTES / sizeof(outscalar_t);\n\n  // find the max\n  accscalar_t threadMax = ilpReduce<MaxFloat, ILP, scalar_t, accscalar_t>(\n    shift, input, classes, MaxFloat<scalar_t, accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n  accscalar_t max_k = blockReduceWarp<Max, accscalar_t>(sdata, threadMax,\n    Max<accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n\n  // reduce all values\n  accscalar_t threadExp = ilpReduce<SumExpFloat, ILP, scalar_t, accscalar_t>(\n    shift, input, classes, SumExpFloat<scalar_t, accscalar_t>(max_k), static_cast<accscalar_t>(0));\n  accscalar_t sumAll = blockReduceWarp<Add, accscalar_t>(sdata, threadExp,\n    Add<accscalar_t>(), static_cast<accscalar_t>(0));\n\n  Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(max_k, sumAll);\n\n  if (shift == output_shift) {\n    WriteFpropResultsVectorized<ILP, scalar_t, accscalar_t, outscalar_t, Epilogue>(classes, shift, input, output, epilogue);\n  } else {\n    WriteFpropResults<ILP, scalar_t, accscalar_t, outscalar_t, Epilogue>(classes, input, output, epilogue);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxForwardReg",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxForwardReg(outscalar_t *output, const scalar_t *input, index_t classes)\n{\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n\n  scalar_t reg[reg_cnt];\n\n  input += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n\n  accscalar_t threadMax = -at::numeric_limits<accscalar_t>::max();\n  accscalar_t threadExp = static_cast<accscalar_t>(0);\n\n  // Load the elements from gmem into reg, and get the max for current thread.\n  MaxFloat<scalar_t, accscalar_t> maxFunc;\n\n  #pragma unroll\n  for(int reg_idx = 0; reg_idx < reg_cnt; reg_idx ++){\n    int offset = threadIdx.x + reg_idx * blockDim.x;\n    if(offset < classes) {\n      reg[reg_idx] = input[offset];\n      threadMax = maxFunc(threadMax, reg[reg_idx]);\n    }\n  }\n\n  // Reduce to the max for block\n  accscalar_t max_k = blockReduceWarp<Max, accscalar_t>(sdata, threadMax,\n    Max<accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n\n  SumExpFloat<scalar_t, accscalar_t> sumExpFunc(max_k);\n  // reduce all values\n  #pragma unroll\n  for(int reg_idx = 0; reg_idx < reg_cnt; reg_idx ++){\n    int offset = threadIdx.x + reg_idx * blockDim.x;\n    if(offset < classes) {\n      threadExp = sumExpFunc(threadExp, reg[reg_idx]);\n    }\n  }\n  accscalar_t sumAll = blockReduceWarp<Add, accscalar_t>(sdata, threadExp,\n    Add<accscalar_t>(), static_cast<accscalar_t>(0));\n\n  Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(max_k, sumAll);\n\n  // Write back the value\n  #pragma unroll\n  for(int reg_idx = 0; reg_idx < reg_cnt; reg_idx ++){\n    int offset = threadIdx.x + reg_idx * blockDim.x;\n    if(offset < classes) {\n      output[offset] = epilogue(reg[reg_idx]);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxForwardGmem",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxForwardGmem(outscalar_t *output, const scalar_t *input, index_t classes)\n{\n  // Each thread block processes a sample in the batch\n  input += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n\n  accscalar_t threadMax = -at::numeric_limits<accscalar_t>::max();\n  accscalar_t threadExp = static_cast<accscalar_t>(0);\n\n  // The first smem segment is used to cache input values and the last\n  // segment is used for thread block reductions\n  extern __shared__ unsigned char smem[];\n  auto smem_reduction_cache = reinterpret_cast<accscalar_t*>(smem);\n\n  using LoadT = at::native::memory::aligned_vector<scalar_t, ILP>;\n  const LoadT* const input_vec_ptr = reinterpret_cast<const LoadT*>(input);\n\n  // Do the first step in max calculation:\n  MaxFloat<scalar_t, accscalar_t> maxFunc;\n  for (index_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = input_vec_ptr[offset];\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      threadMax = maxFunc(threadMax, crnt_vec.val[i]);\n    }\n  }\n\n  accscalar_t max_k = blockReduceWarp<Max, accscalar_t>(smem_reduction_cache, threadMax,\n    Max<accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n\n  // Do the second step in sum exp calculation:\n  SumExpfFloat<scalar_t, accscalar_t> sumExpFunc(max_k);\n  for (index_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = input_vec_ptr[offset];\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      threadExp = sumExpFunc(threadExp, crnt_vec.val[i]);\n    }\n  }\n\n  accscalar_t sumAll = blockReduceWarpInverse<Add, accscalar_t>(smem_reduction_cache, threadExp,\n    Add<accscalar_t>(), static_cast<accscalar_t>(0));\n\n  EpilogueWithMul<scalar_t, accscalar_t, outscalar_t> epilogue(max_k, sumAll);\n\n  using StoreT = at::native::memory::aligned_vector<outscalar_t, ILP>;\n  StoreT* output_vec_ptr = reinterpret_cast<StoreT*>(output);\n  for (index_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = input_vec_ptr[offset];\n    StoreT out_vec;\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      out_vec.val[i] = epilogue(crnt_vec.val[i]);\n    }\n    output_vec_ptr[offset] = out_vec;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxForwardSmem",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxForwardSmem(outscalar_t *output, const scalar_t *input, index_t classes)\n{\n  // Each thread block processes a sample in the batch\n  input += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n\n  accscalar_t threadMax = -at::numeric_limits<accscalar_t>::max();\n  accscalar_t threadExp = static_cast<accscalar_t>(0);\n\n  // The first smem segment is used to cache input values and the last\n  // segment is used for thread block reductions\n  extern __shared__ unsigned char smem[];\n  auto smem_input_cache = reinterpret_cast<scalar_t*>(smem);\n  auto smem_reduction_cache = reinterpret_cast<accscalar_t*>(smem +\n    classes * sizeof(scalar_t));\n\n  using LoadT = at::native::memory::aligned_vector<scalar_t, ILP>;\n  const LoadT* const input_vec_ptr = reinterpret_cast<const LoadT*>(input);\n  LoadT* const smem_input_cache_vec_ptr = reinterpret_cast<LoadT*>(smem_input_cache);\n\n  // Download inputs to shared memory while doing the first step\n  // in max calculation\n  MaxFloat<scalar_t, accscalar_t> maxFunc;\n  for (index_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = input_vec_ptr[offset];\n    smem_input_cache_vec_ptr[offset] = crnt_vec;\n\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      threadMax = maxFunc(threadMax, crnt_vec.val[i]);\n    }\n  }\n\n  accscalar_t max_k = blockReduceWarp<Max, accscalar_t>(smem_reduction_cache, threadMax,\n    Max<accscalar_t>(), -at::numeric_limits<accscalar_t>::max());\n\n  // Reload input from shared memory to compute the sum. The previous\n  // reduce has performed a __syncthreads() so the smem contents are populated.\n  SumExpFloat<scalar_t, accscalar_t> sumExpFunc(max_k);\n  for (index_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = smem_input_cache_vec_ptr[offset];\n\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      threadExp = sumExpFunc(threadExp, crnt_vec.val[i]);\n    }\n  }\n\n  accscalar_t sumAll = blockReduceWarp<Add, accscalar_t>(smem_reduction_cache, threadExp,\n    Add<accscalar_t>(), static_cast<accscalar_t>(0));\n\n  Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(max_k, sumAll);\n\n  // Use vectorized stores to save the output\n  using StoreT = at::native::memory::aligned_vector<outscalar_t, ILP>;\n  StoreT* output_vec_ptr = reinterpret_cast<StoreT*>(output);\n  for (index_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = smem_input_cache_vec_ptr[offset];\n    StoreT out_vec;\n\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      out_vec.val[i] = epilogue(crnt_vec.val[i]);\n    }\n\n    output_vec_ptr[offset] = out_vec;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxBackward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxBackward(scalar_t *gradInput, const outscalar_t *output, const outscalar_t *gradOutput, int64_t classes)\n{\n  using LoadT = at::native::memory::aligned_vector<scalar_t, ILP>;\n  using StoreT = at::native::memory::aligned_vector<outscalar_t, ILP>;\n\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n  gradInput += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n  gradOutput += static_cast<int64_t>(blockIdx.x) * classes;\n\n  const int64_t shift = ((uint64_t)gradInput) % ALIGN_BYTES / sizeof(scalar_t);\n  const int64_t output_shift = ((uint64_t)output) % ALIGN_BYTES / sizeof(outscalar_t);\n  const int64_t grad_output_shift = ((uint64_t)gradOutput) % ALIGN_BYTES / sizeof(outscalar_t);\n\n  const bool can_use_32bit_indexing = is_32bit_representable(shift) && is_32bit_representable(output_shift) && is_32bit_representable(grad_output_shift) && is_32bit_representable(classes);\n  accscalar_t threadSum;\n  if (can_use_32bit_indexing) {\n    threadSum = ilpReduce<AddFloat, ILP, outscalar_t, accscalar_t, int32_t>(\n        static_cast<int32_t>(grad_output_shift), gradOutput, classes, AddFloat<outscalar_t, accscalar_t>(), accscalar_t(0));\n  } else {\n    threadSum = ilpReduce<AddFloat, ILP, outscalar_t, accscalar_t, int64_t>(\n        grad_output_shift, gradOutput, classes, AddFloat<outscalar_t, accscalar_t>(), accscalar_t(0));\n  }\n  accscalar_t sum_k = blockReduce<Add, accscalar_t>(\n        sdata, threadSum, Add<accscalar_t>(), accscalar_t(0));\n\n  Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(sum_k);\n\n  if (shift == output_shift && shift == grad_output_shift) {\n    if (can_use_32bit_indexing) {\n      WriteBpropResultsVectorized<ILP, scalar_t, accscalar_t, outscalar_t, Epilogue, int32_t>(classes, static_cast<int32_t>(shift), gradInput, output, gradOutput, epilogue);\n    } else {\n      WriteBpropResultsVectorized<ILP, scalar_t, accscalar_t, outscalar_t, Epilogue, int64_t>(classes, shift, gradInput, output, gradOutput, epilogue);\n    }\n  } else {\n    if (can_use_32bit_indexing) {\n      WriteBpropResults<ILP, scalar_t, accscalar_t, outscalar_t, Epilogue, int32_t>(classes, gradInput, output, gradOutput, epilogue);\n    } else {\n      WriteBpropResults<ILP, scalar_t, accscalar_t, outscalar_t, Epilogue, int64_t>(classes, gradInput, output, gradOutput, epilogue);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cunn_SoftMaxBackwardSmem",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\ncunn_SoftMaxBackwardSmem(scalar_t *gradInput, const outscalar_t *output, const outscalar_t *gradOutput, int64_t classes)\n{\n  // The first smem segment is used to cache input values and the last\n  // segment is used for thread block reductions\n  extern __shared__ unsigned char smem[];\n  auto smem_input_cache = reinterpret_cast<outscalar_t*>(smem);\n  auto smem_reduction_cache = reinterpret_cast<accscalar_t*>(smem +\n    classes * sizeof(outscalar_t));\n\n  gradInput += static_cast<int64_t>(blockIdx.x) * classes;\n  output += static_cast<int64_t>(blockIdx.x) * classes;\n  gradOutput += static_cast<int64_t>(blockIdx.x) * classes;\n\n  accscalar_t threadSum = 0;\n\n  using LoadT = at::native::memory::aligned_vector<outscalar_t, ILP>;\n  const LoadT* const gradOutput_vec_ptr = reinterpret_cast<const LoadT*>(gradOutput);\n  LoadT* const smem_gradOutput_cache_vec_ptr = reinterpret_cast<LoadT*>(smem_input_cache);\n\n  // Download inputs to shared memory while doing the first step\n  // in sum calculation\n  for (int32_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = gradOutput_vec_ptr[offset];\n    smem_gradOutput_cache_vec_ptr[offset] = crnt_vec;\n\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      threadSum = threadSum + crnt_vec.val[i];\n    }\n  }\n\n  // We need a __syncthreads() here to be safe. However, blockReduceWarp's code\n  // calls a __syncthreads() before reading shared memory so we are safe.\n\n  accscalar_t sum_k = blockReduceWarp<Add, accscalar_t>(smem_reduction_cache, threadSum, Add<accscalar_t>(), accscalar_t(0));\n\n  Epilogue<scalar_t, accscalar_t, outscalar_t> epilogue(sum_k);\n\n  // Use vectorized stores to save the output\n  using StoreT = at::native::memory::aligned_vector<scalar_t, ILP>;\n  StoreT* gradInput_vec_ptr = reinterpret_cast<StoreT*>(gradInput);\n  const LoadT* const output_vec_ptr = reinterpret_cast<const LoadT*>(output);\n  for (int32_t offset = threadIdx.x; offset * ILP < classes; offset += blockDim.x) {\n    LoadT crnt_vec = smem_gradOutput_cache_vec_ptr[offset];\n    LoadT crnt_out = output_vec_ptr[offset];\n    StoreT out_vec;\n\n    #pragma unroll\n    for (int i = 0; i < ILP; ++i) {\n      out_vec.val[i] = epilogue(crnt_vec.val[i], crnt_out.val[i]);\n    }\n\n    gradInput_vec_ptr[offset] = out_vec;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "renormRowsL1",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void renormRowsL1(scalar_t* dist, long rows, long cols) {\n  extern __shared__  unsigned char my_smem[];\n  scalar_t *smem = reinterpret_cast<scalar_t *>(my_smem);\n  scalar_t zero = static_cast<scalar_t>(0);\n  scalar_t val;\n  for (int64_t row = blockIdx.x; row < rows; row += gridDim.x) {\n    scalar_t sum = static_cast<scalar_t>(0);\n    for (int64_t col = threadIdx.x; col < cols; col += blockDim.x) {\n      val = dist[row * cols + col];\n      CUDA_KERNEL_ASSERT(!(val < zero)); // ! < 0 for NaN handling\n      sum = sum + val;\n    }\n\n    sum = cuda_utils::BlockReduceSum(sum, smem);\n    if (threadIdx.x == 0) {\n      CUDA_KERNEL_ASSERT(!(val < zero)); // ! < 0 for NaN handling\n      smem[0] = sum;\n    }\n    __syncthreads();\n\n    sum = smem[0];\n    if (sum > zero) {\n      for (int64_t col = threadIdx.x; col < cols; col += blockDim.x) {\n        dist[row * cols + col] = dist[row * cols + col] / sum;\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "sampleMultinomialWithReplacement",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\nsampleMultinomialWithReplacement(PhiloxCudaState philox_args,\n                                 int totalSamples,\n                                 int64_t* dest,\n                                 int64_t distributions,\n                                 int categories,\n                                 const scalar_t* normDistPrefixSum,\n                                 const scalar_t* normDist) {\n  // At the moment, each warp computes one sample value in the binary\n  // search due to divergence. It seems possible to compute multiple\n  // values and limit divergence though later on.\n\n  auto seeds = at::cuda::philox::unpack(philox_args);\n\n  // global index formula for 2D grid of 1D blocks\n  int idx = blockIdx.y * gridDim.x * blockDim.x + blockIdx.x * blockDim.x + threadIdx.x;\n\n  curandStatePhilox4_32_10_t state;\n  curand_init(std::get<0>(seeds),\n              idx,\n              std::get<1>(seeds),\n              &state);\n\n  // The block determines the distribution for which we generate a point\n  for (int64_t curDist = blockIdx.y;\n       curDist < distributions;\n       curDist += gridDim.y) {\n    for (int sample = blockIdx.x*blockDim.x + threadIdx.x;\n         sample < totalSamples; sample += blockDim.x*gridDim.x) {\n\n      //we are losing 3 out of 4 generated numbers but it's ok\n      //this kernel is not very efficient anyway\n      auto rand = curand_uniform4(&state);\n      scalar_t r = static_cast<scalar_t>(rand.x);\n\n      // Find the bucket that a uniform sample lies in\n      int choice = binarySearchForMultinomial<scalar_t>(\n          normDistPrefixSum + curDist * categories,\n          normDist + curDist * categories,\n          categories,\n          r);\n\n      dest[curDist * totalSamples + sample] = choice;\n\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "sampleMultinomialOnce",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void sampleMultinomialOnce(\n    int64_t* dest,\n    int64_t distributions,\n    int categories,\n    const scalar_t* sampled,\n    const scalar_t* dist,\n    int stride_dist, // dist->stride(0)\n    int stride_categories // dist->stride(1)\n) {\n  extern __shared__  unsigned char my_smem[];\n  __shared__ bool found;\n  __shared__ unsigned foundPos;\n\n  accscalar_t *smem = reinterpret_cast<accscalar_t *>(my_smem);\n\n  accscalar_t accZero = static_cast<accscalar_t>(0);\n  scalar_t zero = static_cast<scalar_t>(0);\n\n  for (int64_t curDist = blockIdx.x;\n       curDist < distributions; curDist += gridDim.x) {\n    // Each block handles one distribution\n    // First pass, find the total sum of the distribution\n    accscalar_t sum = accZero;\n    scalar_t val;\n    for (int cat = threadIdx.x; cat < categories; cat += blockDim.x) {\n      val = dist[curDist * stride_dist + cat * stride_categories];\n      CUDA_KERNEL_ASSERT(!at::_isnan(val));\n      CUDA_KERNEL_ASSERT(!_isinf(val));\n      CUDA_KERNEL_ASSERT(!(val < zero));\n      sum = sum + static_cast<accscalar_t>(val);\n    }\n\n    // threadIdx.x == 0 has the sum value from this\n    sum = cuda_utils::BlockReduceSum(sum, smem);\n\n    // Broadcast sum and sample value\n    if (threadIdx.x == 0) {\n      // Make sure the sum of our distribution didn't overflow\n      CUDA_KERNEL_ASSERT(!_isinf(val));\n      CUDA_KERNEL_ASSERT(sum > accZero);\n\n      foundPos = 0;\n      smem[0] = sum;\n      smem[1] = sampled[curDist];\n    }\n    __syncthreads();\n\n    sum = smem[0];\n    scalar_t sample = static_cast<scalar_t>(smem[1]);\n    __syncthreads();\n\n    if (sum == accZero) {\n      // Choose the first element\n      if (threadIdx.x == 0) {\n        dest[curDist] = 0;\n      }\n\n      continue;\n    }\n\n    int chunks = (categories + (int)blockDim.x - 1) / blockDim.x;\n    accscalar_t prevHighProb = accZero;\n    found = false;\n\n    for (int chunk = 0; chunk < chunks && !found; ++chunk) {\n      // All threads in bounds load a value\n      int cat = chunk * blockDim.x + threadIdx.x;\n\n      accscalar_t dist_val = cat < categories ?\n                             static_cast<accscalar_t>(dist[curDist * stride_dist + cat * stride_categories]) / sum :\n                             accZero;\n\n      smem[threadIdx.x] = dist_val;\n      __syncthreads();\n\n      // Perform an inclusive prefix sum of the shared memory contents\n      for (int offset = 1; offset < blockDim.x; offset *= 2) {\n        accscalar_t val = accZero;\n\n        if (threadIdx.x >= offset) {\n          val = smem[threadIdx.x - offset] + smem[threadIdx.x];\n        }\n\n        __syncthreads();\n        if (threadIdx.x >= offset) {\n          smem[threadIdx.x] = val;\n        }\n        __syncthreads();\n      }\n\n      // Each thread will check to see if the sample falls in its\n      // bucket\n      scalar_t curBucket =\n          static_cast<scalar_t>(smem[threadIdx.x] + prevHighProb);\n      scalar_t prevBucket = static_cast<scalar_t>(\n          threadIdx.x == 0 ? prevHighProb\n                          : smem[threadIdx.x - 1] + prevHighProb);\n      bool inBucket =\n          (cat < categories) &&\n          (!(sample >= curBucket) &&\n          (sample >= prevBucket) &&\n          (dist_val > zero));\n\n      if (inBucket) {\n        // We're done; we have the sample\n        // Torch indices are 1-based\n        atomicMax(&foundPos, cat);\n        found = true;\n      }\n\n      // Store the previous scan's high value for future use\n      prevHighProb = prevHighProb + smem[blockDim.x - 1];\n\n      __syncthreads();\n    }\n\n    if (threadIdx.x == 0) {\n      if (found) {\n          dest[curDist] = foundPos;\n      } else {\n        // This should address a rare bug where we don't select a valid index. This likely occurs when\n        // due to floating point arithmetic rounding errors, our cumulative sum does not add up to 1, but\n        // and our uniform sample is greater than this value. In this case we likely have unitialized memory\n        // in dest[curDist]. So basically we will loop through the distribution and pick the largest index\n        // where the distribution is non-zero. This is obviously terribly inefficient, but due to the\n        // rarity in which this occurs, this should not be an issue.\n        for (int cat = categories - 1; cat >= 0; --cat) {\n          if (dist[curDist * stride_dist + cat * stride_categories] > zero) {\n            dest[curDist] = cat;\n            break;\n          }\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adjacent_difference_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adjacent_difference_kernel(\n    int64_t n,\n    InputIteratorT input,\n    int* output) {\n  CUDA_KERNEL_LOOP(i, n) {\n    output[i] = i > 0 ? input[i] != input[i - 1] : 0;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "scatter_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void scatter_kernel(\n    int64_t n,\n    const int64_t* input,\n    const int64_t* indices,\n    int64_t* output) {\n  CUDA_KERNEL_LOOP(i, n) {\n    output[indices[i]] = input[i];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "unique_bool_write_inverse_indices",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void unique_bool_write_inverse_indices(\n    const int numel,\n    const int *num_true_p,\n    const bool *self,\n    int64_t *inverse_indices_out) {\n  constexpr int false_idx = 0;\n  const int num_true = *num_true_p;\n  const int num_false = numel - num_true;\n  const int true_idx = num_false > 0;\n\n  CUDA_KERNEL_LOOP(i, numel) {\n    const auto value = c10::load(&self[i]);\n    inverse_indices_out[i] = value ? true_idx : false_idx;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "unique_bool_write_output",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void unique_bool_write_output(\n    const int numel,\n    const int *num_true_p,\n    bool *values_out,\n    int64_t *counts_out) {\n  constexpr int false_idx = 0;\n  const int num_true = *num_true_p;\n  const int num_false = numel - num_true;\n  const int true_idx = num_false > 0;\n\n  if (blockIdx.x == 0 && threadIdx.x == 0) {\n    if (num_false > 0) {\n      values_out[false_idx] = false;\n      counts_out[false_idx] = num_false;\n    }\n    if (num_true > 0) {\n      values_out[true_idx] = true;\n      counts_out[true_idx] = num_true;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptiveaveragepool",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptiveaveragepool(\n    const scalar_t *input, scalar_t *output,\n    int isizeT, int isizeH, int isizeW,\n    int osizeT, int osizeH, int osizeW,\n    int64_t istrideD,\n    int64_t istrideT, int64_t istrideH, int64_t istrideW,\n    int64_t offsetZ) {\n  // iterates on output pixels\n  int ot, oh, ow;\n\n  // compute offsets based on thread/block ID\n  int ostartH = blockIdx.y * blockDim.y + threadIdx.y;\n  int oendH = osizeH;\n  int ostepH = gridDim.y * blockDim.y;\n  int ostartW = threadIdx.x;\n  int oendW = osizeW;\n  int ostepW = blockDim.x;\n\n  // select output plane\n  int64_t o_plane = blockIdx.x + offsetZ;\n  ot = o_plane % osizeT; // output frame/time\n  int d = o_plane / osizeT; // slice/feature\n\n  // input frame/time range is fixed.\n  int istartT = start_index(ot, osizeT, isizeT);\n  int iendT = end_index(ot, osizeT, isizeT);\n  int kT = iendT - istartT;\n\n  // input offset by slice/feature and earliest relevant frame/time\n  const scalar_t *input_dt = input + d*istrideD + istartT*istrideT;\n  // output offset by slice/feature and frame/time\n  scalar_t *output_dt = output + o_plane*osizeH*osizeW;\n\n  // For all output pixels...\n  for (oh = ostartH; oh < oendH; oh += ostepH) {\n    int istartH = start_index(oh, osizeH, isizeH);\n    int iendH = end_index(oh, osizeH, isizeH);\n    int kH = iendH - istartH;\n\n    for (ow = ostartW; ow < oendW; ow += ostepW) {\n      int istartW = start_index(ow, osizeW, isizeW);\n      int iendW = end_index(ow, osizeW, isizeW);\n      int kW = iendW - istartW;\n\n      // Compute the average pooling from corresponding input pixels\n      const scalar_t *ptr_input = input_dt + istartH*istrideH + istartW*istrideW;\n      scalar_t *ptr_output = output_dt + oh*osizeW + ow;\n      accscalar_t sum = static_cast<accscalar_t>(0);\n\n      int it, ih, iw;\n      for (it = 0; it < kT; ++it) {\n        for (ih = 0; ih < kH; ++ih) {\n          for (iw = 0; iw < kW; ++iw) {\n            scalar_t val = ptr_input[ih*istrideH + iw*istrideW];\n            sum += static_cast<accscalar_t>(val);\n          }\n        }\n        ptr_input += istrideT; // next input frame\n      }\n      // Update output\n      const accscalar_t divide_factor = static_cast<accscalar_t>(kT * kH * kW);\n      *ptr_output = static_cast<scalar_t>(sum / divide_factor);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptiveaveragegradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptiveaveragegradinput(\n    scalar_t *gradInput, const scalar_t *gradOutput,\n    int isizeT, int isizeH, int isizeW,\n    int osizeT, int osizeH, int osizeW,\n    int64_t offsetZ)\n{\n  // iterators on input pixels\n  int it, ih, iw;\n\n  // compute offsets based on thread/block ID\n  int istartH = blockIdx.y * blockDim.y + threadIdx.y;\n  int iendH = isizeH;\n  int istepH = gridDim.y * blockDim.y;\n  int istartW = threadIdx.x;\n  int iendW = isizeW;\n  int istepW = blockDim.x;\n\n  // select input plane\n  int64_t i_plane = blockIdx.x + offsetZ;\n  it = i_plane % isizeT; // output frame/time\n  int d = i_plane / isizeT; // slice/feature\n\n  // output frame/time range is fixed.\n  int ostartT = start_index(it, isizeT, osizeT);\n  int oendT = end_index(it, isizeT, osizeT);\n\n  // gradInput offset by slice/feature and frame/time.\n  scalar_t *gradInput_dt = gradInput + i_plane*isizeH*isizeW;\n  // gradOutput offset by slice/feature and earliest relevant frame/time\n  const scalar_t *gradOutput_dt = gradOutput + (d*osizeT + ostartT)*osizeH*osizeW;\n\n  // For all input pixels...\n  for (ih = istartH; ih < iendH; ih += istepH) {\n    int ostartH = start_index(ih, isizeH, osizeH);\n    int oendH = end_index(ih, isizeH, osizeH);\n\n    for (iw = istartW; iw < iendW; iw += istepW) {\n      int ostartW = start_index(iw, isizeW, osizeW);\n      int oendW = end_index(iw, isizeW, osizeW);\n\n      // Compute the gradients from corresponding output pixels\n      scalar_t *ptr_gradInput = gradInput_dt + ih*isizeW + iw;\n      const scalar_t *ptr_gradOutput = gradOutput_dt;\n\n      // for all relevant output pixels\n      int ot, oh, ow;\n      for (ot = ostartT; ot < oendT; ++ot) {\n        int kT = end_index(ot, osizeT, isizeT) - start_index(ot, osizeT, isizeT);\n        for (oh = ostartH; oh < oendH; ++oh) {\n          int kH = end_index(oh, osizeH, isizeH) - start_index(oh, osizeH, isizeH);\n          for (ow = ostartW; ow < oendW; ++ow) {\n            int kW = end_index(ow, osizeW, isizeW) - start_index(ow, osizeW, isizeW);\n            const accscalar_t divide_factor = kW * kH * kT;\n            accscalar_t grad_delta = static_cast<accscalar_t>(ptr_gradOutput[oh*osizeW + ow] / divide_factor);\n            *ptr_gradInput += static_cast<scalar_t>(grad_delta);\n          }\n        }\n        ptr_gradOutput += osizeH*osizeW; // next output frame\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "atomicadaptiveaveragegradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void atomicadaptiveaveragegradinput(\n    scalar_t *gradInput, const scalar_t *gradOutput,\n    int isizeT, int isizeH, int isizeW,\n    int osizeT, int osizeH, int osizeW,\n    int64_t offsetZ)\n{\n  // iterators on output pixels\n  int ot, oh, ow;\n\n  // compute offsets based on thread/block ID\n  int ostartH = blockIdx.y * blockDim.y + threadIdx.y;\n  int oendH = osizeH;\n  int ostepH = gridDim.y * blockDim.y;\n  int ostartW = threadIdx.x;\n  int oendW = osizeW;\n  int ostepW = blockDim.x;\n\n  // select output plane\n  int64_t o_plane = blockIdx.x + offsetZ;\n  ot = o_plane % osizeT; // output frame/time\n  int d = o_plane / osizeT; // output slice/feature\n\n  // input frame/time range is fixed.\n  int istartT = start_index(ot, osizeT, isizeT);\n  int iendT = end_index(ot, osizeT, isizeT);\n  int kT = iendT - istartT;\n\n  // gradInput offset by slice/feature and earliest relevant frame/time\n  scalar_t *gradInput_nt = gradInput + (d*isizeT + istartT)*isizeH*isizeW;\n  // gradOutput offset by slice/feature and frame/time\n  const scalar_t *gradOutput_nt = gradOutput + o_plane*osizeH*osizeW;\n\n  // For all output pixels...\n  for (oh = ostartH; oh < oendH; oh += ostepH) {\n    int istartH = start_index(oh, osizeH, isizeH);\n    int iendH = end_index(oh, osizeH, isizeH);\n    int kH = iendH - istartH;\n\n    for (ow = ostartW; ow < oendW; ow += ostepW) {\n      int istartW = start_index(ow, osizeW, isizeW);\n      int iendW = end_index(ow, osizeW, isizeW);\n      int kW = iendW - istartW;\n\n      // Compute the gradients from corresponding input pixels\n      scalar_t *ptr_gradInput = gradInput_nt + istartH*isizeW + istartW;\n      const scalar_t *ptr_gradOutput = gradOutput_nt + oh*osizeW + ow;\n      scalar_t grad_delta = *ptr_gradOutput / kT / kH / kW;\n\n      int it, ih, iw;\n      for (it = 0; it < kT; ++it) {\n        for (ih = 0; ih < kH; ++ih) {\n          for (iw = 0; iw < kW; ++iw) {\n            gpuAtomicAddNoReturn(&(ptr_gradInput[ih*isizeW + iw]), grad_delta);\n          }\n        }\n        ptr_gradInput += isizeH*isizeW; // next input frame\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "sort_postprocess_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void sort_postprocess_kernel(\n    const scalar_t* in,\n    scalar_t* out,\n    int64_t* index,\n    const int2* i_s_ptr,\n    int nsegments,\n    int nsort) {\n  CUDA_KERNEL_LOOP(i, nsegments * nsort) {\n    int segment = i / nsort;\n    int j = i % nsort;\n\n    int offset = segment * nsort;\n    const scalar_t* in_ = in + offset;\n    scalar_t* out_ = out + offset;\n    int64_t* index_ = index + offset;\n    const int2* i_s_ptr_ = i_s_ptr + offset;\n\n    int idx = i_s_ptr_[j].y;\n    index_[j] = idx;\n    out_[j] = in_[idx];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "fill_index_and_segment_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fill_index_and_segment_kernel(\n    int2* data,\n    int numel,\n    at::cuda::detail::IntDivider<uint32_t> nsort_divider) {\n  CUDA_KERNEL_LOOP(idx, numel) {\n    auto div_mod = nsort_divider.divmod(idx);\n    auto segment = static_cast<int>(div_mod.div);\n    auto sort = static_cast<int>(div_mod.mod);\n    data[idx] = int2{segment, sort};\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "fill_reverse_indices_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fill_reverse_indices_kernel(\n    int64_t* data,\n    int numel,\n    at::cuda::detail::IntDivider<uint32_t> nsort_divider) {\n  CUDA_KERNEL_LOOP(idx, numel) {\n    data[idx] = nsort_divider.mod(idx);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "fractional_max_pool2d_out_cuda_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fractional_max_pool2d_out_cuda_frame(\n  PackedTensorAccessor<scalar_t, 4> output,\n  PackedTensorAccessor<int64_t, 4> indices,\n  PackedTensorAccessor<const scalar_t, 4> input,\n  PackedTensorAccessor<const scalar_t, 3> samples,\n  int poolSizeH, int poolSizeW) {\n\n  using accscalar_t = at::acc_type<scalar_t, /*is_cuda=*/true>;\n\n  int ourOutputPoint = threadIdx.x + blockIdx.x * blockDim.x;\n  int plane = blockIdx.y;\n  int batch = blockIdx.z;\n\n  // Each thread generates a specific output point\n  if (ourOutputPoint < output.size(2) * output.size(3)) {\n    int outputW = ourOutputPoint % output.size(3);\n    int outputH = ourOutputPoint / output.size(3);\n\n    int poolW = get_interval<scalar_t, accscalar_t>(\n      static_cast<accscalar_t>(samples[batch][plane][0]),\n        outputW, input.size(3), output.size(3), poolSizeW);\n    int poolH = get_interval<scalar_t, accscalar_t>(\n      static_cast<accscalar_t>(samples[batch][plane][1]),\n        outputH, input.size(2), output.size(2), poolSizeH);\n\n    scalar_t maxVal = at::numeric_limits<scalar_t>::lower_bound();\n    int maxIndex = poolH * input.size(3) + poolW;\n\n    for (int h = poolH; h < poolH + poolSizeH; ++h) {\n      if (poolSizeW < 2 || poolSizeW > 7) {\n        for (int w = poolW; w < poolW + poolSizeW; ++w) {\n          scalar_t val = input[batch][plane][h][w];\n          // for consistency with THNN, favor the first max\n          if (val > maxVal || at::_isnan(val)) {\n            maxIndex = h * input.size(3) + w;\n            maxVal = val;\n          }\n        }\n      } else {\n        for (int i = 0; i < poolSizeW; ++i) {\n          int w = i + poolW;\n          scalar_t val = input[batch][plane][h][w];\n          // for consistency with THNN, favor the first max\n          if (val > maxVal || at::_isnan(val)) {\n            maxIndex = h * input.size(3) + w;\n            maxVal = val;\n          }\n        }\n      }\n    }\n\n    indices[batch][plane][outputH][outputW] = maxIndex;\n    output[batch][plane][outputH][outputW] = maxVal;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "fractional_max_pool2d_backward_out_cuda_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fractional_max_pool2d_backward_out_cuda_frame(\n  PackedTensorAccessor<scalar_t, 4> gradInput,\n  PackedTensorAccessor<const scalar_t, 4> gradOutput,\n  PackedTensorAccessor<const int64_t, 4> indices) {\n  // Output (h, w) point that this thread is responsible for\n  int ourOutputPoint = threadIdx.x + blockIdx.x * blockDim.x;\n  int plane = blockIdx.y;\n  int batch = blockIdx.z;\n\n  // Each thread generates a specific output point\n  if (ourOutputPoint < gradOutput.size(2) *\n    gradOutput.size(3)) {\n    int outputW = ourOutputPoint % gradOutput.size(3);\n    int outputH = ourOutputPoint / gradOutput.size(3);\n\n    int index = indices[batch][plane][outputH][outputW];\n    CUDA_KERNEL_ASSERT(index >= 0);\n    int inputW = index % gradInput.size(3);\n    int inputH = index / gradInput.size(3);\n    CUDA_KERNEL_ASSERT(inputH < gradInput.size(2));\n\n    gpuAtomicAddNoReturn(\n      &gradInput[batch][plane][inputH][inputW],\n      gradOutput[batch][plane][outputH][outputW]\n    );\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "_elementwise_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _elementwise_kernel(int total_n_elems, func_t f) {\n  constexpr int total_work_block = n_threads * n_elems_per_thread;\n  int idx = total_work_block * blockIdx.x + threadIdx.x;\n\n  #pragma unroll\n  for (int i = 0; i < n_elems_per_thread; ++i) {\n    if (idx < total_n_elems) {\n      f(idx);\n      idx += n_threads;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "gatherKthValue",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void gatherKthValue(\n    cuda::detail::TensorInfo<const scalar_t, index_t> input,\n    index_t inputSliceSize,\n    index_t k,\n    index_t numInputSlices,\n    index_t inputWithinSliceStride,\n    cuda::detail::TensorInfo<scalar_t, index_t> kthValue,\n    cuda::detail::TensorInfo<int64_t, index_t> indices) {\n  // Indices are limited to integer fp precision, so counts can fit in\n  // int32, regardless of index_t\n  __shared__ int smem[C10_WARP_SIZE]; // one per each warp, up to warp limit\n\n  index_t slice = getLinearBlockId<index_t>();\n  if (slice >= numInputSlices) {\n    return;\n  }\n\n  // Find the start offset for our slice\n  index_t sliceStartIndex =\n      cuda::detail::IndexToOffset<const scalar_t, index_t, Dim>::get(slice, input);\n  index_t kthValueSliceStartIndex =\n      cuda::detail::IndexToOffset<scalar_t, index_t, Dim>::get(slice, kthValue);\n  index_t indicesSliceStartIndex =\n      cuda::detail::IndexToOffset<int64_t, index_t, Dim>::get(slice, indices);\n\n  const scalar_t* inputSliceStart = &input.data[sliceStartIndex];\n  scalar_t* kthValueSliceStart = &kthValue.data[kthValueSliceStartIndex];\n  int64_t* indicesSliceStart = &indices.data[indicesSliceStartIndex];\n\n  // Find the k-th highest element in our input\n  scalar_t kValue = static_cast<scalar_t>(0);\n  radixSelect<\n      scalar_t,\n      typename TopKTypeConfig<scalar_t>::RadixType,\n      index_t>(\n      inputSliceStart,\n      k,\n      false,\n      inputSliceSize,\n      inputWithinSliceStride,\n      smem,\n      &kValue);\n\n  // Find the index of the k-th highest element\n  index_t kValueIndex = 0;\n  bool foundKValue = false;\n\n  for (index_t i = threadIdx.x; i < inputSliceSize; i += blockDim.x) {\n    bool inRange = (i < inputSliceSize);\n    scalar_t v = inRange ? doLdg(&inputSliceStart[i * inputWithinSliceStride])\n                         : static_cast<scalar_t>(0);\n    bool isKValue = inRange &&\n        ((v == kValue) || (at::_isnan(v) && at::_isnan(kValue)));\n    if (isKValue) {\n      kValueIndex = i;\n      foundKValue = true;\n      break;\n    }\n  }\n\n  if (foundKValue) {\n    kthValueSliceStart[0] = kValue;\n    indicesSliceStart[0] = kValueIndex;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "gatherMedian",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void gatherMedian(\n    cuda::detail::TensorInfo<scalar_t, index_t> values,\n    cuda::detail::TensorInfo<int64_t, index_t> indices,\n    cuda::detail::TensorInfo<const scalar_t, index_t> input,\n    index_t inputSliceSize,\n    index_t numInputSlices,\n    index_t inputWithinSliceStride,\n    bool ignore_nan) {\n  // Shared memory for the subroutine RadixSelect. Note that RadixSelect converts the\n  // floating point type to int with the same relative ordering.\n  __shared__ int smem[C10_WARP_SIZE]; // one per each warp, up to warp limit\n\n  index_t slice = getLinearBlockId<index_t>();\n  if (slice >= numInputSlices) {\n    return;\n  }\n\n  // Finds the start offset for our slice\n  index_t valuesSliceStartIndex =\n      cuda::detail::IndexToOffset<scalar_t, index_t, Dim>::get(slice, values);\n  index_t indicesSliceStartIndex =\n      cuda::detail::IndexToOffset<int64_t, index_t, Dim>::get(slice, indices);\n  index_t inputSliceStartIndex =\n      cuda::detail::IndexToOffset<const scalar_t, index_t, Dim>::get(slice, input);\n\n  scalar_t* valuesSliceStart = &values.data[valuesSliceStartIndex];\n  int64_t* indicesSliceStart = &indices.data[indicesSliceStartIndex];\n  const scalar_t* inputSliceStart = &input.data[inputSliceStartIndex];\n\n  index_t nan_count = 0;\n  for (index_t i = threadIdx.x; i < inputSliceSize; i += blockDim.x) {\n    scalar_t val = doLdg(&inputSliceStart[i * inputWithinSliceStride]);\n    nan_count += at::_isnan(val) ? 1 : 0;\n  }\n\n  // Counts number of nan values\n  // This code performs a parallel sum reduction (not the most efficient code)\n  __shared__ int64_t num_nan;\n  if (threadIdx.x == 0) {\n    num_nan = 0;\n  }\n  __syncthreads();\n  if (nan_count > 0) {\n    gpuAtomicAddNoReturn(&num_nan, nan_count);\n  }\n  __syncthreads();\n\n  // For torch.median, if we found nan set k to last index so the computed value\n  // is nan, otherwise set k to the middle element of the non-nan values\n  index_t k = (!ignore_nan && num_nan > 0) ? inputSliceSize - 1\n                                           : (inputSliceSize - num_nan - 1) / 2;\n\n  // Find the median\n  scalar_t median = static_cast<scalar_t>(0);\n  radixSelect<\n      scalar_t,\n      typename TopKTypeConfig<scalar_t>::RadixType,\n      index_t>(\n      inputSliceStart,\n      k + 1,\n      false,\n      inputSliceSize,\n      inputWithinSliceStride,\n      smem,\n      &median);\n\n  valuesSliceStart[0] = median;\n\n  // Find the index of the median value in the slice\n  for (index_t i = threadIdx.x; i < inputSliceSize; i += blockDim.x) {\n    scalar_t val = doLdg(&inputSliceStart[i * inputWithinSliceStride]);\n    if (val == median || (at::_isnan(val) && at::_isnan(median))) {\n      indicesSliceStart[0] = i;\n      break;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "CatArrayBatchedCopy",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void CatArrayBatchedCopy(\n    T* output,\n    CatArrInputTensorMetadata<T, IndexType, batch_size, stride_size> inputs,\n    TensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> os,\n    const int concatDim,\n    IndexType dimStride) {\n\n    IndexType tid = blockIdx.x * blockDim.x + threadIdx.x;\n    IndexType nElements = inputs.nElements[blockIdx.y];\n    TensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> ins = stride_size > 1 ? inputs.tensorStride[blockIdx.y] : inputs.tensorStride[0];\n    bool isContig = inputs.isContiguous[blockIdx.y];\n\n    if(tid >= nElements) return;\n\n    const T* data = inputs.input[blockIdx.y];\n    IndexType offset = inputs.offset[blockIdx.y];\n    IndexType dimSize = inputs.dimSize[blockIdx.y];\n    IndexType dataOffset = offset * dimStride;\n\n    IndexType stride = gridDim.x * blockDim.x;\n\n    while( tid < nElements){\n      IndexType elementOffset = CatArrIndexToOffset<IndexType, Dims>::compute(\n                    os.tensorSize, os.tensorStride, dimSize, concatDim, tid);\n      if (isContig) {\n        output[dataOffset + elementOffset] = data[tid];\n      } else {\n        IndexType inElementOffset = CatArrIndexToOffset<IndexType, Dims>::compute(\n                    ins.tensorSize, ins.tensorStride, dimSize, concatDim, tid);\n        output[dataOffset + elementOffset] = data[inElementOffset];\n      }\n    tid += stride;\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "CatArrayBatchedCopy_contig",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void CatArrayBatchedCopy_contig(\n    T* output,\n    CatArrInputTensorMetadata<T, IndexType, batch_size, stride_size> inputs,\n    TensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> os,\n    const int concatDim,\n    IndexType dimStride) {\n\n    IndexType tid = blockIdx.x * blockDim.x + threadIdx.x;\n    IndexType nElements = inputs.nElements[blockIdx.y];\n\n    if(tid >= nElements) return;\n\n    const T* data = inputs.input[blockIdx.y];\n    IndexType offset = inputs.offset[blockIdx.y];\n    IndexType dimSize = inputs.dimSize[blockIdx.y];\n    IndexType dataOffset = offset * dimStride;\n\n    IndexType stride = gridDim.x * blockDim.x;\n\n    while( tid < nElements){\n      IndexType elementOffset = CatArrIndexToOffset<IndexType, Dims>::compute(\n                    os.tensorSize, os.tensorStride, dimSize, concatDim, tid);\n      output[dataOffset + elementOffset] = data[tid];\n      tid += stride;\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "CatArrayBatchedCopy_alignedK_contig",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void CatArrayBatchedCopy_alignedK_contig(\n    T* output,\n    CatArrInputTensorMetadata<T, IndexType, batch_size, stride_size> inputs,\n    TensorSizeStride<IndexType, CAT_ARRAY_MAX_INPUT_DIMS> os,\n    const int concatDim,\n    IndexType dimStride) {\n\n    // This kernel tries to use aligned_vec_load_bytes*8 bit loads\n    // Special case 2-byte types to use 8-byte vec loads to reduce register pressure\n    // The below lambda is to allow cc compiler to pass kILP>0 checks for large types (e.g. ComplexDouble, 16 bytes)\n    constexpr int kILP = aligned_vec_load_bytes / sizeof(T) > 0 ? aligned_vec_load_bytes / sizeof(T) : ALIGNED_VEC_LOAD_BYTES_16/sizeof(T);\n\n    IndexType inputOffset = (blockIdx.x * blockDim.x + threadIdx.x) * kILP;\n    IndexType inputStride = gridDim.x * blockDim.x * kILP;\n\n    IndexType nElements = inputs.nElements[blockIdx.y];\n    if (inputOffset >= nElements) {\n      return;\n    }\n\n    const T* data = inputs.input[blockIdx.y];\n    IndexType offset = inputs.offset[blockIdx.y];\n    IndexType dimSize = inputs.dimSize[blockIdx.y];\n    IndexType dataOffset = offset * dimStride;\n\n    IndexType v_elementOffset[kILP];\n    T reg_data[kILP];\n\n    while (inputOffset + kILP <= nElements) {\n      for (int i = 0; i < kILP; ++i) {\n        v_elementOffset[i] = CatArrIndexToOffset<IndexType, Dims>::compute(os.tensorSize,\n          os.tensorStride, dimSize, concatDim, inputOffset + i);\n      }\n\n      using LT = at::native::memory::aligned_vector<T, kILP>;\n      ((LT*)reg_data)[0] = const_cast<LT*>((LT*)(data + inputOffset))[0];\n\n      #pragma unroll\n      for (int i = 0; i < kILP; ++i) {\n        output[dataOffset + v_elementOffset[i]] = reg_data[i];\n      }\n\n      inputOffset += inputStride;\n    }\n\n    // Handle remaining tail in case nElements does not divide\n    // exactly to kILP\n\n    while (inputOffset < nElements) {\n      v_elementOffset[0] = CatArrIndexToOffset<IndexType, Dims>::compute(os.tensorSize,\n        os.tensorStride, dimSize, concatDim, inputOffset);\n      output[dataOffset + v_elementOffset[0]] = data[inputOffset];\n      inputOffset++;\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "glu_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void glu_backward_kernel(\n    int numel,\n    scalar_t* gI,\n    const scalar_t* I,\n    const scalar_t* gO,\n    OffsetCalc offset_calculator,\n    int64_t gI_byte_offset,\n    int64_t I_byte_offset) {\n  using opmath_t = at::opmath_type<scalar_t>;\n\n  const uint32_t linear_index = blockIdx.x * blockDim.x + threadIdx.x;\n  if (linear_index >= numel) {\n    return;\n  }\n  const auto offsets = offset_calculator.get(linear_index);\n\n  // We explicitly iterate over the first half of the input tensor, and\n  // gI_byte_offset and I_byte_offset are the offsets to access the\n  // corresponding index in the second half of the tensor.\n  const opmath_t a = I[offsets[1]];\n  const opmath_t b = *byte_offset(I + offsets[1], I_byte_offset);\n  const opmath_t gO_val = gO[offsets[2]];\n\n  const auto one = opmath_t(1);\n  const opmath_t sigmoid = one / (one + std::exp(-b));\n\n  auto* gA = gI + offsets[0];\n  *gA = sigmoid * gO_val;\n\n  auto* gB = byte_offset(gA, gI_byte_offset);\n  *gB = (one - sigmoid) * sigmoid * gO_val * a;\n}",
      "disabled": true
    },
    {
      "kernel_name": "lstm_cell_forward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void lstm_cell_forward(\n            TensorInfo<scalar_t, index_type> input,\n            TensorInfo<scalar_t, index_type> hidden,\n            TensorInfo<scalar_t, index_type> bias1,\n            TensorInfo<scalar_t, index_type> bias2,\n            TensorInfo<scalar_t, index_type> _cx,\n            TensorInfo<scalar_t, index_type> _hy,\n            TensorInfo<scalar_t, index_type> _cy,\n            TensorInfo<scalar_t, index_type> workspace,\n            index_type hsz,\n            index_type totalElements) {\n    bool has_bias = bias1.data != nullptr;\n    for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n       linearIndex < totalElements;\n       linearIndex += gridDim.x * blockDim.x) {\n      index_type offset = (linearIndex/hsz)*4*hsz+linearIndex%hsz;\n\n      scalar_t iig = DEVICE_LINEAR_GET(input, offset+0*hsz);\n      scalar_t ifg = DEVICE_LINEAR_GET(input, offset+1*hsz);\n      scalar_t icg = DEVICE_LINEAR_GET(input, offset+2*hsz);\n      scalar_t iog = DEVICE_LINEAR_GET(input, offset+3*hsz);\n\n      scalar_t hig = DEVICE_LINEAR_GET(hidden, offset+0*hsz);\n      scalar_t hfg = DEVICE_LINEAR_GET(hidden, offset+1*hsz);\n      scalar_t hcg = DEVICE_LINEAR_GET(hidden,  offset+2*hsz);\n      scalar_t hog = DEVICE_LINEAR_GET(hidden,  offset+3*hsz);\n\n      scalar_t* wig = &DEVICE_LINEAR_GET(workspace, offset+0*hsz);\n      scalar_t* wfg = &DEVICE_LINEAR_GET(workspace, offset+1*hsz);\n      scalar_t* wcg = &DEVICE_LINEAR_GET(workspace, offset+2*hsz);\n      scalar_t* wog = &DEVICE_LINEAR_GET(workspace, offset+3*hsz);\n\n      scalar_t cx = DEVICE_LINEAR_GET(_cx, linearIndex);\n\n      scalar_t* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n      scalar_t* cy = &DEVICE_LINEAR_GET(_cy, linearIndex);\n\n      scalar_t b1i, b1f, b1c, b1o;\n      scalar_t b2i, b2f, b2c, b2o;\n\n      if (has_bias) {\n        b1i = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 0 * hsz);\n        b1f = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 1 * hsz);\n        b1c = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 2 * hsz);\n        b1o = DEVICE_BIAS_GET(bias1, linearIndex % hsz + 3 * hsz);\n\n        b2i = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 0 * hsz);\n        b2f = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 1 * hsz);\n        b2c = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 2 * hsz);\n        b2o = DEVICE_BIAS_GET(bias2, linearIndex % hsz + 3 * hsz);\n      } else {\n#ifndef THC_REAL_IS_HALF\n        b1i = 0.0; b1f = 0.0; b1c = 0.0; b1o = 0.0;\n        b2i = 0.0; b2f = 0.0; b2c = 0.0; b2o = 0.0;\n#else\n        b1i = F2H(0.0); b1f = F2H(0.0); b1c = F2H(0.0); b1o = F2H(0.0);\n        b2i = F2H(0.0); b2f = F2H(0.0); b2c = F2H(0.0); b2o = F2H(0.0);\n#endif\n      }\n\n      accscalar_t ig, fg, cg, og;\n      accscalar_t f_hy, f_cy;\n\n      ig = sigmoid(H2F(iig) + H2F(hig) + H2F(b1i) + H2F(b2i));\n      fg = sigmoid(H2F(ifg) + H2F(hfg) + H2F(b1f) + H2F(b2f));\n      cg = ::tanh(H2F(icg) + H2F(hcg) + H2F(b1c) + H2F(b2c));\n      og = sigmoid(H2F(iog) + H2F(hog) + H2F(b1o) + H2F(b2o));\n\n      f_cy = (fg * H2F(cx)) + (ig * cg);\n      f_hy = og * ::tanh(f_cy);\n\n      *hy = F2H(f_hy);\n      *cy = F2H(f_cy);\n\n      //SAVE FOR BACKWARDS\n      //Also need cy and cx but can be saved easily in python\n      *wig = F2H(ig);\n      *wfg = F2H(fg);\n      *wcg = F2H(cg);\n      *wog = F2H(og);\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "lstm_cell_backward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void lstm_cell_backward(\n              TensorInfo<scalar_t, index_type> storage,\n              TensorInfo<scalar_t, index_type> gradInGates,\n              TensorInfo<scalar_t, index_type> _cx,\n              TensorInfo<scalar_t, index_type> _cy,\n              TensorInfo<scalar_t, index_type> gradoutput,\n              TensorInfo<scalar_t, index_type> gradoutputcell,\n              TensorInfo<scalar_t, index_type> gradInputCx,\n              index_type hsz,\n              index_type totalElements) {\n  bool has_gradoutput = gradoutput.data != nullptr;\n  bool has_gradoutputcell = gradoutputcell.data != nullptr;\n  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n       linearIndex < totalElements;\n       linearIndex += gridDim.x * blockDim.x) {\n    index_type offset = (linearIndex/hsz)*4*hsz+linearIndex%hsz;\n\n    scalar_t ig = DEVICE_LINEAR_GET(storage, offset+0*hsz);\n    scalar_t fg = DEVICE_LINEAR_GET(storage, offset+1*hsz);\n    scalar_t cg = DEVICE_LINEAR_GET(storage, offset+2*hsz);\n    scalar_t og = DEVICE_LINEAR_GET(storage, offset+3*hsz);\n\n    scalar_t* ih = &DEVICE_LINEAR_GET(gradInGates, offset+0*hsz);\n    scalar_t* fh = &DEVICE_LINEAR_GET(gradInGates, offset+1*hsz);\n    scalar_t* ch = &DEVICE_LINEAR_GET(gradInGates, offset+2*hsz);\n    scalar_t* oh = &DEVICE_LINEAR_GET(gradInGates, offset+3*hsz);\n\n    //will return hidden grads here\n    scalar_t cx = DEVICE_LINEAR_GET(_cx, linearIndex);\n    scalar_t cy = DEVICE_LINEAR_GET(_cy, linearIndex);\n\n    scalar_t* gi = &DEVICE_LINEAR_GET(gradInputCx, linearIndex);\n\n    accscalar_t go  = has_gradoutput ? H2F(DEVICE_LINEAR_GET(gradoutput, linearIndex)) : 0.f;\n    accscalar_t goc = has_gradoutputcell ? H2F(DEVICE_LINEAR_GET(gradoutputcell, linearIndex)) : 0.f;\n\n    accscalar_t gcx = ::tanh(H2F(cy));\n\n    accscalar_t gog = go * gcx;\n    gcx = go * H2F(og) * (1 - gcx*gcx) + goc;\n\n    accscalar_t gig = gcx * H2F(cg);\n    accscalar_t gfg = gcx * H2F(cx);\n    accscalar_t gcg = gcx * H2F(ig);\n\n    gcx = gcx * H2F(fg);\n\n    gig = gig * (1-H2F(ig)) * H2F(ig);\n    gfg = gfg * (1-H2F(fg)) * H2F(fg);\n    gcg = gcg * (1-H2F(cg)*H2F(cg));\n    gog = gog * (1-H2F(og)) * H2F(og);\n\n    *ih = F2H(gig);\n    *fh = F2H(gfg);\n    *ch = F2H(gcg);\n    *oh = F2H(gog);\n\n    *gi = F2H(gcx);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "gru_cell_forward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void gru_cell_forward(\n            TensorInfo<scalar_t, index_type> Input,\n            TensorInfo<scalar_t, index_type> Hidden,\n            TensorInfo<scalar_t, index_type> Bias1,\n            TensorInfo<scalar_t, index_type> Bias2,\n            TensorInfo<scalar_t, index_type> _hx,\n            TensorInfo<scalar_t, index_type> _hy,\n            TensorInfo<scalar_t, index_type> storage,\n            index_type hsz,\n            index_type totalElements) {\n  bool has_bias = Bias1.data != nullptr;\n  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n       linearIndex < totalElements;\n       linearIndex += gridDim.x * blockDim.x) {\n      index_type offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;\n\n      scalar_t ir = DEVICE_LINEAR_GET(Input, offset+0*hsz);\n      scalar_t ii = DEVICE_LINEAR_GET(Input, offset+1*hsz);\n      scalar_t in = DEVICE_LINEAR_GET(Input, offset+2*hsz);\n      scalar_t hr = DEVICE_LINEAR_GET(Hidden,offset+0*hsz);\n      scalar_t hi = DEVICE_LINEAR_GET(Hidden,offset+1*hsz);\n      scalar_t hn = DEVICE_LINEAR_GET(Hidden,  offset+2*hsz);\n\n      scalar_t hx = DEVICE_LINEAR_GET(_hx, linearIndex);\n      scalar_t* hy = &DEVICE_LINEAR_GET(_hy, linearIndex);\n\n      scalar_t b1r, b1i, b1n, b2r, b2i, b2n;\n\n      if (has_bias) {\n        b1r = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+0*hsz);\n        b1i = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+1*hsz);\n        b1n = DEVICE_BIAS_GET(Bias1, linearIndex%hsz+2*hsz);\n\n        b2r = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+0*hsz);\n        b2i = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+1*hsz);\n        b2n = DEVICE_BIAS_GET(Bias2, linearIndex%hsz+2*hsz);\n      } else {\n#ifndef THC_REAL_IS_HALF\n        b1r = 0.0; b1i = 0.0; b1n = 0.0;\n        b2r = 0.0; b2i = 0.0; b2n = 0.0;\n#else\n        b1r = F2H(0.0); b1i = F2H(0.0); b1n = F2H(0.0);\n        b2r = F2H(0.0); b2i = F2H(0.0); b2n = F2H(0.0);\n#endif\n      }\n\n      offset = (linearIndex/hsz)*5*hsz+linearIndex%hsz;\n\n      accscalar_t rg, ig, ng;\n\n      rg = sigmoid(H2F(ir) + H2F(hr) + H2F(b1r) + H2F(b2r));\n      ig = sigmoid(H2F(ii) + H2F(hi) + H2F(b1i) + H2F(b2i));\n\n      ng = H2F(in) + H2F(b1n) + rg*( H2F(hn)+H2F(b2n) );\n      ng = ::tanh(ng);\n      *hy = F2H( ng + ig * ( H2F(hx)-ng ) );\n\n      //SAVE FOR BACKWARDS\n      DEVICE_LINEAR_GET(storage, offset+0*hsz) = F2H(rg);\n      DEVICE_LINEAR_GET(storage, offset+1*hsz) = F2H(ig);\n      DEVICE_LINEAR_GET(storage, offset+2*hsz) = F2H(ng);\n      DEVICE_LINEAR_GET(storage, offset+3*hsz) = hx;\n      DEVICE_LINEAR_GET(storage, offset+4*hsz) = F2H(H2F(hn) + H2F(b2n));\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "gru_cell_backward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void gru_cell_backward(\n             TensorInfo<scalar_t, index_type> gradInInput,\n             TensorInfo<scalar_t, index_type> gradInHidden,\n             TensorInfo<scalar_t, index_type> gradOutput,\n             TensorInfo<scalar_t, index_type> gradInputHx,\n             TensorInfo<scalar_t, index_type> storage,\n             index_type hsz,\n             index_type totalElements) {\n  for (index_type linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n       linearIndex < totalElements;\n       linearIndex += gridDim.x * blockDim.x) {\n    index_type offset = (linearIndex/hsz)*5*hsz+linearIndex%hsz;\n\n    scalar_t rg = DEVICE_LINEAR_GET(storage, offset+0*hsz);\n    scalar_t ig = DEVICE_LINEAR_GET(storage, offset+1*hsz);\n    scalar_t ng = DEVICE_LINEAR_GET(storage, offset+2*hsz);\n    scalar_t hx = DEVICE_LINEAR_GET(storage, offset+3*hsz);\n    scalar_t hn = DEVICE_LINEAR_GET(storage, offset+4*hsz);\n\n    scalar_t go = DEVICE_LINEAR_GET(gradOutput, linearIndex);\n\n    offset = (linearIndex/hsz)*3*hsz+linearIndex%hsz;\n\n    accscalar_t gig = H2F(go)*( H2F(hx)-H2F(ng) )*( 1-H2F(ig) )*H2F(ig);\n    accscalar_t ghx = H2F(go)*H2F(ig);\n    accscalar_t gin = H2F(go)*( 1-H2F(ig) )*( 1-H2F(ng)*H2F(ng) );\n    accscalar_t ghn = gin * H2F(rg);\n    accscalar_t grg = gin *H2F(hn)*( 1-H2F(rg) )*H2F(rg);\n\n    DEVICE_LINEAR_GET(gradInInput, offset+0*hsz) = F2H(grg);\n    DEVICE_LINEAR_GET(gradInInput, offset+1*hsz) = F2H(gig);\n    DEVICE_LINEAR_GET(gradInInput, offset+2*hsz) = F2H(gin);\n\n    DEVICE_LINEAR_GET(gradInHidden, offset+0*hsz) = F2H(grg);\n    DEVICE_LINEAR_GET(gradInHidden, offset+1*hsz) = F2H(gig);\n    DEVICE_LINEAR_GET(gradInHidden, offset+2*hsz) = F2H(ghn);\n    DEVICE_LINEAR_GET(gradInputHx, linearIndex) = F2H(ghx);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "lpmax_cleanup",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void lpmax_cleanup(\n    const T* output_per_tensor,\n    TensorListAddresses addr_struct,\n    int max_chunks_per_tensor) {\n  __shared__ T vals[512];\n  const T* output_this_tensor =\n      output_per_tensor + blockIdx.x * max_chunks_per_tensor;\n  T val = std::numeric_limits<T>::lowest();\n  for (size_t i = threadIdx.x; i < max_chunks_per_tensor; i += blockDim.x) {\n    val = max_propagate_nan(val, output_this_tensor[i]);\n  }\n  T final_val = at::native::cuda_utils::BlockReduceMax(val, vals);\n  if (threadIdx.x == 0) {\n    *(T*)addr_struct.addresses[blockIdx.x] = final_val;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "lpnorm_cleanup",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void lpnorm_cleanup(\n    const out_opmath_t* output_per_tensor,\n    TensorListAddresses addr_struct,\n    int max_chunks_per_tensor) {\n  __shared__ out_opmath_t vals[512];\n\n  const out_opmath_t* output_this_tensor =\n      output_per_tensor + blockIdx.x * max_chunks_per_tensor;\n  out_opmath_t val = 0;\n  for (size_t i = threadIdx.x; i < max_chunks_per_tensor; i += blockDim.x) {\n    if constexpr (norm_type == NormType::LInf) {\n      val = max_propagate_nan(val, output_this_tensor[i]);\n    } else {\n      val += output_this_tensor[i];\n    }\n  }\n  out_opmath_t final_val =\n      norm_type == NormType::L1 || norm_type == NormType::L2\n      ? at::native::cuda_utils::BlockReduceSum<out_opmath_t>(val, vals)\n      : at::native::cuda_utils::BlockReduceMax(val, vals);\n  if (threadIdx.x == 0) {\n    *(out_t*)addr_struct.addresses[blockIdx.x] =\n        norm_type == NormType::L1 || norm_type == NormType::LInf\n        ? final_val\n        : ::sqrt(final_val);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "krn_partials_per_segment",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid krn_partials_per_segment(index_t *ret, const index_t *segment_offsets,\n                              const int64_t *num_of_segments_ptr, int64_t numel) {\n  int64_t num_of_segments = *num_of_segments_ptr;\n  const int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if(id < num_of_segments) {\n    const int64_t idx_start = segment_offsets[id];\n    const int64_t idx_end = (id == num_of_segments-1)?numel:segment_offsets[id+1];\n    const int64_t size = idx_end - idx_start;\n    ret[id] = ceil_div(size, NROWS_PER_THREAD);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "krn_partial_segment_offset",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid krn_partial_segment_offset(\n        index_t *ret,\n        const index_t *partials_per_segment,\n        const index_t *partials_per_segment_offset,\n        const index_t *segment_offsets,\n        const int64_t *num_of_segments_ptr) {\n  int64_t num_of_segments = *num_of_segments_ptr;\n  const int id = blockIdx.x * blockDim.x + threadIdx.x;\n  if(id < num_of_segments) {\n    index_t idx = partials_per_segment_offset[id];\n    const index_t num_partials = partials_per_segment[id];\n    const index_t segment_offset = segment_offsets[id];\n    for (int64_t i=0; i<num_partials; ++i) {\n      ret[idx++] = segment_offset + i * NROWS_PER_THREAD;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "compute_grad_weight_bags",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void compute_grad_weight_bags(\n    const index_t *indices, const scalar_t *gradOutput,\n    const index_t *offset2bag, const index_t *count, ptrdiff_t numel,\n    int64_t stride, int mode_mean, const index_t *bag_size,\n    const scalar_t* per_sample_weights, int64_t per_sample_weights_stride,\n    const index_t* segment_offsets, const int64_t *num_of_segments_ptr,\n    acc_type<scalar_t, true> *grad_weight_per_segment,\n    const int64_t stride_warped) {\n\n  int64_t num_of_segments = *num_of_segments_ptr;\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int id = gid / stride_warped;\n  const int startFeature = gid % stride_warped;\n  if (startFeature >= stride) {\n    return;\n  }\n  if (id >= num_of_segments) {\n    return;\n  }\n  const int idx_begin = segment_offsets[id];\n  const int idx_end = (id == num_of_segments-1)?numel:segment_offsets[id+1];\n\n  acc_type<scalar_t, true> weight = 0;\n  for (int idx=idx_begin; idx < idx_end; ++idx) {\n    const int origRow = indices[idx];\n    const int seq_number = offset2bag[origRow];\n    const int gradOutputRow = seq_number * stride;\n\n    acc_type<scalar_t, true> scale = count ? 1.0 / count[idx] : 1.0;\n    if (per_sample_weights) {\n      scale *= per_sample_weights[origRow * per_sample_weights_stride];\n    }\n\n    acc_type<scalar_t, true> gradient = gradOutput[gradOutputRow + startFeature];\n    if (mode_mean) {\n      gradient /= bag_size[seq_number];\n    }\n    weight += gradient * scale;\n  }\n  grad_weight_per_segment[id * stride + startFeature] = weight;\n}",
      "disabled": true
    },
    {
      "kernel_name": "compute_grad_weight",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void compute_grad_weight(\n    const index_t *indices,\n    const scalar_t *gradOutput,\n    const index_t *count,\n    ptrdiff_t numel,\n    int64_t stride,\n    const index_t* segment_offsets,\n    const int64_t *num_of_segments_ptr,\n    acc_type<scalar_t, true> *grad_weight_per_segment,\n    const int64_t stride_warped) {\n\n  int64_t num_of_segments = *num_of_segments_ptr;\n  using accscalar_t = acc_type<scalar_t, true>;\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int id = gid / stride_warped;\n  const int startFeature = gid % stride_warped;\n  if (startFeature >= stride) {\n    return;\n  }\n  if (id >= num_of_segments) {\n    return;\n  }\n  const int idx_begin = segment_offsets[id];\n  const int idx_end = (id == num_of_segments-1)?numel:segment_offsets[id+1];\n\n  accscalar_t weight = 0;\n  for (int idx=idx_begin; idx < idx_end; ++idx) {\n    const index_t target_row = indices[idx];\n    const accscalar_t scale = count ? (accscalar_t)1.0 / count[idx] : 1.0;\n    weight += gradOutput[target_row * stride + startFeature] * scale;\n  }\n  grad_weight_per_segment[id * stride + startFeature] = weight;\n}",
      "disabled": true
    },
    {
      "kernel_name": "sum_and_scatter",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void sum_and_scatter(\n    const index_t *input, scalar_t *gradWeight, int64_t stride,\n    const index_t* segment_offsets, const int64_t *num_of_segments_ptr,\n    const acc_type<scalar_t, true> *grad_weight_per_segment,\n    const index_t *segment_sizes_offsets, const int64_t *num_of_partial_segments_ptr,\n    const int64_t padding_idx,\n    const int64_t stride_warped) {\n\n  int64_t num_of_segments = *num_of_segments_ptr;\n  int64_t num_of_partial_segments = *num_of_partial_segments_ptr;\n  const int gid = blockIdx.x * blockDim.x + threadIdx.x;\n  const int id = gid / stride_warped;\n  const int startFeature = gid % stride_warped;\n  if (startFeature >= stride) {\n    return;\n  }\n  if (id >= num_of_segments) {\n    return;\n  }\n\n  const int idx_begin = segment_sizes_offsets[id];\n  const int idx_end = (id == num_of_segments-1)?num_of_partial_segments:segment_sizes_offsets[id+1];\n  acc_type<scalar_t, true> weight = 0;\n  for (int idx=idx_begin; idx < idx_end; ++idx) {\n    weight += grad_weight_per_segment[idx*stride + startFeature];\n  }\n  int64_t target_row = input[segment_offsets[id]];\n  if (target_row != padding_idx) {\n    gradWeight[target_row * stride + startFeature] = weight;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "compute_num_of_partial_segments",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void compute_num_of_partial_segments(const index_t *partials_per_segment, const index_t *partials_per_segment_offset, const int64_t *num_of_segments_ptr, int64_t *output) {\n  int64_t num_of_segments = *num_of_segments_ptr;\n  *output = partials_per_segment[num_of_segments-1] +\n            partials_per_segment_offset[num_of_segments-1];\n}",
      "disabled": true
    },
    {
      "kernel_name": "write_num_of_segments_for_legacy_thrust_path",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void write_num_of_segments_for_legacy_thrust_path(int64_t *num_of_segments_ptr, int64_t num_of_segments) {\n  *num_of_segments_ptr = num_of_segments;\n}",
      "disabled": true
    },
    {
      "kernel_name": "elementwise_kernel_with_index",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void elementwise_kernel_with_index(index_t N, func_t f, typename function_traits<func_t>::result_type *data) {\n  #pragma unroll\n  for (int i = 0; i < thread_work_size; i++) {\n    index_t idx = block_work_size * blockIdx.x + num_threads() * i + threadIdx.x;\n    if (idx < N) {\n      data[idx] = f(idx);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_bilinear2d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_bilinear2d_out_frame(\n    const int n,\n    const accscalar_t rheight,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    const PackedTensorAccessor<const scalar_t, 4> idata,\n    PackedTensorAccessor<scalar_t, 4> odata) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int height1 = idata.size(2);\n  const int width1 = idata.size(3);\n  const int width2 = odata.size(3);\n\n  if (index < n) {\n    const int w2 = index % width2; // 0:width2-1\n    const int h2 = index / width2; // 0:height2-1\n\n    const accscalar_t h1r = area_pixel_compute_source_index<accscalar_t>(\n        rheight, h2, align_corners, /*cubic=*/false);\n    const int h1 = h1r;\n    const int h1p = (h1 < height1 - 1) ? 1 : 0;\n    const accscalar_t h1lambda = h1r - h1;\n    const accscalar_t h0lambda = static_cast<accscalar_t>(1) - h1lambda;\n    //\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n    //\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; ++c) {\n        const accscalar_t val = h0lambda *\n                (w0lambda * idata[n][c][h1][w1] +\n                 w1lambda * idata[n][c][h1][w1 + w1p]) +\n            h1lambda *\n                (w0lambda * idata[n][c][h1 + h1p][w1] +\n                 w1lambda * idata[n][c][h1 + h1p][w1 + w1p]);\n        odata[n][c][h2][w2] = static_cast<scalar_t>(val);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_bilinear2d_nhwc_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_bilinear2d_nhwc_out_frame(\n    const accscalar_t rheight,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    const int channels,\n    const int height1,\n    const int width1,\n    const int height2,\n    const int width2,\n    const scalar_t* idata,\n    scalar_t* odata,\n    const int out_numel) {\n\n  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (index < out_numel) {\n    const int c = index % channels;\n    const int w2 = (index / channels) % width2;\n    const int h2 = (index / channels / width2) % height2;\n    const int n = index / channels / width2 / height2;\n\n    const accscalar_t h1r = area_pixel_compute_source_index<accscalar_t>(\n        rheight, h2, align_corners, /*cubic=*/false);\n    const int h1 = h1r;\n    const int h1p = (h1 < height1 - 1) ? 1 : 0;\n    const accscalar_t h1lambda = h1r - h1;\n    const accscalar_t h0lambda = static_cast<accscalar_t>(1) - h1lambda;\n\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n\n    const accscalar_t val = h0lambda * (\n        w0lambda * idata[idx_cl(n, h1, w1, c, height1, width1, channels)] +\n        w1lambda * idata[idx_cl(n, h1, w1 + w1p, c, height1, width1, channels)]\n      ) + h1lambda * (\n        w0lambda * idata[idx_cl(n, h1 + h1p, w1, c, height1, width1, channels)] +\n        w1lambda * idata[idx_cl(n, h1 + h1p, w1 + w1p, c, height1, width1, channels)]\n      );\n    odata[idx_cl(n, h2, w2, c, height2, width2, channels)] = static_cast<scalar_t>(val);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_bilinear2d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_bilinear2d_backward_out_frame(\n    const size_t nc,\n    const int height1,\n    const int width1,\n    const int height2,\n    const int width2,\n    const accscalar_t rheight,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    scalar_t* __restrict__ idata,\n    const scalar_t* __restrict__ odata) {\n  const size_t o_numel = nc * width2 * height2;\n  const size_t i_numel = nc * width1 * height1;\n  for (size_t index = blockDim.x * blockIdx.x + threadIdx.x; index < o_numel;\n       index += blockDim.x * gridDim.x) {\n    size_t index_temp = index;\n    const int w2 = index_temp % width2; // 0:width2-1\n    index_temp /= width2;\n    const int h2 = index_temp % height2; // 0:height2-1\n    const size_t nc = index_temp / height2;\n    //\n    const accscalar_t h1r = area_pixel_compute_source_index<accscalar_t>(\n        rheight, h2, align_corners, /*cubic=*/false);\n    const int h1 = h1r;\n    const int h1p = (h1 < height1 - 1) ? 1 : 0;\n    const accscalar_t h1lambda = h1r - h1;\n    const accscalar_t h0lambda = static_cast<accscalar_t>(1) - h1lambda;\n    //\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n    //\n    const scalar_t d2val = odata[index];\n    fastAtomicAdd(\n        idata,\n        idx(nc, height1, width1, h1, w1),\n        i_numel,\n        static_cast<scalar_t>(h0lambda * w0lambda * d2val),\n        true);\n    fastAtomicAdd(\n        idata,\n        idx(nc, height1, width1, h1, w1 + w1p),\n        i_numel,\n        static_cast<scalar_t>(h0lambda * w1lambda * d2val),\n        true);\n    fastAtomicAdd(\n        idata,\n        idx(nc, height1, width1, h1 + h1p, w1),\n        i_numel,\n        static_cast<scalar_t>(h1lambda * w0lambda * d2val),\n        true);\n    fastAtomicAdd(\n        idata,\n        idx(nc, height1, width1, h1 + h1p, w1 + w1p),\n        i_numel,\n        static_cast<scalar_t>(h1lambda * w1lambda * d2val),\n        true);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_bilinear2d_backward_nhwc_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_bilinear2d_backward_nhwc_out_frame(\n    const int height1,\n    const int width1,\n    const int height2,\n    const int width2,\n    const accscalar_t rheight,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    scalar_t* __restrict__ idata,\n    const scalar_t* __restrict__ odata,\n    const int channels,\n    const size_t o_numel,\n    const size_t i_numel) {\n\n  const int index = blockIdx.x * blockDim.x + threadIdx.x;\n\n  if (index < o_numel) {\n    const int c = index % channels;\n    const int w2 = (index / channels) % width2;\n    const int h2 = (index / channels / width2) % height2;\n    const int n = index / channels / width2 / height2;\n\n    const accscalar_t h1r = area_pixel_compute_source_index<accscalar_t>(\n        rheight, h2, align_corners, /*cubic=*/false);\n    const int h1 = h1r;\n    const int h1p = (h1 < height1 - 1) ? 1 : 0;\n    const accscalar_t h1lambda = h1r - h1;\n    const accscalar_t h0lambda = static_cast<accscalar_t>(1) - h1lambda;\n\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n\n    const scalar_t d2val = odata[index];\n    fastAtomicAdd(\n        idata,\n        idx_cl(n, h1, w1, c, height1, width1, channels),\n        i_numel,\n        static_cast<scalar_t>(h0lambda * w0lambda * d2val),\n        true);\n    fastAtomicAdd(\n        idata,\n        idx_cl(n, h1, w1 + w1p, c, height1, width1, channels),\n        i_numel,\n        static_cast<scalar_t>(h0lambda * w1lambda * d2val),\n        true);\n    fastAtomicAdd(\n        idata,\n        idx_cl(n, h1 + h1p, w1, c, height1, width1, channels),\n        i_numel,\n        static_cast<scalar_t>(h1lambda * w0lambda * d2val),\n        true);\n    fastAtomicAdd(\n        idata,\n        idx_cl(n, h1 + h1p, w1 + w1p, c, height1, width1, channels),\n        i_numel,\n        static_cast<scalar_t>(h1lambda * w1lambda * d2val),\n        true);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_gen2d_aa_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_gen2d_aa_out_frame(\n    const accscalar_t height_scale,\n    const accscalar_t width_scale,\n    const PackedTensorAccessor64<const scalar_t, 4> idata,\n    PackedTensorAccessor64<scalar_t, 4> odata,\n    const InterpFilter & interp_filter) {\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int input_height = idata.size(2);\n  const int input_width = idata.size(3);\n  const int output_height = odata.size(2);\n  const int output_width = odata.size(3);\n\n  const int output_x = threadIdx.x + blockIdx.x * blockDim.x;\n  const int output_y = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (output_x >= output_width || output_y >= output_height) {\n    return;\n  }\n\n  const accscalar_t half = 0.5;\n  const accscalar_t support_h = static_cast<accscalar_t>(\n      (height_scale >= 1.0) ? (interp_filter.size * half) * height_scale : interp_filter.size * half);\n  const accscalar_t support_w = static_cast<accscalar_t>(\n      (width_scale >= 1.0) ? (interp_filter.size * half) * width_scale : interp_filter.size * half);\n\n  const int interp_height = (int)ceilf(support_h) * 2 + 1;\n  const int interp_width = (int)ceilf(support_w) * 2 + 1;\n\n  // Setup weights and a buffer using shared memory\n  extern __shared__ int smem[];\n  scalar_t* wx = reinterpret_cast<scalar_t*>(smem) + interp_width * threadIdx.x;\n  scalar_t* wy = reinterpret_cast<scalar_t*>(smem) + interp_width * blockDim.x + interp_height * threadIdx.y;\n  const int offset = interp_width * blockDim.x + interp_height * blockDim.y;\n  scalar_t *buffer2 = reinterpret_cast<scalar_t*>(smem) + offset + \\\n      interp_height * (threadIdx.x + threadIdx.y * blockDim.x);\n\n  // Compute weights and kernel spans\n  int xmin, xsize, ymin, ysize;\n  accscalar_t xcenter, ycenter;\n  upsample_antialias::_compute_weights_span(\n      output_x, input_width, width_scale, support_w, xmin, xsize, xcenter);\n  upsample_antialias::_compute_weights_span(\n      output_y, input_height, height_scale, support_h, ymin, ysize, ycenter);\n\n  if (threadIdx.y == 0)\n  {\n    // All threadIdx.y have the same wx weights\n    upsample_antialias::_compute_weights<scalar_t, accscalar_t>(\n        wx,\n        width_scale,\n        interp_width,\n        interp_filter,\n        xmin - xcenter,\n        xsize);\n  }\n\n  if (threadIdx.x == 0)\n  {\n    // All threadIdx.x have the same wy weights\n    upsample_antialias::_compute_weights<scalar_t, accscalar_t>(\n        wy,\n        height_scale,\n        interp_height,\n        interp_filter,\n        ymin - ycenter,\n        ysize);\n  }\n\n  __syncthreads();\n\n  const scalar_t * buffer1;\n\n  // Parallelized across batch/channels\n  for (int i = blockIdx.z; i < batchsize * channels; i += gridDim.z) {\n    int n = i / channels;\n    int c = i % channels;\n    // interpolate on y-axis for ymin to ymin + ysize\n    for (int y = 0; y < ysize; y++) {\n      buffer1 = &(idata[n][c][ymin + y][xmin]);\n      buffer2[y] = static_cast<scalar_t>(\n          upsample_antialias::interpolate_aa_single_dim<scalar_t, accscalar_t>(\n              buffer1, wx, xsize));\n    }\n    odata[n][c][output_y][output_x] = static_cast<scalar_t>(\n        upsample_antialias::interpolate_aa_single_dim<scalar_t, accscalar_t>(\n            buffer2, wy, ysize));\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_gen2d_aa_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_gen2d_aa_backward_out_frame(\n    const accscalar_t height_scale,\n    const accscalar_t width_scale,\n    PackedTensorAccessor64<scalar_t, 4> idata,\n    const PackedTensorAccessor64<const scalar_t, 4> odata,\n    const InterpFilter & interp_filter) {\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int input_height = idata.size(2);\n  const int input_width = idata.size(3);\n  const int output_height = odata.size(2);\n  const int output_width = odata.size(3);\n\n  const int output_x = threadIdx.x + blockIdx.x * blockDim.x;\n  const int output_y = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (output_x >= output_width || output_y >= output_height) {\n    return;\n  }\n\n  // special case: output just copy\n  if (input_height == output_height && input_width == output_width) {\n    for (int i = blockIdx.z; i < batchsize * channels; i += gridDim.z) {\n      int n = i / channels;\n      int c = i % channels;\n      const scalar_t val = odata[n][c][output_y][output_x];\n      idata[n][c][output_y][output_x] = val;\n    }\n    return;\n  }\n\n  const accscalar_t support_h = static_cast<accscalar_t>(\n      (height_scale >= 1.0) ? (interp_filter.size * 0.5) * height_scale\n                            : interp_filter.size * 0.5);\n  const accscalar_t support_w = static_cast<accscalar_t>(\n      (width_scale >= 1.0) ? (interp_filter.size * 0.5) * width_scale\n                           : interp_filter.size * 0.5);\n\n  const int interp_height = (int)ceilf(support_h) * 2 + 1;\n  const int interp_width = (int)ceilf(support_w) * 2 + 1;\n\n  // Setup weights using shared memory\n  extern __shared__ int smem[];\n  scalar_t* wx = reinterpret_cast<scalar_t*>(smem) + interp_width * threadIdx.x;\n  scalar_t* wy = reinterpret_cast<scalar_t*>(smem) + interp_width * blockDim.x + interp_height * threadIdx.y;\n\n  // Compute weights and kernel spans\n  int xmin, xsize, ymin, ysize;\n  accscalar_t xcenter, ycenter;\n  upsample_antialias::_compute_weights_span(\n      output_x, input_width, width_scale, support_w, xmin, xsize, xcenter);\n  upsample_antialias::_compute_weights_span(\n      output_y, input_height, height_scale, support_h, ymin, ysize, ycenter);\n\n  if (threadIdx.y == 0)\n  {\n    // All threadIdx.y have the same wx weights\n    upsample_antialias::_compute_weights<scalar_t, accscalar_t>(\n        wx,\n        width_scale,\n        interp_width,\n        interp_filter,\n        xmin - xcenter,\n        xsize);\n  }\n\n  if (threadIdx.x == 0)\n  {\n    // All threadIdx.x have the same wy weights\n    upsample_antialias::_compute_weights<scalar_t, accscalar_t>(\n        wy,\n        height_scale,\n        interp_height,\n        interp_filter,\n        ymin - ycenter,\n        ysize);\n  }\n\n  __syncthreads();\n\n  // Parallelized across batch/channels\n  for (int i = blockIdx.z; i < batchsize * channels; i += gridDim.z) {\n    int n = i / channels;\n    int c = i % channels;\n    scalar_t out_value = odata[n][c][output_y][output_x];\n    for (int y = 0; y < ysize; y++) {\n      for (int x = 0; x < xsize; x++) {\n        upsample_increment_value_bounded<scalar_t, accscalar_t>(\n            idata,\n            n,\n            c,\n            input_height,\n            input_width,\n            ymin + y,\n            xmin + x,\n            wx[x] * wy[y] * out_value);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "triu_indices_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid triu_indices_kernel(scalar_t * tensor,\n                         int64_t col_offset,\n                         int64_t m_first_row,\n                         int64_t col,\n                         int64_t rectangle_size,\n                         int64_t triu_size) {\n  int64_t linear_index = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n\n  if (linear_index < triu_size) {\n    int64_t r, c;\n    if (linear_index < rectangle_size) {\n      // the coordinate is within the top rectangle\n      r = linear_index / col;\n      c = linear_index % col;\n    } else {\n      // the coordinate falls in the bottom trapezoid\n      get_coordinate_in_triu_trapezoid(\n        m_first_row, linear_index - rectangle_size, r, c);\n      r += rectangle_size / col;\n    }\n\n    c += col_offset;\n    tensor[linear_index] = r;\n    tensor[linear_index + triu_size] = c;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "vectorized_gather_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void vectorized_gather_kernel(char * out, char * inp, index_t * idx, int num_ind, int64_t slice_size, int64_t ind_dim_size, int64_t inp_stride, int64_t out_stride, bool allow_neg_indices) {\n    int64_t ind = idx[blockIdx.x];\n    if (allow_neg_indices) {\n        ind = (ind < 0) ? ind + ind_dim_size : ind;\n    }\n    CUDA_KERNEL_ASSERT(ind >=0 && ind < ind_dim_size && \"vectorized gather kernel index out of bounds\");\n    int32_t off = (blockDim.x * blockIdx.y + threadIdx.x) * Alignment; // off is guaranteed to be within int32 limits\n    if (off >= slice_size) return;\n    auto vec = at::native::memory::ld_vec<Alignment>(inp + ind * inp_stride + off);\n    at::native::memory::st_vec<Alignment>(out + blockIdx.x * (int32_t)out_stride + off, vec);  // out offset is guaranteed to be within int32 limits\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad1d_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad1d_out_kernel(\n    const scalar_t * input, scalar_t * output,\n    int64_t input_w,\n    int64_t pad_l, int64_t pad_r) {\n  auto output_x = threadIdx.x + blockIdx.x * blockDim.x;\n  auto output_w = input_w + pad_l + pad_r;\n\n  if (output_x < output_w) {\n    auto index_pair = get_index_mapping1d(input_w, output_w, output_x, pad_l);\n    output[index_pair.second] = input[index_pair.first];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad1d_backward_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad1d_backward_out_kernel(\n    scalar_t * grad_input, const scalar_t * grad_output,\n    int64_t input_w,\n    int64_t pad_l, int64_t pad_r) {\n  auto output_x = threadIdx.x + blockIdx.x * blockDim.x;\n  auto output_w = input_w + pad_l + pad_r;\n\n  if (output_x < output_w) {\n    auto index_pair = get_index_mapping1d(input_w, output_w, output_x, pad_l);\n    gpuAtomicAddNoReturn(\n      &grad_input[index_pair.first], grad_output[index_pair.second]);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad2d_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad2d_out_kernel(\n    const scalar_t * input, scalar_t * output,\n    int64_t input_dim_x, int64_t input_dim_y,\n    int pad_t, int pad_b, int pad_l, int pad_r, int y_shift, int z_shift, int nplane) {\n  auto output_xy = threadIdx.x + blockIdx.x * blockDim.x;\n  auto output_dim_x = input_dim_x + pad_l + pad_r;\n  auto output_dim_y = input_dim_y + pad_t + pad_b;\n\n  if (output_xy < output_dim_x * output_dim_y) {\n    auto index_pair = get_index_mapping2d(\n      input_dim_x, input_dim_y,\n      output_dim_x, output_dim_y,\n      pad_l, pad_t,\n      output_xy, y_shift, z_shift, nplane);\n\n    output[index_pair.second] = input[index_pair.first];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad2d_backward_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad2d_backward_out_kernel(\n    scalar_t * grad_input, const scalar_t * grad_output,\n    int64_t input_dim_x, int64_t input_dim_y,\n    int pad_t, int pad_b, int pad_l, int pad_r, int y_shift, int z_shift, int nplane) {\n  auto output_xy = threadIdx.x + blockIdx.x * blockDim.x;\n  auto output_dim_x = input_dim_x + pad_l + pad_r;\n  auto output_dim_y = input_dim_y + pad_t + pad_b;\n\n  if (output_xy < output_dim_x * output_dim_y) {\n    auto index_pair = get_index_mapping2d(\n      input_dim_x, input_dim_y,\n      output_dim_x, output_dim_y,\n      pad_l, pad_t,\n      output_xy, y_shift, z_shift, nplane);\n\n    gpuAtomicAddNoReturn(&grad_input[index_pair.first], grad_output[index_pair.second]);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad2d_backward_det_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad2d_backward_det_out_kernel(\n    scalar_t* grad_input,\n    const scalar_t* grad_output,\n    int64_t input_dim_x,\n    int64_t input_dim_y,\n    int pad_t,\n    int pad_b,\n    int pad_l,\n    int pad_r,\n    int batch,\n    int channels,\n    int) {\n  const int64_t input_xy_ = threadIdx.x + blockIdx.x * blockDim.x;\n  const auto output_dim_x = input_dim_x + pad_l + pad_r;\n  const auto output_dim_y = input_dim_y + pad_t + pad_b;\n  const auto N = output_dim_x * output_dim_y;\n  const int64_t width = output_dim_x;\n  const int64_t height = output_dim_y;\n  const int64_t stride =\n      static_cast<int64_t>(gridDim.x) * static_cast<int64_t>(blockDim.x);\n  const int64_t end =\n      static_cast<int64_t>(batch) * channels * input_dim_x * input_dim_y;\n\n  for (int64_t input_xy = input_xy_; input_xy < end; input_xy += stride) {\n    scalar_t partial = 0;\n\n    const int64_t b = input_xy / (channels * input_dim_x * input_dim_y);\n    const int64_t c = (input_xy / (input_dim_x * input_dim_y)) % channels;\n    const int64_t pos_xy = input_xy % (input_dim_x * input_dim_y);\n    const int64_t inp_row = pos_xy / input_dim_x;\n    const int64_t inp_col = pos_xy % input_dim_x;\n\n    const bool is_top = (inp_row >= 1) && (inp_row <= pad_t);\n    const bool is_bottom =\n        (inp_row < input_dim_y - 1) && (inp_row >= input_dim_y - pad_b - 1);\n    const bool is_left = (inp_col >= 1) && (inp_col <= pad_l);\n    const bool is_right =\n        (inp_col < input_dim_x - 1) && (inp_col >= input_dim_x - pad_r - 1);\n\n    if (is_top) {\n      const int64_t border_top_row = 0;\n      const int64_t dist_from_t = inp_row;\n\n      const int64_t border_top_out_row = border_top_row + pad_t;\n      const int64_t border_top_out_col = pad_l + inp_col;\n\n      const int64_t reflected_top_row = border_top_out_row - dist_from_t;\n      const int64_t reflected_top_out =\n          reflected_top_row * width + border_top_out_col;\n\n      if (reflected_top_out < N) {\n        partial += grad_output\n            [b * (channels * width * height) + c * (width * height) +\n             reflected_top_out];\n      }\n\n      if (is_left) { // top left\n        const int64_t corner_tl_out_row = pad_t;\n        const int64_t corner_tl_out_col = pad_l;\n        const int64_t dist_rows = inp_row;\n        const int64_t dist_cols = inp_col;\n        const int64_t reflect_tl_out_row = (corner_tl_out_row - dist_rows);\n        const int64_t reflect_tl_out_col = (corner_tl_out_col - dist_cols);\n        const int64_t reflect_tl_out =\n            (reflect_tl_out_row * width) + reflect_tl_out_col;\n\n        if (reflect_tl_out >= 0 && reflect_tl_out < N) {\n          partial += grad_output\n              [b * (channels * width * height) + c * (width * height) +\n               reflect_tl_out];\n        }\n      } else if (is_right) { // top right\n        // TR corner is just (0, cols - 1)\n        const int64_t corner_tr_out_row = pad_t;\n        const int64_t corner_tr_out_col = pad_l + input_dim_x - 1;\n        const int64_t dist_rows = inp_row; // as the TR corner is (0, cols - 1)\n        const int64_t dist_cols = ::abs(inp_col - (input_dim_x - 1));\n\n        // we were dist_rows after, now we want to be dist_rows before\n        // we were dist_cols before, now we wnat to be dist_cols after\n        const int64_t reflect_tr_out_row = (corner_tr_out_row - dist_rows);\n        const int64_t reflect_tr_out_col = (corner_tr_out_col + dist_cols);\n        const int64_t reflect_tr_out =\n            (reflect_tr_out_row * width) + reflect_tr_out_col;\n\n        if (reflect_tr_out >= 0 && reflect_tr_out < N) {\n          partial += grad_output\n              [b * (channels * width * height) + c * (width * height) +\n               reflect_tr_out];\n        }\n      }\n    }\n\n    if (is_bottom) {\n      const int64_t border_bot_row =\n          input_dim_y - 1; // must use last row, not inp row\n      const int64_t border_bot_col = inp_col;\n      const int64_t dist_from_bot = ::abs(inp_row - border_bot_row);\n\n      // we are dist_from_bot rows before it. Now we want to be after it.\n      const int64_t border_bot_out_row = pad_t + border_bot_row;\n      const int64_t border_bot_out_col = pad_l + border_bot_col;\n      const int64_t reflect_bot_row = (border_bot_out_row + dist_from_bot);\n      const int64_t reflect_bot_out =\n          (reflect_bot_row * width) + border_bot_out_col;\n\n      if (reflect_bot_out >= 0 && reflect_bot_out < N) {\n        partial += grad_output\n            [b * (channels * width * height) + c * (width * height) +\n             reflect_bot_out];\n      }\n\n      if (is_left) {\n        // (rows - 1, 0)\n        const int64_t corner_bl_row = input_dim_y - 1;\n        const int64_t corner_bl_col = 0;\n\n        const int64_t corner_bl_out_row = pad_t + corner_bl_row;\n        const int64_t corner_bl_out_col = pad_l + corner_bl_col;\n\n        // we are inp_rows before it. inp_cols after it.\n        const int64_t dist_rows = ::abs(inp_row - corner_bl_row);\n        const int64_t dist_cols = inp_col;\n\n        // Now we want to be inp_rows after, and inp_cols before.\n        const int64_t reflect_bl_out_row = (corner_bl_out_row + dist_rows);\n        const int64_t reflect_bl_out_col = (corner_bl_out_col - dist_cols);\n        const int64_t reflect_bl_out =\n            (reflect_bl_out_row * width) + reflect_bl_out_col;\n\n        if (reflect_bl_out >= 0 && reflect_bl_out < N) {\n          partial += grad_output\n              [b * (channels * width * height) + c * (width * height) +\n               reflect_bl_out];\n        }\n      } else if (is_right) {\n        // (rows-1, cols-1)\n        const int64_t corner_br_row = input_dim_y - 1;\n        const int64_t corner_br_col = input_dim_x - 1;\n        const int64_t dist_rows = ::abs(inp_row - corner_br_row);\n        const int64_t dist_cols = ::abs(inp_col - corner_br_col);\n\n        const int64_t corner_br_out_row = pad_t + corner_br_row;\n        const int64_t corner_br_out_col = pad_l + corner_br_col;\n\n        const int64_t reflect_br_out_row = (corner_br_out_row + dist_rows);\n        const int64_t reflect_br_out_col = (corner_br_out_col + dist_cols);\n        const int64_t reflect_br_out =\n            (reflect_br_out_row * width) + reflect_br_out_col;\n\n        if (reflect_br_out >= 0 && reflect_br_out < N) {\n          partial += grad_output\n              [b * (channels * width * height) + c * (width * height) +\n               reflect_br_out];\n        }\n      }\n    }\n    if (is_left) {\n      const int64_t border_left_row = inp_row;\n      const int64_t border_left_out_row = border_left_row + pad_t;\n      const int64_t border_left_out_col = pad_l;\n\n      const int64_t dist_from_left = inp_col;\n\n      const int64_t reflect_left_out_row = border_left_out_row;\n      const int64_t reflect_left_out_col = border_left_out_col - dist_from_left;\n      const int64_t reflect_left_out =\n          reflect_left_out_row * width + reflect_left_out_col;\n\n      if (reflect_left_out >= 0 && reflect_left_out < N) {\n        partial += grad_output\n            [b * (channels * width * height) + c * (width * height) +\n             reflect_left_out];\n      }\n    }\n    if (is_right) {\n      const int64_t border_right_row = inp_row;\n      const int64_t border_right_col = input_dim_x - 1;\n\n      const int64_t border_right_out_row = border_right_row + pad_t;\n      const int64_t border_right_out_col = border_right_col + pad_l;\n\n      const int64_t dist_from_right = ::abs(inp_col - border_right_col);\n\n      const int64_t reflect_right_out_row = border_right_out_row;\n      const int64_t reflect_right_out_col =\n          border_right_out_col + dist_from_right;\n      const int64_t reflect_right_out =\n          reflect_right_out_row * width + reflect_right_out_col;\n\n      if (reflect_right_out >= 0 && reflect_right_out < N) {\n        partial += grad_output\n            [b * (channels * width * height) + c * (width * height) +\n             reflect_right_out];\n      }\n    }\n    const int64_t out_row = inp_row + pad_t;\n    const int64_t out_col = inp_col + pad_l;\n\n    partial += grad_output\n        [b * (channels * width * height) + c * (width * height) +\n         out_row * width + out_col];\n\n    grad_input[input_xy] += partial;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad3d_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad3d_out_kernel(\n    PackedTensorAccessor64<const scalar_t, 5> input,\n    PackedTensorAccessor64<scalar_t, 5> output,\n    int64_t pad_left,  int64_t pad_top, int64_t pad_front,\n    int64_t y_shift, int64_t z_shift\n){\n  parallel_reflection_pad3d(\n      input,\n      output,\n      pad_left,\n      pad_top,\n      pad_front,\n      y_shift,\n      z_shift,\n      [&] __device__(\n          int64_t plane,\n          int64_t batch,\n          int64_t output_z,\n          int64_t output_y,\n          int64_t output_x,\n          int64_t input_z,\n          int64_t input_y,\n          int64_t input_x) {\n        auto value_to_copy = input[batch][plane][input_z][input_y][input_x];\n        output[batch][plane][output_z][output_y][output_x] = value_to_copy;\n      });\n}",
      "disabled": true
    },
    {
      "kernel_name": "reflection_pad3d_backward_out_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void reflection_pad3d_backward_out_kernel(\n    PackedTensorAccessor64<scalar_t, 5> grad_input,\n    PackedTensorAccessor64<const scalar_t, 5> grad_output,\n    int64_t pad_left,  int64_t pad_top, int64_t pad_front,\n    int64_t y_shift, int64_t z_shift\n) {\n  parallel_reflection_pad3d(\n      grad_input,\n      grad_output,\n      pad_left,\n      pad_top,\n      pad_front,\n      y_shift,\n      z_shift,\n      [&] __device__(\n          int64_t plane,\n          int64_t batch,\n          int64_t output_z,\n          int64_t output_y,\n          int64_t output_x,\n          int64_t input_z,\n          int64_t input_y,\n          int64_t input_x) {\n        auto value_to_add = grad_output[batch][plane][output_z][output_y][output_x];\n        auto target = &grad_input[batch][plane][input_z][input_y][input_x];\n        gpuAtomicAddNoReturn(target, value_to_add);\n      });\n}",
      "disabled": true
    },
    {
      "kernel_name": "gatherTopK",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void gatherTopK(at::cuda::detail::TensorInfo<const T, IndexType> input,\n                           IndexType inputSliceSize,\n                           IndexType outputSliceSize, // aka `k`\n                           bool largest,\n\n                           IndexType numInputSlices,\n                           IndexType inputWithinSliceStride,\n\n                           at::cuda::detail::TensorInfo<T, IndexType> topK,\n                           IndexType topKWithinSliceStride,\n\n                           at::cuda::detail::TensorInfo<int64_t, IndexType> indices,\n                           IndexType indicesWithinSliceStride,\n                           T* kthValues) {\n  // Indices are limited to integer fp precision, so counts can fit in\n  // int32, regardless of IndexType\n#if defined(USE_ROCM)\n  __shared__ int smem[64];\n#else\n  __shared__ int smem[32]; // one per each warp, up to warp limit\n#endif\n  IndexType slice = getLinearBlockId<IndexType>();\n  if (slice >= numInputSlices) {\n    return;\n  }\n\n  // Find the start offset for our slice\n  IndexType sliceStartIndex =\n    at::cuda::detail::IndexToOffset<const T, IndexType, Dim>::get(slice, input);\n  IndexType topKSliceStartIndex =\n    at::cuda::detail::IndexToOffset<T, IndexType, Dim>::get(slice, topK);\n  IndexType indicesSliceStartIndex =\n    at::cuda::detail::IndexToOffset<int64_t, IndexType, Dim>::get(slice, indices);\n\n  const T* inputSliceStart = &input.data[sliceStartIndex];\n  T* topKSliceStart = &topK.data[topKSliceStartIndex];\n  int64_t* indicesSliceStart = &indices.data[indicesSliceStartIndex];\n\n  // Find the k-th highest element in our input\n  T topKValue;\n  if (WithKthValues){\n    topKValue = kthValues[slice];\n  } else {\n    topKValue = static_cast<T>(0);\n    radixSelect<T, typename TopKTypeConfig<T>::RadixType, IndexType>(\n      inputSliceStart, outputSliceSize, largest,\n      inputSliceSize, inputWithinSliceStride,\n      smem, &topKValue);\n  }\n  const auto topKConverted = at::native::TopKTypeConfig<T>::convert(topKValue);\n\n  // Every value that is strictly less/greater than `pattern`\n  // (depending on sort dir) in sorted int format is in the top-K.\n  // The top-K value itself might not be unique.\n  //\n  // Since there are a variable number of elements that we see that\n  // are within the top-k, we don't know at what index to write out\n  // the resulting values.\n  // In order to get this, we perform an exclusive prefix sum of\n  // `hasTopK`. This will return the resulting index into which we\n  // need to write the result, if a thread has a result.\n\n  // All threads need to participate in the loop and the prefix sum,\n  // but not necessarily in the load; hence loop bounds being rounded\n  // up to a multiple of the block dim.\n  IndexType numIterations = round_up(inputSliceSize, (IndexType) blockDim.x);\n  IndexType writeIndexStart = 0;\n\n  for (IndexType i = threadIdx.x; i < numIterations; i += blockDim.x) {\n    bool inRange = (i < inputSliceSize);\n    T v =\n      inRange ? doLdg(&inputSliceStart[i * inputWithinSliceStride]) : static_cast<T>(0);\n    const auto convertedV = at::native::TopKTypeConfig<T>::convert(v);\n    bool hasTopK;\n    if (largest) {\n      hasTopK = inRange && (convertedV > topKConverted);\n    } else {\n      hasTopK = inRange && (convertedV < topKConverted);\n    }\n\n    int index;\n    int carry;\n    at::cuda::exclusiveBinaryPrefixScan<int, true>(\n        smem, hasTopK, &index, &carry, AddOp<int>());\n\n    if (hasTopK) {\n      int writeIndex = writeIndexStart + index;\n      CUDA_KERNEL_ASSERT(writeIndex < outputSliceSize);\n\n      IndexType topKOffset = writeIndex * topKWithinSliceStride;\n      IndexType indexOffset = writeIndex * indicesWithinSliceStride;\n\n      topKSliceStart[topKOffset] = v;\n      indicesSliceStart[indexOffset] = i;\n    }\n\n    writeIndexStart += carry;\n  }\n\n  // We need to fill in the rest with actual == top-K values.\n  // The number that we need is outputSliceSize -\n  // writeIndexStart. There might be more than that number available,\n  // in which case we have to choose the first seen set. We do this\n  // via a prefix sum to calculate indices for writing results.\n  CUDA_KERNEL_ASSERT(outputSliceSize >= writeIndexStart);\n  IndexType topKRemaining = (outputSliceSize - writeIndexStart);\n\n  for (IndexType i = threadIdx.x; i < numIterations; i += blockDim.x) {\n    bool inRange = (i < inputSliceSize);\n    T v =\n      inRange ? doLdg(&inputSliceStart[i * inputWithinSliceStride]) : static_cast<T>(0);\n    const auto convertedV = at::native::TopKTypeConfig<T>::convert(v);\n    bool hasTopK = inRange && (convertedV == topKConverted);\n\n    int index;\n    int carry;\n    at::cuda::exclusiveBinaryPrefixScan<int, true>(\n        smem, hasTopK, &index, &carry, AddOp<int>());\n\n    if (hasTopK && index < topKRemaining) {\n      int writeIndex = writeIndexStart + index;\n      CUDA_KERNEL_ASSERT(writeIndex < outputSliceSize);\n\n      IndexType topKOffset = writeIndex * topKWithinSliceStride;\n      IndexType indexOffset = writeIndex * indicesWithinSliceStride;\n\n      topKSliceStart[topKOffset] = v;\n      indicesSliceStart[indexOffset] = i;\n    }\n\n    if (carry >= topKRemaining) {\n      break;\n    }\n\n    topKRemaining -= carry;\n    writeIndexStart += carry;\n  }\n\n}",
      "disabled": true
    },
    {
      "kernel_name": "fill",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fill(T* x, T value, IndexType size) {\n  IndexType idx = blockIdx.x * blockDim.x + threadIdx.x;\n  for (IndexType i = idx; i < size; i += gridDim.x * blockDim.x) {\n    x[i] = value;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "radixFindKthValues",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void radixFindKthValues(\n    at::cuda::detail::TensorInfo<const T, IndexType> input,\n    uint32_t slice_size,\n    uint32_t* ks_to_find,  // size: num_slices, unused arg but for mysterious reasons perf is better when it's present\n    uint32_t num_slices,\n    IndexType withinSliceStride,\n    int current_bit,\n    int items_per_thread,\n    uint32_t blocks_per_slice,\n    Bitwise desiredMask,\n    Bitwise* desires,      // size: num_slices\n    short* counts         // size: num_slices * blocks_per_slice * radix_digits\n  ) {\n\n  int items_per_block = items_per_thread * BLOCK_THREADS;\n  int tidx = threadIdx.x;\n  uint32_t block_idx = getLinearBlockId<uint32_t>();\n  uint32_t slice_idx = block_idx / blocks_per_slice;\n  uint32_t blk_idx_in_slice = block_idx % blocks_per_slice;\n  if (slice_idx >= num_slices) {\n    return;\n  }\n\n  Bitwise desired = desires[slice_idx];\n  IndexType slice_start_index = at::cuda::detail::IndexToOffset<const T, IndexType, Dim>::get(slice_idx, input);\n  const T* data = &input.data[slice_start_index];\n\n  static_assert(MAX_ITEMS_PER_THREAD * BLOCK_THREADS < std::numeric_limits<short>::max(),\n    \"blockwise counter too large\");\n  union __align__(16) TempStorage {\n    uint32_t digit_counters[RADIX_DIGITS];\n  };\n  __shared__ TempStorage temp_storage;\n\n  // fill digit_counters with zeros\n  if (tidx < RADIX_DIGITS) {\n    temp_storage.digit_counters[tidx] = 0;\n  }\n  __syncthreads();\n\n  items_per_thread = (blk_idx_in_slice + 1 < blocks_per_slice)\n      ? items_per_thread\n      : at::ceil_div((int64_t)(slice_size - blk_idx_in_slice * items_per_block), (int64_t)BLOCK_THREADS);\n\n  // collect digit counts and store in shared memory\n  for (int i = 0; i < items_per_thread; ++i) {\n    // Find the start offset for this slice\n    IndexType idx = blk_idx_in_slice * items_per_block + i * BLOCK_THREADS + tidx;\n    if (idx < slice_size) {\n      idx *= withinSliceStride;\n      Bitwise val = TopKTypeConfig<T>::convert(doLdg(&data[idx]));\n      bool has_val = ((val & desiredMask) == (desired & desiredMask));\n      Bitwise digit = at::cuda::Bitfield<Bitwise>::getBitfield(val, current_bit, RADIX_BITS);\n      if (has_val) {\n        atomicAdd(&temp_storage.digit_counters[digit], 1);\n      }\n    }\n  }\n\n  __syncthreads();\n\n  // load digit counter to register, one digit per thread\n  static_assert(RADIX_DIGITS <= BLOCK_THREADS, \"this kernel requires RADIX_DIGITS <= BLOCK_THREADS\");\n  uint32_t digit_count = 0;\n  if (tidx < RADIX_DIGITS) {\n    digit_count = temp_storage.digit_counters[tidx];\n  }\n\n  // We always write out counts regardless if blocks_per_slice == 1 because\n  // it will be used to compute offsets for `gatherTopK`.\n  if (tidx < RADIX_DIGITS) {\n    counts[block_idx * RADIX_DIGITS + tidx] = digit_count;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "computeBlockwiseWithinKCounts",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void computeBlockwiseWithinKCounts(\n  Bitwise* desires_in,          // size: num_slices\n  short* counts,             // size: num_slices * blocks_per_slice * radix_digits\n  uint32_t* ks_to_find_in,  // size: num_slices\n  uint32_t blocks_per_slice,\n  int current_bit,\n  bool largest,\n  // outputs:\n  uint32_t* withinKCounts,  // size: num_slices * blocks_per_slice == num_blocks\n  T* kthValues,           // size: num_slices, only write when current_bit reaches 0\n  uint32_t* ks_to_find_out,\n  Bitwise* desires_out,\n  uint32_t num_blocks\n) {\n  // This kernel should be launched with the same number of blocks as the `radixFindKthValues` kernel.\n  int tidx = threadIdx.x;\n  uint32_t block_idx = getLinearBlockId<uint32_t>();\n  uint32_t slice_idx = block_idx / blocks_per_slice;\n\n  // The grid is computed from `getGridFromTiles`, when there are lots of\n  // elements, we will use both blockIdx.x and blockIdx.y, and maybe blockIdx.z\n  // when this is the case, the number of blocks that we are launching can be\n  // more than the number of blocks we need. So we need to check the range of\n  // `block_idx`.\n  if (block_idx >= num_blocks) {\n    return;\n  }\n  typedef cub::BlockScan<uint32_t, BLOCK_THREADS> BlockScan;\n  union __align__(16) TempStorage {\n    uint32_t digit_count_cumsum[RADIX_DIGITS]; // only used if this it the last block for this slice\n    typename BlockScan::TempStorage scan_storage;\n  };\n  __shared__ TempStorage temp_storage;\n\n  // accumulates counters from multiple blocks\n  uint32_t digit_count = 0;\n  if (tidx < RADIX_DIGITS) {\n    for (int blk = 0; blk < blocks_per_slice; ++blk) {\n      digit_count += counts[(slice_idx * blocks_per_slice + blk) * RADIX_DIGITS + tidx];\n    }\n  }\n\n  // compute the block-wide inclusive prefix sum\n  uint32_t digit_count_cumsum;\n  BlockScan(temp_storage.scan_storage).InclusiveSum(digit_count, digit_count_cumsum);\n  __syncthreads();\n  // every thread also need the perfix_sum of it's left value for comparison, so save a copy in shared mem\n  if (tidx < RADIX_DIGITS) {\n    temp_storage.digit_count_cumsum[tidx] = digit_count_cumsum;\n  }\n  __syncthreads();\n\n  __shared__ Bitwise desired;\n  uint32_t k_to_find = ks_to_find_in[slice_idx];\n\n  if (tidx < RADIX_DIGITS) {\n    uint32_t digit_count_cumsum_left = (tidx == 0) ? 0 : temp_storage.digit_count_cumsum[tidx - 1];\n\n    // if not the last pass: update desired and ks_to_find\n    // if last pass: write out the kth value\n    // only one thread in block enters this condition\n    if (digit_count_cumsum_left < k_to_find && k_to_find <= digit_count_cumsum) {\n      desired = desires_in[slice_idx];\n      desired = at::cuda::Bitfield<Bitwise>::setBitfield(desired, tidx, current_bit, RADIX_BITS);\n      // let a single block per slice update the values\n      if (block_idx == slice_idx * blocks_per_slice) {\n        desires_out[slice_idx] = desired;\n        if (current_bit > 0) {\n          ks_to_find_out[slice_idx] = k_to_find - digit_count_cumsum_left;\n        } else {\n          kthValues[slice_idx] = TopKTypeConfig<T>::deconvert(desired);\n        }\n      }\n    }\n  }\n  __syncthreads();\n\n#if !CUB_SUPPORTS_SCAN_BY_KEY()\n  return;\n#endif\n\n  Bitwise desired_digit = at::cuda::Bitfield<Bitwise>::getBitfield(desired, current_bit, RADIX_BITS);\n\n  // if largest, then only threads that has tidx > desired_digit are active\n  // if !largest, then only threads that has tidx < desired_digit are active\n  // each active thread will read the count for its corresponding, and\n  // do warp reduction followed by shared memory reduction to get the total count\n  // non-active thread should not load, and non-active warp should not do reduction.\n  bool warp_is_active, thread_is_active;\n  int warp = tidx / C10_WARP_SIZE;\n  if (largest) {\n    int end_of_warp = warp * C10_WARP_SIZE + C10_WARP_SIZE - 1;\n    warp_is_active = end_of_warp > desired_digit;\n    thread_is_active = tidx > desired_digit;\n  } else {\n    int start_of_warp = warp * C10_WARP_SIZE;\n    warp_is_active = start_of_warp < desired_digit;\n    thread_is_active = tidx < desired_digit;\n  }\n  uint32_t count = 0;\n  if (warp_is_active) {\n    if (thread_is_active) {\n      count = doLdg(counts + block_idx * RADIX_DIGITS + tidx);\n    }\n    for (int offset = C10_WARP_SIZE / 2; offset > 0; offset /= 2) {\n      count += WARP_SHFL_DOWN(count, offset);\n    }\n  }\n\n  constexpr int num_warps = RADIX_DIGITS / C10_WARP_SIZE;\n  __shared__ uint32_t warp_counts[num_warps];\n  if (tidx % C10_WARP_SIZE == 0) {\n    warp_counts[warp] = count;\n  }\n  __syncthreads();\n  static_assert(RADIX_DIGITS < C10_WARP_SIZE * C10_WARP_SIZE,\n    \"Assuming only 1 warp is needed for final reduction\");\n  if (warp != 0) {\n    return;\n  }\n  count = 0;\n  if (tidx < num_warps) {\n    count = warp_counts[tidx];\n  }\n  for (int offset = num_warps / 2; offset > 0; offset /= 2) {\n    count += WARP_SHFL_DOWN(count, offset);\n  }\n  if (tidx == 0) {\n    withinKCounts[block_idx] += count;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "computeBlockwiseKthCounts",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void computeBlockwiseKthCounts(\n  Bitwise* desires,            // size: num_slices\n  short* counts,               // size: num_slices * blocks_per_slice * radix_digits\n  uint32_t num_blocks,         // the number of blocks used by `radixFindKthValues` kernel\n  uint32_t blocks_per_slice,\n  // outputs:\n  uint32_t* kthCounts          // size: num_slices * blocks_per_slice == num_blocks\n) {\n  CUDA_KERNEL_LOOP_TYPE(idx, num_blocks, uint32_t) {\n    uint32_t slice_idx = idx / blocks_per_slice;\n    Bitwise desired = doLdg(desires + slice_idx);\n    Bitwise desired_digit = at::cuda::Bitfield<Bitwise>::getBitfield(desired, 0, RADIX_BITS);\n    kthCounts[idx] = doLdg(counts + idx * RADIX_DIGITS + desired_digit);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "gatherTopK",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void gatherTopK(at::cuda::detail::TensorInfo<const T, IndexType> input,\n                           IndexType inputSliceSize,\n                           IndexType outputSliceSize, // aka `k`\n                           bool largest,\n\n                           uint32_t numInputSlices,\n                           IndexType inputWithinSliceStride,\n\n                           at::cuda::detail::TensorInfo<T, IndexType> topK,\n                           IndexType topKWithinSliceStride,\n\n                           at::cuda::detail::TensorInfo<int64_t, IndexType> indices,\n                           IndexType indicesWithinSliceStride,\n\n                           uint32_t items_per_thread,\n                           uint32_t blocks_per_slice,\n\n                           T *kthValues,\n                           uint32_t* withinKCounts,\n                           uint32_t* kthCounts,\n                           uint32_t num_blocks) {\n\n  uint32_t items_per_block = items_per_thread * BLOCK_THREADS;\n  uint32_t tidx = threadIdx.x;\n  uint32_t block_idx = getLinearBlockId<uint32_t>();\n\n  // The grid is computed from `getGridFromTiles`, when there are lots of\n  // elements, we will use both blockIdx.x and blockIdx.y, and maybe blockIdx.z\n  // when this is the case, the number of blocks that we are launching can be\n  // more than the number of blocks we need. So we need to check the range of\n  // `block_idx`.\n  if (block_idx >= num_blocks) {\n    return;\n  }\n\n  uint32_t slice_idx = block_idx / blocks_per_slice;\n  uint32_t blk_idx_in_slice = block_idx % blocks_per_slice;\n\n  items_per_thread = (blk_idx_in_slice + 1 < blocks_per_slice)\n      ? items_per_thread\n      : at::ceil_div((int64_t)(inputSliceSize - blk_idx_in_slice * items_per_block), (int64_t)BLOCK_THREADS);\n\n  // Find the start offset for our slice\n  IndexType sliceStartIndex =\n    at::cuda::detail::IndexToOffset<const T, IndexType, Dim>::get(slice_idx, input);\n  IndexType topKSliceStartIndex =\n    at::cuda::detail::IndexToOffset<T, IndexType, Dim>::get(slice_idx, topK);\n  IndexType indicesSliceStartIndex =\n    at::cuda::detail::IndexToOffset<int64_t, IndexType, Dim>::get(slice_idx, indices);\n\n  const T* inputSliceStart = &input.data[sliceStartIndex];\n  T* topKSliceStart = &topK.data[topKSliceStartIndex];\n  int64_t* indicesSliceStart = &indices.data[indicesSliceStartIndex];\n\n  // Find the k-th highest element in our input\n  T kthValue = kthValues[slice_idx];\n  const auto kthValueConverted = at::native::TopKTypeConfig<T>::convert(kthValue);\n\n  // Find the start index in output tensor of this block\n  uint32_t startWithinK = 0;\n  if (blk_idx_in_slice > 0) {\n    startWithinK = withinKCounts[block_idx - 1];\n  }\n  uint32_t startKth = withinKCounts[slice_idx * blocks_per_slice + blocks_per_slice - 1];\n  if (blk_idx_in_slice > 0) {\n    startKth += kthCounts[block_idx - 1];\n  }\n\n  // Read input, select topk out and write\n  typedef cub::BlockScan<uint32_t, BLOCK_THREADS> BlockScan;\n  __shared__ typename BlockScan::TempStorage temp_storage;\n  for (int i = 0; i < items_per_thread; ++i) {\n    // Find the start offset for this slice\n    IndexType idx = blk_idx_in_slice * items_per_block + i * BLOCK_THREADS + tidx;\n    T val;\n    int withinK = 0;\n    int kth = 0;\n    if (idx < inputSliceSize) {\n      val = doLdg(inputSliceStart + idx * inputWithinSliceStride);\n      const auto valConverted = at::native::TopKTypeConfig<T>::convert(val);\n      withinK = (largest ? valConverted > kthValueConverted : valConverted < kthValueConverted);\n      kth = (valConverted == kthValueConverted);\n    }\n\n    uint32_t withinKIndex;\n    uint32_t numWithinK;\n    BlockScan(temp_storage).ExclusiveSum(withinK, withinKIndex, numWithinK);\n    __syncthreads();\n    if (withinK) {\n      uint32_t offset = withinKIndex + startWithinK;\n      topKSliceStart[offset * topKWithinSliceStride] = val;\n      indicesSliceStart[offset * indicesWithinSliceStride] = idx;\n    }\n    startWithinK += numWithinK;\n\n    if (startKth < outputSliceSize) {\n      uint32_t kthIndex;\n      uint32_t numKth;\n      BlockScan(temp_storage).ExclusiveSum(kth, kthIndex, numKth);\n      __syncthreads();\n      if (kth) {\n        uint32_t offset = kthIndex + startKth;\n        if (offset < outputSliceSize) {\n          topKSliceStart[offset * topKWithinSliceStride] = val;\n          indicesSliceStart[offset * indicesWithinSliceStride] = idx;\n        }\n      }\n      startKth += numKth;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_trilinear3d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_trilinear3d_out_frame(\n    const int n,\n    const accscalar_t rdepth,\n    const accscalar_t rheight,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    const PackedTensorAccessor64<const scalar_t, 5> idata,\n    PackedTensorAccessor64<scalar_t, 5> odata) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int depth1 = idata.size(2);\n  const int height1 = idata.size(3);\n  const int width1 = idata.size(4);\n  const int depth2 = odata.size(2);\n  const int height2 = odata.size(3);\n  const int width2 = odata.size(4);\n\n  if (index < n) {\n    const int w2 = (index % (height2 * width2)) % width2; // 0:width2-1\n    const int h2 = (index % (height2 * width2)) / width2; // 0:height2-1\n    const int t2 = index / (height2 * width2); // 0:depth2-1\n    // special case: just copy\n    if (depth1 == depth2 && height1 == height2 && width1 == width2) {\n      const int t1 = t2;\n      const int h1 = h2;\n      const int w1 = w2;\n\n      for (int n = 0; n < batchsize; n++) {\n        for (int c = 0; c < channels; ++c) {\n          const scalar_t val = idata[n][c][t1][h1][w1];\n          odata[n][c][t2][h2][w2] = val;\n        }\n      }\n      return;\n    }\n    //\n    const accscalar_t t1r = area_pixel_compute_source_index<accscalar_t>(\n        rdepth, t2, align_corners, /*cubic=*/false);\n    const int t1 = t1r;\n    const int t1p = (t1 < depth1 - 1) ? 1 : 0;\n    const accscalar_t t1lambda = t1r - t1;\n    const accscalar_t t0lambda = static_cast<accscalar_t>(1) - t1lambda;\n    //\n    const accscalar_t h1r = area_pixel_compute_source_index<accscalar_t>(\n        rheight, h2, align_corners, /*cubic=*/false);\n    const int h1 = h1r;\n    const int h1p = (h1 < height1 - 1) ? 1 : 0;\n    const accscalar_t h1lambda = h1r - h1;\n    const accscalar_t h0lambda = static_cast<accscalar_t>(1) - h1lambda;\n    //\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n    //\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; ++c) {\n        const accscalar_t val = t0lambda *\n                (h0lambda *\n                     (w0lambda * idata[n][c][t1][h1][w1] +\n                      w1lambda * idata[n][c][t1][h1][w1 + w1p]) +\n                 h1lambda *\n                     (w0lambda * idata[n][c][t1][h1 + h1p][w1] +\n                      w1lambda * idata[n][c][t1][h1 + h1p][w1 + w1p])) +\n            t1lambda *\n                (h0lambda *\n                     (w0lambda * idata[n][c][t1 + t1p][h1][w1] +\n                      w1lambda * idata[n][c][t1 + t1p][h1][w1 + w1p]) +\n                 h1lambda *\n                     (w0lambda * idata[n][c][t1 + t1p][h1 + h1p][w1] +\n                      w1lambda * idata[n][c][t1 + t1p][h1 + h1p][w1 + w1p]));\n        odata[n][c][t2][h2][w2] = static_cast<scalar_t>(val);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_trilinear3d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_trilinear3d_backward_out_frame(\n    const int num_kernels,\n    const accscalar_t rdepth,\n    const accscalar_t rheight,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    PackedTensorAccessor64<scalar_t, 5> idata,\n    const PackedTensorAccessor64<const scalar_t, 5> odata,\n    scalar_t* idata_ptr) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int depth1 = idata.size(2);\n  const int height1 = idata.size(3);\n  const int width1 = idata.size(4);\n  const int depth2 = odata.size(2);\n  const int height2 = odata.size(3);\n  const int width2 = odata.size(4);\n\n  const size_t i_numel = batchsize * channels * depth1 * height1 * width1;\n\n  if (index < num_kernels) {\n    const int w2 = (index % (height2 * width2)) % width2; // 0:width2-1\n    const int h2 = (index % (height2 * width2)) / width2; // 0:height2-1\n    const int t2 = index / (height2 * width2); // 0:depth2-1\n    // special case: just copy\n    if (depth1 == depth2 && height1 == height2 && width1 == width2) {\n      const int t1 = t2;\n      const int h1 = h2;\n      const int w1 = w2;\n\n      for (int n = 0; n < batchsize; n++) {\n        for (int c = 0; c < channels; ++c) {\n          const scalar_t val = odata[n][c][t1][h1][w1];\n          idata[n][c][t2][h2][w2] = val;\n        }\n      }\n      return;\n    }\n    //\n    const accscalar_t t1r = area_pixel_compute_source_index<accscalar_t>(\n        rdepth, t2, align_corners, /*cubic=*/false);\n    const int t1 = t1r;\n    const int t1p = (t1 < depth1 - 1) ? 1 : 0;\n    const accscalar_t t1lambda = t1r - t1;\n    const accscalar_t t0lambda = static_cast<accscalar_t>(1) - t1lambda;\n    //\n    const accscalar_t h1r = area_pixel_compute_source_index<accscalar_t>(\n        rheight, h2, align_corners, /*cubic=*/false);\n    const int h1 = h1r;\n    const int h1p = (h1 < height1 - 1) ? 1 : 0;\n    const accscalar_t h1lambda = h1r - h1;\n    const accscalar_t h0lambda = static_cast<accscalar_t>(1) - h1lambda;\n    //\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n    //\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; ++c) {\n        const scalar_t d2val = odata[n][c][t2][h2][w2];\n        const size_t nc = n * channels + c;\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1, h1, w1),\n          i_numel,\n          static_cast<scalar_t>(t0lambda * h0lambda * w0lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1, h1, w1 + w1p),\n          i_numel,\n          static_cast<scalar_t>(t0lambda * h0lambda * w1lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1, h1 + h1p, w1),\n          i_numel,\n          static_cast<scalar_t>(t0lambda * h1lambda * w0lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1, h1 + h1p, w1 + w1p),\n          i_numel,\n          static_cast<scalar_t>(t0lambda * h1lambda * w1lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1 + t1p, h1, w1),\n          i_numel,\n          static_cast<scalar_t>(t1lambda * h0lambda * w0lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1 + t1p, h1, w1 + w1p),\n          i_numel,\n          static_cast<scalar_t>(t1lambda * h0lambda * w1lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1 + t1p, h1 + h1p, w1),\n          i_numel,\n          static_cast<scalar_t>(t1lambda * h1lambda * w0lambda * d2val),\n          true);\n        fastAtomicAdd(\n          idata_ptr,\n          idx_3d(nc, depth1, height1, width1, t1 + t1p, h1 + h1p, w1 + w1p),\n          i_numel,\n          static_cast<scalar_t>(t1lambda * h1lambda * w1lambda * d2val),\n          true);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "searchsorted_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void searchsorted_cuda_kernel(\n  output_t *data_out,\n  const input_t *data_in,\n  const input_t *data_bd,\n  const int64_t *data_sort,\n  int64_t idim_in,\n  int64_t idim_bd,\n  int64_t numel_in,\n  bool right,\n  bool is_1d_boundaries) {\n\n  for (int64_t tid = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x; tid < numel_in; tid += blockDim.x * gridDim.x) {\n    // If boundaries tensor is 1d, we always search the entire boundary tensor\n    int64_t start_bd = is_1d_boundaries ? 0 : tid / idim_in * idim_bd;\n    int64_t end_bd = start_bd + idim_bd;\n\n    int64_t pos = !right ?\n      lower_bound<input_t>(data_bd, start_bd, end_bd, data_in[tid], data_sort) - start_bd :\n      upper_bound<input_t>(data_bd, start_bd, end_bd, data_in[tid], data_sort) - start_bd;\n\n    // type conversion might happen here\n    data_out[tid] = pos;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "index_elementwise_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void index_elementwise_kernel(const int64_t N, const func_t f) {\n  const auto tid = threadIdx.x;\n  const auto nv = nt * vt;\n  auto idx = nv * blockIdx.x + tid;\n  #pragma unroll\n  for (int i = 0; i < vt; i++) {\n    if (idx < N) {\n      f(idx);\n      idx += nt;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "masked_scatter_size_check",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void masked_scatter_size_check(\n  const int64_t* const mask_exclusive_sum,\n  const bool* const mask,\n  const int64_t srcSize) {\n  // Convert exclusive sum to inclusive sum\n  const auto totalElements = *mask_exclusive_sum + *mask;\n  CUDA_KERNEL_ASSERT(totalElements <= srcSize);\n}",
      "disabled": true
    },
    {
      "kernel_name": "_elemwise_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _elemwise_kernel(int total_n_elems, func_t f) {\n  constexpr int total_work_block = n_threads * n_elems_per_thread;\n  int idx = total_work_block * blockIdx.x + threadIdx.x;\n\n  #pragma unroll\n  for (int i = 0; i < n_elems_per_thread; ++i) {\n    if (idx < total_n_elems) {\n      f(idx);\n      idx += n_threads;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cross_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void cross_kernel(\n    int numel, T* out, const T* x1, const T* x2, OffsetCalc offset_calculator,\n    StrideType ostride, StrideType x1stride, StrideType x2stride) {\n  CUDA_KERNEL_LOOP(i, numel) {\n    const auto offsets = offset_calculator.get(i);\n    auto* out_row = out + offsets[0];\n    const auto* x1_row = x1 + offsets[1];\n    const auto* x2_row = x2 + offsets[2];\n\n    const T val0 = (x1_row[1 * x1stride] * x2_row[2 * x2stride] -\n                    x1_row[2 * x1stride] * x2_row[1 * x2stride]);\n\n    const T val1 = (x1_row[2 * x1stride] * x2_row[0 * x2stride] -\n                    x1_row[0 * x1stride] * x2_row[2 * x2stride]);\n\n    const T val2 = (x1_row[0 * x1stride] * x2_row[1 * x2stride] -\n                    x1_row[1 * x1stride] * x2_row[0 * x2stride]);\n\n\n    out_row[0 * ostride] = val0;\n    out_row[1 * ostride] = val1;\n    out_row[2 * ostride] = val2;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "RowwiseMomentsCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void RowwiseMomentsCUDAKernel(\n    int64_t N,\n    T_ACC eps,\n    const T* X,\n    T_ACC* mean,\n    T_ACC* rstd) {\n  using WelfordType = WelfordData<T_ACC, int64_t>;\n  using WelfordOp =\n      WelfordOps<T_ACC, T_ACC, int64_t, thrust::pair<T_ACC, T_ACC>>;\n\n  __shared__\n      typename std::aligned_storage<sizeof(WelfordType), alignof(WelfordType)>::\n          type val_shared[C10_WARP_SIZE];\n  WelfordType* val_shared_ptr = reinterpret_cast<WelfordType*>(val_shared);\n\n  const int64_t i = blockIdx.x;\n  WelfordOp welford_op = {/*correction=*/0, /*take_sqrt=*/false};\n  WelfordType val(0, 0, 0, 0);\n\n  for (int64_t j = threadIdx.x; j < N; j += blockDim.x) {\n    const int64_t index = i * N + j;\n    val = welford_op.reduce(val, static_cast<T_ACC>(X[index]), index);\n  }\n  val = cuda_utils::BlockReduce(\n      val,\n      welford_op,\n      /*identity_element=*/WelfordType(0, 0, 0, 0),\n      val_shared_ptr);\n\n  if (threadIdx.x == 0) {\n    T_ACC m1;\n    T_ACC m2;\n    thrust::tie(m2, m1) = welford_op.project(val);\n    mean[i] = m1;\n    rstd[i] = c10::cuda::compat::rsqrt(m2 + eps);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "LayerNormForwardCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void LayerNormForwardCUDAKernel(\n    int64_t N,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    const T* gamma,\n    const T* beta,\n    T* Y) {\n  const int64_t i = blockIdx.x;\n  for (int64_t j = threadIdx.x; j < N; j += blockDim.x) {\n    const int64_t index = i * N + j;\n    const T_ACC gamma_v =\n        gamma == nullptr ? T_ACC(1) : static_cast<T_ACC>(gamma[j]);\n    const T_ACC beta_v =\n        beta == nullptr ? T_ACC(0) : static_cast<T_ACC>(beta[j]);\n    Y[index] = (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n            static_cast<T_ACC>(rstd[i]) * gamma_v +\n        beta_v;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "vectorized_layer_norm_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void vectorized_layer_norm_kernel(\n  const int N,\n  T_ACC eps,\n  const  T* __restrict__ X,\n  const  T* gamma,\n  const  T* beta,\n  T_ACC* mean,\n  T_ACC* rstd,\n  T* Y){\n    vectorized_layer_norm_kernel_impl(N, eps, X, gamma, beta, mean, rstd, Y);\n  }",
      "disabled": true
    },
    {
      "kernel_name": "layer_norm_grad_input_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void layer_norm_grad_input_kernel(\n  const T* __restrict__ dY,\n  const T* __restrict__ X,\n  const T_ACC* __restrict__ mean,\n  const T_ACC* __restrict__ rstd,\n  const T* __restrict__ gamma,\n  T*  dX,\n  const int N){\n    alignas(sizeof(double)) extern __shared__ char s_data1[];\n    T_ACC * buf = reinterpret_cast<T_ACC*>(&s_data1);\n\n    compute_gI(dY, X, mean, rstd, gamma, dX, N, buf);\n  }",
      "disabled": true
    },
    {
      "kernel_name": "layer_norm_grad_input_kernel_vectorized",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void layer_norm_grad_input_kernel_vectorized(\n  const T* __restrict__ dY,\n  const T* __restrict__ X,\n  const T_ACC* __restrict__ mean,\n  const T_ACC* __restrict__ rstd,\n  const T* __restrict__ gamma,\n  T* dX,\n  const int N) {\n  alignas(sizeof(double)) extern __shared__ char shared_data[];\n  T_ACC* reduce_buf = reinterpret_cast<T_ACC*>(&shared_data);\n\n  const auto bIdx = blockIdx.x;\n  const T_ACC mean_val = mean[bIdx];\n  const T_ACC rstd_val = rstd[bIdx];\n  const T* X_i = X + bIdx * N;\n  const T* dY_i = dY + bIdx * N;\n  T* dX_i = dX + bIdx * N;\n\n  using vec_t = aligned_vector<T, vec_size>;\n  const vec_t* const X_i_vec_ptr = reinterpret_cast<const vec_t*>(X_i);\n  const vec_t* const dY_i_vec_ptr = reinterpret_cast<const vec_t*>(dY_i);\n  const vec_t* const gamma_vec_ptr = (gamma != nullptr) ? reinterpret_cast<const vec_t*>(gamma) : nullptr;\n  vec_t* const dX_i_vec = reinterpret_cast<vec_t*>(dX_i);\n\n  vec_t X_i_vec_reg, dY_i_vec_reg, gamma_vec_reg, dX_i_vec_reg;\n  for (int k = 0; k < vec_size; ++k) {\n    gamma_vec_reg.val[k] = T(1);\n  }\n\n  T_ACC stats_x1{0}, stats_x2{0};\n  unsigned int l = threadIdx.x * vec_size;\n  for (; l + vec_size - 1 < N; l += blockDim.x * vec_size) {\n    unsigned int vec_idx = l / vec_size;\n    if (gamma != nullptr) {\n      gamma_vec_reg = gamma_vec_ptr[vec_idx];\n    }\n\n    X_i_vec_reg = X_i_vec_ptr[vec_idx];\n    dY_i_vec_reg = dY_i_vec_ptr[vec_idx];\n\n    for (int k = 0; k < vec_size; ++k) {\n      const auto gamma_val = static_cast<T_ACC>(gamma_vec_reg.val[k]);\n      const auto c_h = static_cast<T_ACC>(X_i_vec_reg.val[k]);\n      const auto c_loss = static_cast<T_ACC>(dY_i_vec_reg.val[k]);\n      stats_x1 += c_loss * gamma_val;\n      stats_x2 += c_loss * gamma_val * (c_h - mean_val) * rstd_val;\n    }\n  }\n\n  // Tail Loop\n  for (; l < N; l++) {\n    const auto gamma_val = (gamma != nullptr) ? static_cast<T_ACC>(gamma[l]) : T_ACC(1);\n    const auto c_h = static_cast<T_ACC>(X_i[l]);\n    const auto c_loss = static_cast<T_ACC>(dY_i[l]);\n    stats_x1 += c_loss * gamma_val;\n    stats_x2 += c_loss * gamma_val * (c_h - mean_val) * rstd_val;\n  }\n\n  // Reduction in Shared Memory\n  stats_x1 = cuda_utils::BlockReduceSum(stats_x1, reduce_buf);\n  stats_x2 = cuda_utils::BlockReduceSum(stats_x2, reduce_buf);\n  if (threadIdx.x == 0) {\n    reduce_buf[0] = stats_x1;\n    reduce_buf[1] = stats_x2;\n  }\n  __syncthreads();\n  stats_x1 = reduce_buf[0];\n  stats_x2 = reduce_buf[1];\n\n  T_ACC fH = N;\n  T_ACC term1 = (T_ACC(1) / fH) * rstd_val;\n\n  l = threadIdx.x * vec_size;\n  for (; l + vec_size - 1 < N; l += blockDim.x * vec_size) {\n    unsigned int vec_idx = l / vec_size;\n    if (gamma != nullptr) {\n      gamma_vec_reg = gamma_vec_ptr[vec_idx];\n    }\n\n    X_i_vec_reg = X_i_vec_ptr[vec_idx];\n    dY_i_vec_reg = dY_i_vec_ptr[vec_idx];\n\n    for (int k = 0; k < vec_size; ++k) {\n      const auto gamma_val = static_cast<T_ACC>(gamma_vec_reg.val[k]);\n      const auto x = static_cast<T_ACC>(X_i_vec_reg.val[k]);\n      const auto dy = static_cast<T_ACC>(dY_i_vec_reg.val[k]);\n\n      T_ACC f_grad_input = fH * gamma_val * dy;\n      f_grad_input -= (x - mean_val) * rstd_val * stats_x2;\n      f_grad_input -= stats_x1;\n      f_grad_input *= term1;\n      dX_i_vec_reg.val[k] = f_grad_input;\n    }\n\n    dX_i_vec[vec_idx] = dX_i_vec_reg;\n  }\n\n  // Tail Loop\n  for (; l < N; l += blockDim.x) {\n    const auto x = X_i[l];\n    const auto dy = dY_i[l];\n    const auto gamma_val = (gamma != nullptr) ? static_cast<T_ACC>(gamma[l]) : T_ACC(1);\n\n    T_ACC f_grad_input = fH * gamma_val * dy;\n    f_grad_input -= (x - mean_val) * rstd_val * stats_x2;\n    f_grad_input -= stats_x1;\n    f_grad_input *= term1;\n    dX_i[l] = f_grad_input;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "GammaBetaBackwardSimpleCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void GammaBetaBackwardSimpleCUDAKernel(\n    int64_t M,\n    int64_t N,\n    const T* dY,\n    const T* X,\n    const T_ACC* mean,\n    const T_ACC* rstd,\n    T* dg,\n    T* db) {\n  const int64_t j = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (j < N) {\n    T_ACC sum1 = 0;\n    T_ACC sum2 = 0;\n    for (int64_t i = 0; i < M; ++i) {\n      const int64_t index = i * N + j;\n      sum1 += dg == nullptr ? T_ACC(0)\n                            : static_cast<T_ACC>(dY[index]) *\n              (static_cast<T_ACC>(X[index]) - static_cast<T_ACC>(mean[i])) *\n              static_cast<T_ACC>(rstd[i]);\n      sum2 += db == nullptr ? T_ACC(0) : static_cast<T_ACC>(dY[index]);\n    }\n    if (dg != nullptr) {\n      dg[j] = sum1;\n    }\n    if (db != nullptr) {\n      db[j] = sum2;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "__launch_bounds__",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid\n__launch_bounds__(block_dim_x * block_dim_y)\n GammaBetaBackwardCUDAKernelTemplate(\n    int64_t M,\n    int64_t N,\n    const T* __restrict__ dY,\n    const T* __restrict__ X,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T* __restrict__ dg,\n    T* __restrict__ db) {\n  // This assert is a compile-time check only.\n  constexpr int rows_per_thread_y = rows_per_block_y / block_dim_y;\n  static_assert(rows_per_thread_y <= kWarpSize);\n\n  T_ACC dg_sum = 0;\n  T_ACC db_sum = 0;\n\n  if (aligned_grid) {\n    // When N and M align perfectly with block_dim_x and block_dim_y, we\n    // can skip boundary condition checks that waste instruction issue slots.\n    blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, false>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n  } else {\n    // In the general case we need to check boundary conditions in the M\n    // dimension. However, we can still avoid boundary checks in the N dimension\n    // for the inner blocks. So try to avoid those checks when possible.\n    if (blockIdx.x * block_dim_x + block_dim_x - 1 < N) {\n      blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, false, true>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    } else {\n      blockReduceGammaBetaBackwardsWithChecks\n          <T, T_ACC, block_dim_x, block_dim_y, rows_per_block_y, true, true>\n          (M, N, dY, X, mean, rstd, dg, db, dg_sum, db_sum);\n    }\n  }\n\n  int64_t thread_x = ((int64_t)blockIdx.x) * block_dim_x + threadIdx.x;\n\n  // When partial_reduction is requested, we don't reduce within a block.\n  // We also don't reduce if we are only a single block in the y dimension.\n  if (partial_reduction || (blockDim.y == 1 && gridDim.y == 1)) {\n    if (aligned_grid || thread_x < N) {\n      int64_t thread_y = ((int64_t)blockIdx.y) * blockDim.y + threadIdx.y;\n      if (dg) {\n        dg[thread_y * N + thread_x] = dg_sum;\n      }\n      if (db) {\n        db[thread_y * N + thread_x] = db_sum;\n      }\n    }\n  } else {\n    // The caller requested a full reduction so we must reduce across\n    // warps using shared memory and warp shuffles.\n    static_assert(rows_per_thread_y <= C10_WARP_SIZE);\n    alignas(sizeof(double)) extern __shared__ char s_data1[];\n    T_ACC* s_data_typed = reinterpret_cast<T_ACC*>(&s_data1);\n    T_ACC* s_dg;\n    T_ACC* s_db;\n    int padded_bx = (block_dim_x + 1);\n    // Transpose dg and db.\n    s_dg = s_data_typed;\n    s_db = s_data_typed + (padded_bx * block_dim_y);\n    s_dg[threadIdx.y * padded_bx + threadIdx.x] = dg_sum;\n    s_db[threadIdx.y * padded_bx + threadIdx.x] = db_sum;\n    __syncthreads();\n\n    // Load transposed so that a warp holds an entire column\n    // Because block_dim_x != block_dim_y in the general case, we need\n    // some code to handle the general case.\n    static_assert(block_dim_x * block_dim_y % C10_WARP_SIZE == 0);\n    constexpr int warps_available_to_reduce = block_dim_x * block_dim_y / C10_WARP_SIZE;\n    int thread_id = threadIdx.y * block_dim_x + threadIdx.x;\n    int warp_id = thread_id / C10_WARP_SIZE;\n    int lane_id = thread_id & (C10_WARP_SIZE - 1);\n    #pragma unroll\n    for (int i = warp_id; i < block_dim_x; i += warps_available_to_reduce) {\n      T_ACC reg_db, reg_dg;\n      if (lane_id < block_dim_y) {\n        reg_dg = s_dg[lane_id * padded_bx + i];\n        reg_db = s_db[lane_id * padded_bx + i];\n      }\n      #pragma unroll\n      for (unsigned delta = block_dim_y >> 1; delta >= 1; delta >>= 1) {\n        reg_dg += WARP_SHFL_XOR(reg_dg, delta, kWarpSize);\n        reg_db += WARP_SHFL_XOR(reg_db, delta, kWarpSize);\n      }\n      // Reduce is done. Now write it out to global memory.\n      int64_t out_index = ((int64_t)blockIdx.x) * block_dim_x + i;\n      if (threadIdx.x == 0 && (aligned_grid || out_index < N)) {\n        if (dg) {\n          dg[out_index] = reg_dg;\n        }\n        if (db) {\n          db[out_index] = reg_db;\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cuComputePartGradGammaBeta",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid cuComputePartGradGammaBeta(\n    const T* __restrict__ dout,\n    const T* __restrict__ input,\n    const int64_t M,\n    const int64_t N,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    T_ACC* part_grad_gamma,\n    T_ACC* part_grad_beta)\n{\n    const int numsegs_M = (M+blockDim.y*blockDim.y-1) / (blockDim.y*blockDim.y);\n    const int segs_per_block = (numsegs_M + gridDim.y - 1) / gridDim.y;\n    const int i1_beg = blockIdx.y * segs_per_block * blockDim.y*blockDim.y;\n    const int i1_beg_plus_one = (blockIdx.y+1) * segs_per_block * blockDim.y*blockDim.y;\n    const int i1_end = i1_beg_plus_one < M ? i1_beg_plus_one : M;\n    const int row_stride = blockDim.x+1;\n    const int thr_load_col_off = (threadIdx.x*blockDim.y)&(blockDim.x-1);\n    const int thr_load_row_off = (threadIdx.x*blockDim.y)/blockDim.x + threadIdx.y*blockDim.y;\n    const int i2_off = blockIdx.x * blockDim.x + thr_load_col_off;\n    alignas(sizeof(double)) extern __shared__ char shared[];\n    T_ACC * buf = reinterpret_cast<T_ACC*>(&shared); // buf has at least blockDim.x * blockDim.y * blockDim.y + (blockDim.y - 1)*(blockDim.x/blockDim.y) elements\n    T_ACC* warp_buf1 = (T_ACC*)buf;\n    T_ACC* warp_buf2 = warp_buf1 + blockDim.y * blockDim.y * row_stride;\n    // compute partial sums from strided inputs\n    // do this to increase number of loads in flight\n    cuLoadWriteStridedInputs(i1_beg,thr_load_row_off,thr_load_col_off,i2_off,row_stride,warp_buf1,warp_buf2,input,dout,i1_end,N,mean,rstd);\n    for (int i1_block = i1_beg+blockDim.y*blockDim.y;  i1_block < i1_end;  i1_block+=blockDim.y*blockDim.y) {\n      cuLoadAddStridedInputs(i1_block,thr_load_row_off,thr_load_col_off,i2_off,row_stride,warp_buf1,warp_buf2,input,dout,i1_end,N,mean,rstd);\n    }\n    __syncthreads();\n    // inter-warp reductions\n    // sum within each warp\n    T_ACC acc1 = T_ACC(0);\n    T_ACC acc2 = T_ACC(0);\n    for (int k = 0;  k < blockDim.y;  ++k) {\n      int row1 = threadIdx.y + k*blockDim.y;\n      int idx1 = row1*row_stride + threadIdx.x;\n      acc1 += warp_buf1[idx1];\n      acc2 += warp_buf2[idx1];\n    }\n    warp_buf1[threadIdx.y*row_stride+threadIdx.x] = acc1;\n    warp_buf2[threadIdx.y*row_stride+threadIdx.x] = acc2;\n    __syncthreads();\n    // sum all warps\n    for (int offset = blockDim.y/2;  offset > 1;  offset /= 2) {\n      if (threadIdx.y < offset) {\n        int row1 = threadIdx.y;\n        int row2 = threadIdx.y + offset;\n        int idx1 = row1*row_stride + threadIdx.x;\n        int idx2 = row2*row_stride + threadIdx.x;\n        warp_buf1[idx1] += warp_buf1[idx2];\n        warp_buf2[idx1] += warp_buf2[idx2];\n      }\n      __syncthreads();\n    }\n    int i2 = blockIdx.x * blockDim.x + threadIdx.x;\n    if (threadIdx.y == 0 && i2 < N) {\n      int row1 = threadIdx.y;\n      int row2 = threadIdx.y + 1;\n      int idx1 = row1*row_stride + threadIdx.x;\n      int idx2 = row2*row_stride + threadIdx.x;\n      part_grad_beta[blockIdx.y*N+i2] = warp_buf1[idx1] + warp_buf1[idx2];\n      part_grad_gamma[blockIdx.y*N+i2] = warp_buf2[idx1] + warp_buf2[idx2];\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cuComputeGradGammaBeta",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid cuComputeGradGammaBeta(\n    const T_ACC* part_grad_gamma,\n    const T_ACC* part_grad_beta,\n    const int part_size,\n    const int64_t M,\n    const int64_t N,\n    T* grad_gamma,\n    T* grad_beta)\n{\n    // sum partial gradients for gamma and beta\n    alignas(sizeof(double)) extern __shared__ char shared[];\n    T_ACC * buf = reinterpret_cast<T_ACC*>(&shared);\n    int i2 = blockIdx.x * blockDim.x + threadIdx.x;\n\n    // each warp does sequential reductions until reduced part_size is num_warps\n    int num_warp_reductions = part_size / blockDim.y;\n    T_ACC sum_gamma = T_ACC(0);\n    T_ACC sum_beta = T_ACC(0);\n    const T_ACC* part_grad_gamma_ptr = part_grad_gamma + threadIdx.y * num_warp_reductions * N + i2;\n    const T_ACC* part_grad_beta_ptr = part_grad_beta + threadIdx.y * num_warp_reductions * N + i2;\n\n    if (i2 < N) {\n        for (int warp_offset = 0;  warp_offset < num_warp_reductions;  ++warp_offset) {\n          sum_gamma += part_grad_gamma_ptr[warp_offset*N];\n          sum_beta += part_grad_beta_ptr[warp_offset*N];\n        }\n    }\n\n    // inter-warp reductions\n    const int nbsize3 = blockDim.x * blockDim.y / 2;\n    for (int offset = blockDim.y/2;  offset >= 1;  offset /= 2) {\n      // top half write to shared memory\n      if (threadIdx.y >= offset && threadIdx.y < 2*offset) {\n        const int write_idx = (threadIdx.y - offset) * blockDim.x + threadIdx.x;\n        buf[write_idx] = sum_gamma;\n        buf[write_idx+nbsize3] = sum_beta;\n      }\n      __syncthreads();\n      // bottom half sums\n      if (threadIdx.y < offset) {\n        const int read_idx = threadIdx.y * blockDim.x + threadIdx.x;\n        sum_gamma += buf[read_idx];\n        sum_beta += buf[read_idx+nbsize3];\n      }\n      __syncthreads();\n    }\n\n    // write out fully summed gradients\n    if (threadIdx.y == 0 && i2 < N) {\n      if (grad_gamma) {\n          grad_gamma[i2] = sum_gamma;\n      }\n      if (grad_beta) {\n          grad_beta[i2] = sum_beta;\n      }\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "cuComputeGradInput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__\nvoid cuComputeGradInput(\n    const T* __restrict__ dout,\n    const T* __restrict__ input,\n    const int64_t M,\n    const int64_t N,\n    const T_ACC* __restrict__ mean,\n    const T_ACC* __restrict__ rstd,\n    const T* gamma,\n    T* grad_input)\n{\n  for (int i1=blockIdx.y; i1 < M; i1 += gridDim.y) {\n    T_ACC sum_loss1 = T_ACC(0);\n    T_ACC sum_loss2 = T_ACC(0);\n    T_ACC c_mean = mean[i1];\n    const T_ACC c_rstd = rstd[i1];\n    const T* k_input = input + i1*N;\n    const T* k_dout = dout + i1*N;\n    const int numx = blockDim.x * blockDim.y;\n    const int thrx = threadIdx.x + threadIdx.y * blockDim.x;\n    if (gamma != NULL) {\n      // Optimization for ROCm MI100\n      for( int l = 0; l < N ; l += numx) {\n        int idx = l + thrx;\n        const T_ACC gamma_idx = static_cast<T_ACC>((idx<N) ? gamma[idx] : T(0));\n        const T_ACC c_h = static_cast<T_ACC>((idx<N) ? k_input[idx] : T(0));\n        const T_ACC c_loss = static_cast<T_ACC>((idx<N) ? k_dout[idx] : T(0));\n        sum_loss1 += c_loss * gamma_idx;\n        sum_loss2 += c_loss * gamma_idx * (c_h - c_mean) * c_rstd;\n      }\n    } else {\n      for( int l = 0; l < N ; l += numx) {\n        int idx = l + thrx;\n        const T_ACC c_h = static_cast<T_ACC>((idx<N) ? k_input[idx] : T(0));\n        const T_ACC c_loss = static_cast<T_ACC>((idx<N) ? k_dout[idx] : T(0));\n        sum_loss1 += c_loss;\n        sum_loss2 += c_loss * (c_h - c_mean) * c_rstd;\n      }\n    }\n    // intra-warp reductions\n    for (int mask = blockDim.x/2;  mask > 0;  mask /= 2) {\n      sum_loss1 += WARP_SHFL_XOR(sum_loss1, mask);\n      sum_loss2 += WARP_SHFL_XOR(sum_loss2, mask);\n    }\n    // inter-warp reductions\n    if (blockDim.y > 1) {\n      alignas(sizeof(double)) extern __shared__ char shared[];\n      T_ACC * buf = reinterpret_cast<T_ACC*>(&shared);\n      for (int offset = blockDim.y/2;  offset > 0;  offset /= 2) {\n        // upper half of warps write to shared\n        if (threadIdx.y >= offset && threadIdx.y < 2*offset) {\n          const int wrt_i = (threadIdx.y - offset) * blockDim.x + threadIdx.x;\n          buf[2*wrt_i] = sum_loss1;\n          buf[2*wrt_i+1] = sum_loss2;\n        }\n        __syncthreads();\n        // lower half merges\n        if (threadIdx.y < offset) {\n          const int read_i = threadIdx.y * blockDim.x + threadIdx.x;\n          sum_loss1 += buf[2*read_i];\n          sum_loss2 += buf[2*read_i+1];\n        }\n        __syncthreads();\n      }\n      if (threadIdx.y == 0) {\n        buf[2*threadIdx.x] = sum_loss1;\n        buf[2*threadIdx.x+1] = sum_loss2;\n      }\n      __syncthreads();\n      if (threadIdx.y !=0) {\n        sum_loss1 = buf[2*threadIdx.x];\n        sum_loss2 = buf[2*threadIdx.x+1];\n      }\n    }\n    // all threads now have the two sums over l\n    T_ACC fH = (T_ACC)N;\n    T_ACC term1 = (T_ACC(1) / fH) * c_rstd;\n    T* k_grad_input = grad_input + i1*N;\n    if (gamma != NULL) {\n      for (int l = thrx;  l < N;  l+=numx) {\n        const T_ACC c_h = static_cast<T_ACC>(k_input[l]);\n        const T_ACC c_loss = static_cast<T_ACC>(k_dout[l]);\n        T_ACC f_grad_input = fH * c_loss * gamma[l];\n        f_grad_input -= sum_loss1;\n        f_grad_input -= (c_h - c_mean) * c_rstd * sum_loss2;\n        f_grad_input *= term1;\n        k_grad_input[l] = static_cast<T>(f_grad_input);\n      }\n    } else {\n      for (int l = thrx;  l < N;  l+=numx) {\n        const T_ACC c_h = static_cast<T_ACC>(k_input[l]);\n        const T_ACC c_loss = static_cast<T_ACC>(k_dout[l]);\n        T_ACC f_grad_input = fH * c_loss;\n        f_grad_input -= sum_loss1;\n        f_grad_input -= (c_h - c_mean) * c_rstd * sum_loss2;\n        f_grad_input *= term1;\n        k_grad_input[l] = static_cast<T>(f_grad_input);\n      }\n    }\n    // prevent race where buf is written again before reads are done\n    __syncthreads();\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest3d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest3d_out_frame(\n    const scalar_t* input,\n    size_t dim_b,\n    size_t dim_c,\n    size_t src_dim_d,\n    size_t src_dim_h,\n    size_t src_dim_w,\n    size_t dst_dim_d,\n    size_t dst_dim_h,\n    size_t dst_dim_w,\n    scalar_t* output,\n    float depth_scale,\n    float height_scale,\n    float width_scale) {\n\n  int64_t dst_idx = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;\n  if (dst_idx >= dim_c * dst_dim_d * dst_dim_h * dst_dim_w)\n    return;\n\n  int64_t dst_c_stride = dst_dim_d * dst_dim_h * dst_dim_w;\n  int64_t src_c_stride = src_dim_d * src_dim_h * src_dim_w;\n\n  int c = (dst_idx / (dst_c_stride)) % dim_c;\n\n  int dst_z = (dst_idx / dst_dim_h / dst_dim_w) % dst_dim_d;\n  int src_z = nn_compute_source_index_fn(depth_scale, dst_z, src_dim_d);\n  int dst_y = (dst_idx / dst_dim_w) % dst_dim_h;\n  int src_y = nn_compute_source_index_fn(height_scale, dst_y, src_dim_h);\n\n  int dst_x = dst_idx % dst_dim_w;\n  int src_x = nn_compute_source_index_fn(width_scale, dst_x, src_dim_w);\n\n  int64_t src_idx = c * src_c_stride + src_z * src_dim_h * src_dim_w +\n      src_y * src_dim_w + src_x;\n  for (int b = 0; b < dim_b; b++) {\n    output[dst_idx] = input[src_idx];\n    src_idx += dim_c * src_c_stride;\n    dst_idx += dim_c * dst_c_stride;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest3d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest3d_backward_out_frame(\n    const scalar_t* grad_o,\n    size_t dim_b,\n    size_t dim_c,\n    size_t src_dim_d,\n    size_t src_dim_h,\n    size_t src_dim_w,\n    size_t dst_dim_d,\n    size_t dst_dim_h,\n    size_t dst_dim_w,\n    scalar_t* grad_i,\n    float depth_scale,\n    float height_scale,\n    float width_scale) {\n\n  int64_t dst_idx = static_cast<int64_t>(blockIdx.x) * blockDim.x + threadIdx.x;\n  if (dst_idx >= dim_c * dst_dim_d * dst_dim_h * dst_dim_w)\n    return;\n\n  int64_t dst_c_stride = dst_dim_d * dst_dim_h * dst_dim_w;\n  int64_t src_c_stride = src_dim_d * src_dim_h * src_dim_w;\n\n  int c = (dst_idx / (dst_c_stride)) % dim_c;\n\n  int dst_z = (dst_idx / dst_dim_h / dst_dim_w) % dst_dim_d;\n  // note that we do not want to clamp src_z to src_dim_z, since we might\n  // intentionally want to skip in case of scale_factor < 1.0\n  int src_z = nn_bw_compute_source_index_fn(depth_scale, dst_z, src_dim_d);\n  int src_z_up = nn_bw_compute_source_index_fn(depth_scale, dst_z+1, src_dim_d);\n\n  int dst_y = (dst_idx / dst_dim_w) % dst_dim_h;\n  // note that we do not want to clamp src_y to src_dim_y, since we might\n  // intentionally want to skip in case of scale_factor < 1.0\n  int src_y = nn_bw_compute_source_index_fn(height_scale, dst_y, src_dim_h);\n  int src_y_up = nn_bw_compute_source_index_fn(height_scale, dst_y+1, src_dim_h);\n\n  int dst_x = dst_idx % dst_dim_w;\n  // note that we do not want to clamp src_x to src_dim_w, since we might\n  // intentionally want to skip in case of scale_factor < 1.0\n  int src_x = nn_bw_compute_source_index_fn(width_scale, dst_x, src_dim_w);\n  int src_x_up = nn_bw_compute_source_index_fn(width_scale, dst_x+1, src_dim_w);\n\n  for (int b = 0; b < dim_b; b++) {\n    accscalar_t grad = 0;\n    for (int z = src_z; z < src_z_up; z++) {\n      for (int y = src_y; y < src_y_up; y++) {\n        for (int x = src_x; x < src_x_up; x++) {\n          int64_t src_idx = b * dim_c * src_c_stride + c * src_c_stride +\n              z * src_dim_h * src_dim_w + y * src_dim_w + x;\n          grad += grad_o[src_idx];\n        }\n      }\n    }\n    grad_i[dst_idx] = grad;\n    dst_idx += dim_c * dst_c_stride;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "segment_reduce_forward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void segment_reduce_forward_kernel(\n    ReductionType reduction,\n    scalar_t* output_data,\n    const scalar_t* values_data,\n    const index_t* lengths_data,\n    const index_t* lengths_cumsum_data,\n    const int64_t segment_count,\n    const int64_t lengths_stride_axis,\n    bool is_initial_set,\n    scalar_t initial_value,\n    const int64_t outer_offset,\n    const int64_t inner_offset,\n    const int64_t data_stride_axis,\n    const int64_t data_size_axis,\n    const int64_t output_stride_axis,\n    const int64_t output_size_axis,\n    const int64_t lengths_cumsum_stride_axis) {\n  int64_t idx = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (idx >= (outer_offset * segment_count * inner_offset)) {\n    return;\n  }\n  int64_t row_id = idx / inner_offset;\n  int64_t lane_id = idx % inner_offset;   // lane_id is the inner_idx\n  int64_t outer_idx = row_id / segment_count;\n  int64_t dim_idx = row_id % segment_count;\n\n  int64_t offset_idx = outer_idx * lengths_cumsum_stride_axis * (segment_count + 1) + dim_idx;\n  index_t offset_start = lengths_cumsum_data[offset_idx];\n  index_t offset_end = lengths_cumsum_data[offset_idx + 1];\n\n  // ===== step2: apply reduction\n  for (index_t j = offset_start; j < offset_end; ++j) {\n    int64_t data_index = outer_idx * data_stride_axis * data_size_axis\n                         + j * data_stride_axis + lane_id;\n    const auto data = values_data[data_index];\n    // TODO: There is no need to branch with every element\n    if (reduction == ReductionType::MAX) {\n      initial_value =\n          at::_isnan(data) ? data : std::max<scalar_t>(initial_value, data);\n    } else if (\n        reduction == ReductionType::MEAN ||\n        reduction == ReductionType::SUM) {\n      initial_value = initial_value + data;\n    } else if (reduction == ReductionType::MIN) {\n      initial_value =\n          at::_isnan(data) ? data : std::min<scalar_t>(initial_value, data);\n    } else if (\n      reduction == ReductionType::PROD) {\n      initial_value = initial_value * data;\n    }\n  }\n\n  // ===== step3: finalize reduction\n  int64_t lengths_idx = outer_idx * lengths_stride_axis * segment_count + dim_idx;\n  CUDA_KERNEL_ASSERT(lengths_data[lengths_idx] >= 0);\n  if (lengths_data[lengths_idx] == 0 && !is_initial_set &&\n      reduction == ReductionType::MEAN) {\n    initial_value = static_cast<scalar_t>(NAN);\n  } else if (\n      reduction == ReductionType::MEAN && lengths_data[lengths_idx] > 0 &&\n      !at::_isnan(initial_value)) {\n    initial_value = initial_value / lengths_data[lengths_idx];\n  }\n  int64_t output_index = outer_idx * output_stride_axis * output_size_axis\n                         + dim_idx * output_stride_axis + lane_id;\n  output_data[output_index] = initial_value;\n}",
      "disabled": true
    },
    {
      "kernel_name": "segment_reduce_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void segment_reduce_backward_kernel(\n    ReductionType reduction,\n    scalar_t* grad_input_data,\n    const scalar_t* grad_data,\n    const scalar_t* output_data,\n    const scalar_t* values_data,\n    const index_t* lengths_data,\n    const index_t* lengths_cumsum_data,\n    const int64_t segment_count,\n    const int64_t lengths_stride_axis,\n    scalar_t initial_prod_value,\n    const int64_t outer_offset,\n    const int64_t inner_offset,\n    const int64_t data_stride_axis,\n    const int64_t data_size_axis,\n    const int64_t output_stride_axis,\n    const int64_t output_size_axis,\n    const int64_t lengths_cumsum_stride_axis) {\n  int64_t idx = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (idx >= (outer_offset * segment_count * inner_offset)) {\n    return;\n  }\n  int64_t row_id = idx / inner_offset;\n  int64_t lane_id = idx % inner_offset;  // lane_id is the inner_idx\n  int64_t outer_idx = row_id / segment_count;\n  int64_t dim_idx = row_id % segment_count;\n\n  int64_t lengths_idx = outer_idx * lengths_stride_axis * segment_count + dim_idx;\n  auto segment_length = lengths_data[lengths_idx];\n  if (segment_length == 0) {\n    return;\n  }\n\n  int64_t offset_idx = outer_idx * lengths_cumsum_stride_axis * (segment_count + 1) + dim_idx;\n  index_t offset_start = lengths_cumsum_data[offset_idx];\n  index_t offset_end = lengths_cumsum_data[offset_idx + 1];\n\n  int64_t output_index = outer_idx * output_stride_axis * output_size_axis\n                         + dim_idx * output_stride_axis + lane_id;\n\n  if (reduction == ReductionType::MAX ||\n      reduction == ReductionType::MIN) {\n    int64_t counter = 0;\n    for (int64_t j = offset_start; j < offset_end; ++j) {\n      int64_t data_index = outer_idx * data_stride_axis * data_size_axis\n                           + j * data_stride_axis + lane_id;\n      if (at::_isnan(values_data[data_index]) ||\n          values_data[data_index] == output_data[output_index]) {\n        grad_input_data[data_index] = grad_data[output_index];\n        counter++;\n      }\n    }\n    // Average gradient based on number of maximum elements in the\n    // segment\n    if (counter < 2) {\n      return;\n    }\n    for (int64_t j = offset_start; j < offset_end; ++j) {\n      int64_t data_index = outer_idx * data_stride_axis * data_size_axis\n                           + j * data_stride_axis + lane_id;\n      if (grad_input_data[data_index] > 0) {\n        grad_input_data[data_index] =\n            grad_input_data[data_index] / counter;\n      }\n    }\n  } else if (reduction == ReductionType::MEAN) {\n    auto grad_val = grad_data[output_index] / segment_length;\n    for (int64_t j = offset_start; j < offset_end; ++j) {\n      int64_t data_index = outer_idx * data_stride_axis * data_size_axis\n                           + j * data_stride_axis + lane_id;\n      grad_input_data[data_index] = grad_val;\n    }\n  } else if (reduction == ReductionType::SUM) {\n    const auto& grad_val = grad_data[output_index];\n    for (int64_t j = offset_start; j < offset_end; ++j) {\n      int64_t data_index = outer_idx * data_stride_axis * data_size_axis\n                           + j * data_stride_axis + lane_id;\n      grad_input_data[data_index] = grad_val;\n    }\n  } else if (reduction == ReductionType::PROD) {\n    const auto& grad_val = grad_data[output_index] * output_data[output_index];\n    for (int64_t j = offset_start; j < offset_end; ++j) {\n      int64_t data_index = outer_idx * data_stride_axis * data_size_axis\n                           + j * data_stride_axis + lane_id;\n      if (at::_isnan(values_data[data_index]) ||\n          values_data[data_index] == 0) {\n        // explicitly compute exclusive prod\n        scalar_t exclusive_prod = initial_prod_value;\n        int64_t prod_idx;\n        for (int64_t k = offset_start; k < offset_end; ++k) {\n          if (k != j) {\n            prod_idx = outer_idx * data_stride_axis * data_size_axis\n                       + k * data_stride_axis + lane_id;\n            exclusive_prod *= values_data[prod_idx];\n          }\n        }\n        grad_input_data[data_index] = grad_data[output_index] * exclusive_prod;\n      } else {\n        grad_input_data[data_index] = grad_val / values_data[data_index];\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "split_with_sizes_copy_out_contiguous_no_cast_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void split_with_sizes_copy_out_contiguous_no_cast_kernel(\n    char** dst_base_addrs,\n    char** src_base_addrs,\n    int64_t* split_chunk_sizes,\n    int64_t* block_idx_to_split_idx,\n    int64_t* blocks_cumsums,\n    int64_t src_stride,\n    int64_t num_chunks) {\n  const int64_t split_idx = block_idx_to_split_idx[blockIdx.x];\n  const int64_t split_blocks =\n      blocks_cumsums[split_idx + 1] - blocks_cumsums[split_idx];\n  const int64_t split_threads = split_blocks * blockDim.x;\n  const int64_t split_thread_idx =\n      (blockIdx.x - blocks_cumsums[split_idx]) * blockDim.x + threadIdx.x;\n  const int64_t split_chunk_size = split_chunk_sizes[split_idx];\n\n  char* dst_base_addr = dst_base_addrs[split_idx];\n  char* src_base_addr = src_base_addrs[split_idx];\n\n  for (int64_t i = blockIdx.y; i < num_chunks; i += gridDim.y) {\n    copy_chunk(\n        dst_base_addr + i * split_chunk_size,\n        src_base_addr + i * src_stride,\n        split_chunk_size,\n        split_thread_idx,\n        split_threads);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "chunk_cat_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void chunk_cat_cuda_kernel(\n    src_t** src,\n    dst_t* dst,\n    int64_t* block_idx_to_tensor_idx,\n    int64_t* tensor_idx_to_start_tensor_bytes,\n    int64_t* start_block_idx_per_tensor_chunk,\n    int64_t* actual_tensor_sizes,\n    int64_t* pad_tensor_chunk_sizes,\n    int64_t* num_blocks_per_tensor_chunk,\n    int64_t slice_size,\n    int64_t chunk_size,\n    int64_t dst_to_src_ratio) {\n  const int64_t slice_idx = blockIdx.z;\n  const int64_t chunk_idx = blockIdx.y;\n  const int64_t tensor_idx = block_idx_to_tensor_idx[blockIdx.x];\n  const int64_t tile_idx =\n      blockIdx.x - start_block_idx_per_tensor_chunk[tensor_idx];\n  // Number of threads for the `tensor_idx`-th tensor chunk.\n  const int64_t num_threads =\n      num_blocks_per_tensor_chunk[tensor_idx] * BLOCK_SIZE;\n  const int64_t thread_idx = tile_idx * BLOCK_SIZE + threadIdx.x;\n  char* src_addr = reinterpret_cast<char**>(src)[tensor_idx] +\n      slice_idx * actual_tensor_sizes[tensor_idx] +\n      chunk_idx * pad_tensor_chunk_sizes[tensor_idx] / dst_to_src_ratio;\n  char* dst_addr = reinterpret_cast<char*>(dst) + slice_idx * slice_size +\n      chunk_idx * chunk_size + tensor_idx_to_start_tensor_bytes[tensor_idx];\n  // Compute the actual number of bytes to copy from src.\n  const int64_t actual_copy_size = std::min(\n      pad_tensor_chunk_sizes[tensor_idx] / dst_to_src_ratio,\n      std::max(\n          (int64_t)0,\n          actual_tensor_sizes[tensor_idx] -\n              chunk_idx * pad_tensor_chunk_sizes[tensor_idx] /\n                  dst_to_src_ratio));\n  copy_chunk_with_pad<dst_t, src_t>(\n      reinterpret_cast<dst_t*>(dst_addr),\n      reinterpret_cast<src_t*>(src_addr),\n      pad_tensor_chunk_sizes[tensor_idx],\n      actual_copy_size,\n      thread_idx,\n      num_threads);\n}",
      "disabled": true
    },
    {
      "kernel_name": "matrix_to_m16n8k16_Bint4_layout",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void matrix_to_m16n8k16_Bint4_layout(\n    // size [n][k / 2]\n    const at::PackedTensorAccessor32<uint8_t, 2, at::RestrictPtrTraits> in,\n    // size [ceil(n / 8)][ceil(k / (InnerKTiles * 16))][32][InnerKTiles / 2]\n    at::PackedTensorAccessor32<int32_t, 4, at::RestrictPtrTraits> out) {\n  // int4 values are packed into int32 values, which require at least 8. Given\n  // m16n8k16 B layout requires 4 scalar values/lane, the minimum number of\n  // innermost k-tiles that we can use is 2.\n  static_assert(InnerKTiles >= 2 && isPowerOf2(InnerKTiles), \"\");\n\n#if defined(USE_ROCM)\n  constexpr int32_t kNTileSize = 16;\n#else\n  constexpr int32_t kNTileSize = 8;\n#endif\n  constexpr int32_t kKTileSize = 16;\n\n  // gridDim.x corresponds to the number of k-tiles divided by InnerKTiles\n  auto kOuterTile = blockIdx.x;\n  auto nTile = blockIdx.y;\n  auto t = threadIdx.x;\n\n  // Two k-tiles are packed into an int32 at a time\n#pragma unroll\n  for (int innerKTile = 0; innerKTile < InnerKTiles; innerKTile += 2) {\n    // n dimension that this lane loads from\n#if defined(USE_ROCM)\n    auto n0 = nTile * kNTileSize + (t % kNTileSize);\n#else\n    auto n0 = nTile * kNTileSize + (t / 4);\n#endif\n\n    bool n0Valid = n0 < in.size(0);\n\n    // Four uint8 are packed into an int32\n    int32_t ks[4];\n\n    auto kBase0 = (kOuterTile * InnerKTiles + innerKTile) * kKTileSize / 2;\n\n#if defined(USE_ROCM)\n    ks[0] = kBase0 + (t / kNTileSize) * 2;\n    ks[1] = ks[0] + 1;\n\n    auto kBase1 = kBase0 + kKTileSize / 2;\n    ks[2] = kBase1 + (t / kNTileSize) * 2;\n    ks[3] = ks[2] + 1;\n#else\n    ks[0] = kBase0 + t % 4;\n    ks[1] = ks[0] + 4;\n\n    auto kBase1 = kBase0 + kKTileSize / 2;\n    ks[2] = kBase1 + t % 4;\n    ks[3] = ks[2] + 4;\n#endif\n\n    auto pIn = &in[n0][0];\n\n    uint8_t v[4];\n#pragma unroll\n    for (int i = 0; i < 4; ++i) {\n      v[i] = (n0Valid && ks[i] < in.size(1)) ? pIn[ks[i]] : uint8_t(0);\n    }\n\n    // To clearly explain the packed result with 8 int4 values (4 uint8)\n    // into one int32, we use the follow figure:\n    // [n][k]     int32: v[0] v[1] v[2] v[3] v[4] v[5] v[6] v[7]\n    // [n][k / 2] uint8:    v[0]     v[1]      v[2]      v[3]\n    // When using int32 weight as input, the packed result is consisted of\n    // v[7] | v[5] | v[3] | v[1] | v[6] | v[4] | v[2] | v[0],\n    // which epuals to\n    // v[3]L | v[2]L | v[1]L | v[0]L | v[3]H | v[2]H | v[1]H | v[0]H\n    // when using uint8 weight as input.\n    int32_t pack = ((uint32_t)(v[3] & 0xF) << 28) |\n        ((uint32_t)(v[2] & 0xF) << 24) | ((uint32_t)(v[1] & 0xF) << 20) |\n        ((uint32_t)(v[0] & 0xF) << 16) | ((uint32_t)(v[3] & 0xF0) << 8) |\n        ((uint32_t)(v[2] & 0xF0) << 4) | ((uint32_t)(v[1] & 0xF0)) |\n        ((uint32_t)(v[0] & 0xF0) >> 4);\n\n    // inner k-tiles pack two at a time\n#if defined(USE_ROCM)\n    // The output tensor shape is [ceil(n / 8)][ceil(k / (InnerKTiles * 16))][32][InnerKTiles / 2], which is specific to Nvidia\n    // But AMD needs [ceil(n / 16)][ceil(k / (InnerKTiles * 16))][64][InnerKTiles / 2]\n    // So construct the pointer accordingly\n    auto bPtr = out.data() +\n      ((nTile * out.size(1) * kWarpSize * (InnerKTiles / 2)) +\n        (kOuterTile * kWarpSize * (InnerKTiles / 2)) +\n          (t * (InnerKTiles / 2)) +\n            (innerKTile / 2));\n    *bPtr = pack;\n#else\n    out[nTile][kOuterTile][t][innerKTile / 2] = pack;\n#endif\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_linear1d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_linear1d_out_frame(\n    const int n,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    const PackedTensorAccessor64<const scalar_t, 3> idata,\n    PackedTensorAccessor64<scalar_t, 3> odata) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int width1 = idata.size(2);\n  const int width2 = odata.size(2);\n\n  if (index < n) {\n    const int w2 = index % width2;\n    // special case: just copy\n    if (width1 == width2) {\n      const int w1 = w2;\n      for (int n = 0; n < batchsize; n++) {\n        for (int c = 0; c < channels; ++c) {\n          const scalar_t val = idata[n][c][w1];\n          odata[n][c][w2] = val;\n        }\n      }\n      return;\n    }\n    //\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n    //\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; ++c) {\n        const accscalar_t val =\n            w0lambda * idata[n][c][w1] + w1lambda * idata[n][c][w1 + w1p];\n        odata[n][c][w2] = static_cast<scalar_t>(val);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_linear1d_out_frame_backward",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_linear1d_out_frame_backward(\n    const int n,\n    const accscalar_t rwidth,\n    const bool align_corners,\n    PackedTensorAccessor64<scalar_t, 3> idata,\n    const PackedTensorAccessor64<const scalar_t, 3> odata) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int width1 = idata.size(2);\n  const int width2 = odata.size(2);\n\n  if (index < n) {\n    const int w2 = index % width2;\n    // special case: just copy\n    if (width1 == width2) {\n      const int w1 = w2;\n      for (int n = 0; n < batchsize; n++) {\n        for (int c = 0; c < channels; ++c) {\n          const scalar_t val = odata[n][c][w1];\n          idata[n][c][w2] = val;\n        }\n      }\n      return;\n    }\n    //\n    const accscalar_t w1r = area_pixel_compute_source_index<accscalar_t>(\n        rwidth, w2, align_corners, /*cubic=*/false);\n    const int w1 = w1r;\n    const int w1p = (w1 < width1 - 1) ? 1 : 0;\n    const accscalar_t w1lambda = w1r - w1;\n    const accscalar_t w0lambda = static_cast<accscalar_t>(1) - w1lambda;\n    //\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; ++c) {\n        const scalar_t d2val = odata[n][c][w2];\n        gpuAtomicAddNoReturn(&idata[n][c][w1], static_cast<scalar_t>(w0lambda * d2val));\n        gpuAtomicAddNoReturn(\n            &idata[n][c][w1 + w1p], static_cast<scalar_t>(w1lambda * d2val));\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "RowwiseMomentsCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void RowwiseMomentsCUDAKernel(\n    int64_t N,\n    T eps,\n    const T* X,\n    T* mean,\n    T* rstd) {\n  using T_ACC = acc_type<T, true>;\n  using WelfordType = WelfordData<T_ACC, int64_t>;\n  using WelfordOp =\n      WelfordOps<T_ACC, T_ACC, int64_t, thrust::pair<T_ACC, T_ACC>>;\n\n  const int64_t i = blockIdx.x;\n  WelfordOp welford_op = {/*correction=*/0, /*take_sqrt=*/false};\n  WelfordType val(0, 0, 0, 0);\n  for (int64_t j = threadIdx.x; j < N; j += blockDim.x) {\n    const int64_t index = i * N + j;\n    val = welford_op.reduce(val, static_cast<T_ACC>(X[index]), index);\n  }\n  if (blockDim.x <= C10_WARP_SIZE) {\n    val = cuda_utils::WarpReduce(val, welford_op);\n  } else {\n    // There will be a warning if we declare a __shared__ WelfordType array.\n    // https://github.com/pytorch/pytorch/pull/13967\n    __shared__ typename std::aligned_storage<\n        sizeof(WelfordType),\n        alignof(WelfordType)>::type val_shared[C10_WARP_SIZE];\n    WelfordType* val_shared_ptr = reinterpret_cast<WelfordType*>(val_shared);\n    val = cuda_utils::BlockReduce(\n        val,\n        welford_op,\n        /*identity_element=*/WelfordType(0, 0, 0, 0),\n        val_shared_ptr);\n  }\n  if (threadIdx.x == 0) {\n    T_ACC m1;\n    T_ACC m2;\n    thrust::tie(m2, m1) = welford_op.project(val);\n    mean[i] = m1;\n    rstd[i] = c10::cuda::compat::rsqrt(m2 + static_cast<T_ACC>(eps));\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "ComputeFusedParamsCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void ComputeFusedParamsCUDAKernel(\n    int64_t N,\n    int64_t C,\n    int64_t group,\n    const T* mean,\n    const T* rstd,\n    const T* gamma,\n    const T* beta,\n    acc_type<T, true>* a,\n    acc_type<T, true>* b) {\n  using T_ACC = acc_type<T, true>;\n  const int64_t index = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (index < N * C) {\n    const int64_t ng = index / (C / group);\n    const int64_t c = index % C;\n    const T_ACC scale = (gamma == nullptr)\n        ? static_cast<T_ACC>(rstd[ng])\n        : static_cast<T_ACC>(rstd[ng]) * static_cast<T_ACC>(gamma[c]);\n    a[index] = scale;\n    b[index] = -scale * static_cast<T_ACC>(mean[ng]) +\n        ((beta == nullptr) ? 0 : static_cast<T_ACC>(beta[c]));\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "Compute1dBackwardFusedParamsCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void Compute1dBackwardFusedParamsCUDAKernel(\n    int64_t C,\n    int64_t group,\n    const T* dY,\n    const T* X,\n    const T* mean,\n    const T* rstd,\n    const T* gamma,\n    acc_type<T, true>* c2,\n    acc_type<T, true>* c3) {\n  using T_ACC = acc_type<T, true>;\n  const int64_t G = group;\n  const int64_t D = C / G;\n  const int64_t n = blockIdx.x;\n  const int64_t g = blockIdx.y;\n  const int64_t ng = n * G + g;\n  T_ACC sum1 = 0;\n  T_ACC sum2 = 0;\n  for (int64_t i = threadIdx.x; i < D; i += blockDim.x) {\n    const int64_t index = ng * D + i;\n    const int64_t c = g * D + i;\n    const T_ACC gamma_v =\n        gamma == nullptr ? T_ACC(1) : static_cast<T_ACC>(gamma[c]);\n    sum1 += dY[index] * X[index] * gamma_v;\n    sum2 += dY[index] * gamma_v;\n  }\n  if (blockDim.x <= C10_WARP_SIZE) {\n    sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n    sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  } else {\n    __shared__ T_ACC ds_shared[C10_WARP_SIZE];\n    __shared__ T_ACC db_shared[C10_WARP_SIZE];\n    sum1 = cuda_utils::BlockReduceSum<T_ACC>(sum1, ds_shared);\n    sum2 = cuda_utils::BlockReduceSum<T_ACC>(sum2, db_shared);\n  }\n  if (threadIdx.x == 0) {\n    const T_ACC s = T_ACC(1) / static_cast<T_ACC>(D);\n    const T_ACC x = (sum2 * static_cast<T_ACC>(mean[ng]) - sum1) *\n        static_cast<T_ACC>(rstd[ng]) * static_cast<T_ACC>(rstd[ng]) *\n        static_cast<T_ACC>(rstd[ng]) * s;\n    c2[ng] = x;\n    c3[ng] = -x * static_cast<T_ACC>(mean[ng]) -\n        sum2 * static_cast<T_ACC>(rstd[ng]) * s;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "GammaBeta1dBackwardCUDAKernel1",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void GammaBeta1dBackwardCUDAKernel1(\n    int64_t N,\n    int64_t C,\n    int64_t group,\n    const T* dY,\n    const T* X,\n    const T* mean,\n    const T* rstd,\n    T* dgamma,\n    T* dbeta) {\n  using T_ACC = acc_type<T, true>;\n  const int64_t c = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (c < C) {\n    const int64_t G = group;\n    const int64_t D = C / G;\n    T_ACC sum1 = 0;\n    T_ACC sum2 = 0;\n    for (int64_t n = 0; n < N; ++n) {\n      const int64_t nc = n * C + c;\n      const int64_t ng = n * G + c / D;\n      const T_ACC dy_acc = static_cast<T_ACC>(dY[nc]);\n      const T_ACC x_acc = static_cast<T_ACC>(X[nc]);\n      sum1 += (dgamma == nullptr)\n          ? T_ACC(0)\n          : ((dy_acc * x_acc - dy_acc * static_cast<T_ACC>(mean[ng])) *\n             static_cast<T_ACC>(rstd[ng]));\n      sum2 += (dbeta == nullptr) ? T_ACC(0) : dy_acc;\n    }\n    if (dgamma != nullptr) {\n      dgamma[c] = sum1;\n    }\n    if (dbeta != nullptr) {\n      dbeta[c] = sum2;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "GammaBeta1dBackwardCUDAKernel2",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void GammaBeta1dBackwardCUDAKernel2(\n    int64_t N,\n    int64_t C,\n    int64_t group,\n    const T* dY,\n    const T* X,\n    const T* mean,\n    const T* rstd,\n    T* dgamma,\n    T* dbeta) {\n  using T_ACC = acc_type<T, true>;\n  __shared__ T_ACC g_shared[kReduceTileSize][kReduceTileSize + 1];\n  __shared__ T_ACC b_shared[kReduceTileSize][kReduceTileSize + 1];\n  const int64_t c = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  T_ACC dg_sum1 = 0;\n  T_ACC dg_sum2 = 0;\n  T_ACC db_sum1 = 0;\n  T_ACC db_sum2 = 0;\n  if (c < C) {\n    const int64_t G = group;\n    const int64_t D = C / G;\n    // Accumulate each 32 cols into a 32 * 32 tile.\n    // Since the blockDim is (32, 16), accumulate twice for 1st and 2nd 16 rows\n    // of a 32 contiguous elements.\n    for (int64_t n = threadIdx.y; n < N; n += blockDim.y * 2) {\n      const int64_t n1 = n;\n      const int64_t n2 = n + blockDim.y;\n      const int64_t nc1 = n1 * C + c;\n      const int64_t nc2 = n2 * C + c;\n      const int64_t ng1 = n1 * G + c / D;\n      const int64_t ng2 = n2 * G + c / D;\n      const T_ACC dy1_acc = static_cast<T_ACC>(dY[nc1]);\n      const T_ACC x1_acc = static_cast<T_ACC>(X[nc1]);\n      dg_sum1 += dgamma == nullptr\n          ? T_ACC(0)\n          : ((dy1_acc * x1_acc - dy1_acc * static_cast<T_ACC>(mean[ng1])) *\n             static_cast<T_ACC>(rstd[ng1]));\n      db_sum1 += dbeta == nullptr ? T_ACC(0) : dy1_acc;\n      if (n2 < N) {\n        const T_ACC dy2_acc = static_cast<T_ACC>(dY[nc2]);\n        const T_ACC x2_acc = static_cast<T_ACC>(X[nc2]);\n        dg_sum2 += dgamma == nullptr\n            ? T_ACC(0)\n            : ((dy2_acc * x2_acc - dy2_acc * static_cast<T_ACC>(mean[ng2])) *\n               static_cast<T_ACC>(rstd[ng2]));\n        db_sum2 += dbeta == nullptr ? T_ACC(0) : dy2_acc;\n      }\n    }\n  }\n\n  // Write accumulated tile to shared memory.\n  g_shared[threadIdx.y][threadIdx.x] = dg_sum1;\n  g_shared[threadIdx.y + blockDim.y][threadIdx.x] = dg_sum2;\n  b_shared[threadIdx.y][threadIdx.x] = db_sum1;\n  b_shared[threadIdx.y + blockDim.y][threadIdx.x] = db_sum2;\n  __syncthreads();\n\n  // Do warp reduce for the 1st 16 cols in the tile.\n  T_ACC sum1 = g_shared[threadIdx.x][threadIdx.y];\n  T_ACC sum2 = b_shared[threadIdx.x][threadIdx.y];\n  sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n  sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  if (threadIdx.x == 0) {\n    const int64_t c = blockIdx.x * blockDim.x + threadIdx.y;\n    if (c < C) {\n      if (dgamma != nullptr) {\n        dgamma[c] = sum1;\n      }\n      if (dbeta != nullptr) {\n        dbeta[c] = sum2;\n      }\n    }\n  }\n\n  // Do warp reduce for the 2nd 16 cols in the tile.\n  sum1 = g_shared[threadIdx.x][threadIdx.y + blockDim.y];\n  sum2 = b_shared[threadIdx.x][threadIdx.y + blockDim.y];\n  sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n  sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  if (threadIdx.x == 0) {\n    const int64_t c = blockIdx.x * blockDim.x + threadIdx.y + blockDim.y;\n    if (c < C) {\n      if (dgamma != nullptr) {\n        dgamma[c] = sum1;\n      }\n      if (dbeta != nullptr) {\n        dbeta[c] = sum2;\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "ComputeInternalGradientsCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void ComputeInternalGradientsCUDAKernel(\n    int64_t HxW,\n    const T* dY,\n    const T* X,\n    acc_type<T, true>* ds,\n    acc_type<T, true>* db) {\n  using T_ACC = acc_type<T, true>;\n  const int64_t nc = blockIdx.x;\n  T_ACC sum1 = 0;\n  T_ACC sum2 = 0;\n  for (int64_t hw = threadIdx.x; hw < HxW; hw += blockDim.x) {\n    const int64_t index = nc * HxW + hw;\n    sum1 += static_cast<T_ACC>(dY[index]) * static_cast<T_ACC>(X[index]);\n    sum2 += static_cast<T_ACC>(dY[index]);\n  }\n  if (blockDim.x <= C10_WARP_SIZE) {\n    sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n    sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  } else {\n    __shared__ T_ACC ds_shared[C10_WARP_SIZE];\n    __shared__ T_ACC db_shared[C10_WARP_SIZE];\n    sum1 = cuda_utils::BlockReduceSum<T_ACC>(sum1, ds_shared);\n    sum2 = cuda_utils::BlockReduceSum<T_ACC>(sum2, db_shared);\n  }\n  if (threadIdx.x == 0) {\n    ds[nc] = sum1;\n    db[nc] = sum2;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "ComputeBackwardFusedParamsCUDAKernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void ComputeBackwardFusedParamsCUDAKernel(\n    int64_t C,\n    int64_t HxW,\n    int64_t group,\n    const T* mean,\n    const T* rstd,\n    const T* gamma,\n    const acc_type<T, true>* ds,\n    const acc_type<T, true>* db,\n    acc_type<T, true>* c2,\n    acc_type<T, true>* c3) {\n  using T_ACC = acc_type<T, true>;\n  const int64_t G = group;\n  const int64_t D = C / G;\n  const int64_t n = blockIdx.x;\n  const int64_t g = blockIdx.y;\n  const int64_t ng = n * G + g;\n  T_ACC sum1 = 0;\n  T_ACC sum2 = 0;\n  for (int64_t i = threadIdx.x; i < D; i += blockDim.x) {\n    const int64_t index = ng * D + i;\n    const int64_t c = g * D + i;\n    const T_ACC gamma_v =\n        gamma == nullptr ? T_ACC(1) : static_cast<T_ACC>(gamma[c]);\n    sum1 += ds[index] * gamma_v;\n    sum2 += db[index] * gamma_v;\n  }\n  if (blockDim.x <= C10_WARP_SIZE) {\n    sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n    sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  } else {\n    __shared__ T_ACC ds_shared[C10_WARP_SIZE];\n    __shared__ T_ACC db_shared[C10_WARP_SIZE];\n    sum1 = cuda_utils::BlockReduceSum<T_ACC>(sum1, ds_shared);\n    sum2 = cuda_utils::BlockReduceSum<T_ACC>(sum2, db_shared);\n  }\n  if (threadIdx.x == 0) {\n    const T_ACC s = T_ACC(1) / static_cast<T_ACC>(D * HxW);\n    const T_ACC x = (sum2 * static_cast<T_ACC>(mean[ng]) - sum1) *\n        static_cast<T_ACC>(rstd[ng]) * static_cast<T_ACC>(rstd[ng]) *\n        static_cast<T_ACC>(rstd[ng]) * s;\n    c2[ng] = x;\n    c3[ng] = -x * static_cast<T_ACC>(mean[ng]) -\n        sum2 * static_cast<T_ACC>(rstd[ng]) * s;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "GammaBetaBackwardCUDAKernel1",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void GammaBetaBackwardCUDAKernel1(\n    int64_t N,\n    int64_t C,\n    int64_t group,\n    const T* mean,\n    const T* rstd,\n    const acc_type<T, true>* ds,\n    const acc_type<T, true>* db,\n    T* dgamma,\n    T* dbeta) {\n  using T_ACC = acc_type<T, true>;\n  const int64_t c = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (c < C) {\n    const int64_t G = group;\n    const int64_t D = C / G;\n    T_ACC sum1 = 0;\n    T_ACC sum2 = 0;\n    for (int64_t n = 0; n < N; ++n) {\n      const int64_t nc = n * C + c;\n      const int64_t ng = n * G + c / D;\n      sum1 += (dgamma == nullptr)\n          ? T_ACC(0)\n          : ((ds[nc] - db[nc] * static_cast<T_ACC>(mean[ng])) *\n             static_cast<T_ACC>(rstd[ng]));\n      sum2 += (dbeta == nullptr) ? T_ACC(0) : db[nc];\n    }\n    if (dgamma != nullptr) {\n      dgamma[c] = sum1;\n    }\n    if (dbeta != nullptr) {\n      dbeta[c] = sum2;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "GammaBetaBackwardCUDAKernel2",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void GammaBetaBackwardCUDAKernel2(\n    int64_t N,\n    int64_t C,\n    int64_t group,\n    const T* mean,\n    const T* rstd,\n    const acc_type<T, true>* ds,\n    const acc_type<T, true>* db,\n    T* dgamma,\n    T* dbeta) {\n  using T_ACC = acc_type<T, true>;\n  __shared__ T_ACC g_shared[kReduceTileSize][kReduceTileSize + 1];\n  __shared__ T_ACC b_shared[kReduceTileSize][kReduceTileSize + 1];\n  const int64_t c = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  T_ACC dg_sum1 = 0;\n  T_ACC dg_sum2 = 0;\n  T_ACC db_sum1 = 0;\n  T_ACC db_sum2 = 0;\n  if (c < C) {\n    const int64_t G = group;\n    const int64_t D = C / G;\n    // Accumulate each 32 cols into a 32 * 32 tile.\n    // Since the blockDim is (32, 16), accumulate twice for 1st and 2nd 16 rows\n    // of a 32 contiguous elements.\n    for (int64_t n = threadIdx.y; n < N; n += blockDim.y * 2) {\n      const int64_t n1 = n;\n      const int64_t n2 = n + blockDim.y;\n      const int64_t nc1 = n1 * C + c;\n      const int64_t nc2 = n2 * C + c;\n      const int64_t ng1 = n1 * G + c / D;\n      const int64_t ng2 = n2 * G + c / D;\n      dg_sum1 += dgamma == nullptr\n          ? T_ACC(0)\n          : ((ds[nc1] - db[nc1] * static_cast<T_ACC>(mean[ng1])) *\n             static_cast<T_ACC>(rstd[ng1]));\n      db_sum1 += dbeta == nullptr ? T_ACC(0) : db[nc1];\n      if (n2 < N) {\n        dg_sum2 += dgamma == nullptr\n            ? T_ACC(0)\n            : ((ds[nc2] - db[nc2] * static_cast<T_ACC>(mean[ng2])) *\n               static_cast<T_ACC>(rstd[ng2]));\n        db_sum2 += dbeta == nullptr ? T_ACC(0) : db[nc2];\n      }\n    }\n  }\n\n  // Write accumulated tile to shared memory.\n  g_shared[threadIdx.y][threadIdx.x] = dg_sum1;\n  g_shared[threadIdx.y + blockDim.y][threadIdx.x] = dg_sum2;\n  b_shared[threadIdx.y][threadIdx.x] = db_sum1;\n  b_shared[threadIdx.y + blockDim.y][threadIdx.x] = db_sum2;\n  __syncthreads();\n\n  // Do warp reduce for the 1st 16 cols in the tile.\n  T_ACC sum1 = g_shared[threadIdx.x][threadIdx.y];\n  T_ACC sum2 = b_shared[threadIdx.x][threadIdx.y];\n  sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n  sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  if (threadIdx.x == 0) {\n    const int64_t c = blockIdx.x * blockDim.x + threadIdx.y;\n    if (c < C) {\n      if (dgamma != nullptr) {\n        dgamma[c] = sum1;\n      }\n      if (dbeta != nullptr) {\n        dbeta[c] = sum2;\n      }\n    }\n  }\n\n  // Do warp reduce for the 2st 16 cols in the tile.\n  sum1 = g_shared[threadIdx.x][threadIdx.y + blockDim.y];\n  sum2 = b_shared[threadIdx.x][threadIdx.y + blockDim.y];\n  sum1 = cuda_utils::WarpReduceSum<T_ACC>(sum1);\n  sum2 = cuda_utils::WarpReduceSum<T_ACC>(sum2);\n  if (threadIdx.x == 0) {\n    const int64_t c = blockIdx.x * blockDim.x + threadIdx.y + blockDim.y;\n    if (c < C) {\n      if (dgamma != nullptr) {\n        dgamma[c] = sum1;\n      }\n      if (dbeta != nullptr) {\n        dbeta[c] = sum2;\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptive_average_pool",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptive_average_pool(const scalar_t *input, scalar_t *output,\n                          int isizeH, int isizeW,\n                          int osizeH, int osizeW,\n                          int64_t istrideD, int64_t istrideH, int64_t istrideW)\n  {\n    using opmath_t = at::opmath_type<scalar_t>;\n    // iterators on output pixels\n    int oh, ow;\n\n    // select input/output plane based on thread/block ID\n    int o_plane = blockIdx.x;\n    int i_plane = o_plane;\n\n    output = output + o_plane*osizeH*osizeW;\n    input = input + i_plane*istrideD;\n\n    int ostartH = blockDim.y*blockIdx.y + threadIdx.y;\n    int oendH = osizeH;\n    const int ostepH = blockDim.y*gridDim.y;\n\n    int ostartW = threadIdx.x;\n    int oendW = osizeW;\n    const int ostepW = blockDim.x;\n\n    // For all output pixels...\n    for(oh = ostartH; oh < oendH; oh += ostepH) {\n\n      int istartH = START_IND(oh, osizeH, isizeH);\n      int iendH   = END_IND(oh, osizeH, isizeH);\n      int kH = iendH - istartH;\n\n      for(ow = ostartW; ow < oendW; ow += ostepW) {\n\n        int istartW = START_IND(ow, osizeW, isizeW);\n        int iendW   = END_IND(ow, osizeW, isizeW);\n        int kW = iendW - istartW;\n\n        // Compute the average pooling over corresponding input pixels\n        const scalar_t *ptr_input = input + istartH*istrideH + istartW*istrideW;\n        scalar_t *ptr_output = output + oh*osizeW + ow;\n        opmath_t sum = static_cast<opmath_t>(0);\n        int ih, iw;\n        for(ih = 0; ih < kH; ++ih) {\n          for(iw = 0; iw < kW; ++iw) {\n            scalar_t val = ptr_input[iw*istrideW];\n            sum += val;\n          }\n          ptr_input += istrideH; // next input line\n        }\n        // Update output\n        *ptr_output = sum / kH / kW;\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "adaptive_average_gradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptive_average_gradinput(\n    T *gradInput, const T *gradOutput,\n    int isizeH, int isizeW, int osizeH, int osizeW\n  )\n  {\n    // iterators on input pixels\n    int ih, iw;\n\n    // select input/output plane based on thread/block ID\n    int i_plane = blockIdx.x;\n    int o_plane = i_plane;\n\n    gradOutput = gradOutput + o_plane*osizeH*osizeW;\n    gradInput = gradInput + i_plane*isizeH*isizeW;\n\n    int istartH = blockDim.y*blockIdx.y + threadIdx.y;\n    int iendH = isizeH;\n    int istepH = blockDim.y*gridDim.y;\n\n    int istartW = threadIdx.x;\n    int iendW = isizeW;\n    int istepW = blockDim.x;\n\n    // compute gradInput\n    for(ih = istartH; ih < iendH; ih += istepH) {\n\n      int ostartH = START_IND(ih, isizeH, osizeH);\n      int oendH   = END_IND(ih, isizeH, osizeH);\n\n      for(iw = istartW; iw < iendW; iw += istepW) {\n\n        int ostartW = START_IND(iw, isizeW, osizeW);\n        int oendW   = END_IND(iw, isizeW, osizeW);\n\n        // Compute the gradients over corresponding output pixels\n        T *ptr_gradInput = gradInput + ih*isizeW + iw;\n\n        int oh, ow;\n        for(oh = ostartH; oh < oendH; ++oh) {\n          int kH = START_IND(oh, osizeH, isizeH) - END_IND(oh, osizeH, isizeH);\n          for(ow = ostartW; ow < oendW; ++ow) {\n            int kW = START_IND(ow, osizeW, isizeW) - END_IND(ow, osizeW, isizeW);\n            T grad_delta = gradOutput[ow + oh*osizeW] / kH / kW;\n            *ptr_gradInput += grad_delta;\n          }\n        }\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "atomic_adaptive_average_gradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void atomic_adaptive_average_gradinput(\n    T *gradInput, const T *gradOutput,\n    int isizeH, int isizeW, int osizeH, int osizeW\n  )\n  {\n    // iterators on output indices\n    int oh, ow;\n\n    // select input/output plane based on thread/block ID\n    int o_plane = blockIdx.x;\n    int i_plane = o_plane;\n\n    gradOutput = gradOutput + o_plane*osizeW*osizeH;\n    gradInput = gradInput + i_plane*isizeW*isizeH;\n\n    int ostartH = blockDim.y*blockIdx.y + threadIdx.y;\n    int oendH = osizeH;\n    int ostepH = blockDim.y*gridDim.y;\n\n    int ostartW = threadIdx.x;\n    int oendW = osizeW;\n    int ostepW = blockDim.x;\n\n    // For all output pixels...\n    for(oh = ostartH; oh < oendH; oh += ostepH) {\n\n      int istartH = START_IND(oh, osizeH, isizeH);\n      int iendH   = END_IND(oh, osizeH, isizeH);\n      int kH = iendH - istartH;\n\n      for(ow = ostartW; ow < oendW; ow += ostepW) {\n\n        int istartW = START_IND(ow, osizeW, isizeW);\n        int iendW   = END_IND(ow, osizeW, isizeW);\n        int kW = iendW - istartW;\n\n        // Compute the gradients for over corresponding input pixels\n        T *ptr_gradInput = gradInput + istartH*isizeW + istartW;\n        const T *ptr_gradOutput = gradOutput + oh*osizeW + ow;\n        T grad_delta = *ptr_gradOutput / kW / kH;\n\n        int ih, iw;\n        for(ih = 0; ih < kH; ++ih) {\n          for(iw = 0; iw < kW; ++iw) {\n            // atomic add since different threads could update same variable\n            gpuAtomicAddNoReturn(&(ptr_gradInput[iw]), grad_delta);\n          }\n          ptr_gradInput += isizeW; // next input line\n        }\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "adaptive_average_pool_nhwc",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptive_average_pool_nhwc(const scalar_t* __restrict__ input, scalar_t* __restrict__ output,\n                          int sizeB, int sizeC,\n                          int isizeH, int isizeW,\n                          int osizeH, int osizeW,\n                          int kernel_stride_C, int kernel_size_C,\n                          index_t istrideB, index_t istrideC,\n                          index_t istrideH, index_t istrideW)\n  {\n    using opmath_t = at::opmath_type<scalar_t>;\n    extern __shared__ int smem[];\n    opmath_t *out_cached = reinterpret_cast<opmath_t*>(smem);\n\n    // flattening cta for pre-computation & smem initialization;\n    int thread_id = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);\n    int block_size = blockDim.x * blockDim.y * blockDim.z;\n\n    // use shared memory to store temporary output value. This is simply to\n    // reduce register usage.\n    for (index_t i = thread_id; i < kernel_size_C*blockDim.x*blockDim.y*blockDim.z; i+= block_size) {\n      out_cached[i] = opmath_t(0.0);\n    }\n\n    __syncthreads();\n\n    // each CTA handles a portion of a single slice on batch dimension;\n    int batch_id = blockIdx.x % sizeB;\n    int channel_id = blockIdx.x / sizeB;\n    int channel_offset = threadIdx.x + channel_id * blockDim.x;\n\n    // each CTA handles a single slice on batch dimension;\n    // We use gridDim.x to handle striding on C as well.\n    output = output + batch_id * osizeH * osizeW * sizeC;\n    input = input + batch_id * istrideB;\n\n    // split out_cached and exclusively it assigned to each thread;\n    out_cached = &out_cached[(threadIdx.z * blockDim.y + threadIdx.y) * kernel_size_C * blockDim.x];\n\n    // iterate on output H & W.\n    // Each CTA handles a consecutive H & W section (TILE); Do NOT stride CTA on\n    // tile so there's a better chance to hit L1 cache.\n    index_t oH = (osizeH + gridDim.z-1) / gridDim.z;\n    index_t oW = (osizeW + gridDim.y-1) / gridDim.y;\n    index_t ostartH = threadIdx.z + blockIdx.z*oH;\n    index_t oendH = ::min(ostartH+oH, osizeH);\n    index_t ostartW = threadIdx.y + blockIdx.y*oW;\n    index_t oendW = ::min(ostartW+oW, osizeW);\n\n    // Stride for threads, each warp can reuse L1 as they go. So theoretically\n    // better chance to survive cache eviction.\n    for (int oh = ostartH; oh < oendH; oh+=blockDim.z) {\n      int istartH = START_IND_INT(oh, osizeH, isizeH);\n      int iendH = END_IND_INT(oh, osizeH, isizeH);\n      for (int ow = ostartW; ow < oendW; ow+=blockDim.y) {\n        int istartW = START_IND_INT(ow, osizeW, isizeW);\n        int iendW = END_IND_INT(ow, osizeW, isizeW);\n        scalar_t factor = scalar_t(1.0) / ((iendH-istartH) * (iendW-istartW));\n\n        // loop on input: hierarchy h->w->c, use shared memory here hopefully\n        // would not stall global memory read;\n        for (index_t ih = istartH; ih < iendH; ih++) {\n          for (index_t iw = istartW; iw < iendW; iw++) {\n            int cached_index = threadIdx.x;\n            const scalar_t *ptr_input = input + ih*istrideH + iw*istrideW;\n            for (index_t c = channel_offset;\n                 c < sizeC;\n                 c += blockDim.x*kernel_stride_C) {\n              out_cached[cached_index] += ptr_input[c*istrideC];\n              cached_index += blockDim.x;\n            }\n          }\n        }\n        scalar_t *ptr_output = output + (oh * osizeW + ow) * sizeC;\n\n        int cached_index = threadIdx.x;\n        // write accumulated output to global memory;\n        for (index_t c = channel_offset;\n             c < sizeC;\n             c += blockDim.x*kernel_stride_C) {\n          // This causes numerical issueptr when unit test with NCHW kernel;\n          // switch to could verify the correctness;\n          // output[c] = out_cached[c] / (iendH-istartH) / (iendW-istartW);\n          ptr_output[c] = out_cached[cached_index] * factor;\n          out_cached[cached_index] = opmath_t(0.0);\n          cached_index += blockDim.x;\n        }\n        // no need to __syncthreads() since out_cached is not shared.\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "adaptive_average_gradinput_nhwc",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptive_average_gradinput_nhwc(scalar_t* __restrict__ gradInput, const scalar_t* __restrict__ gradOutput,\n                          int sizeB, int sizeC,\n                          int isizeH, int isizeW,\n                          int osizeH, int osizeW,\n                          int kernel_stride_C, int kernel_size_C,\n                          index_t ostrideB, index_t ostrideC,\n                          index_t ostrideH, index_t ostrideW)\n  {\n    extern __shared__ int smem[];\n    index_t *ostartW_cached = smem;\n    index_t *oendW_cached = &ostartW_cached[isizeW];\n\n    // be careful with alignment, in case scalar_t is fp16, we want to assign\n    // int pointers first.\n    scalar_t *r_kW_cached = reinterpret_cast<scalar_t*>(&oendW_cached[isizeW]);\n    scalar_t *r_kH_cached = &r_kW_cached[osizeW];\n    scalar_t *out_cached = &r_kH_cached[osizeH];\n\n    // flattening cta for pre-computation & smem initialization;\n    int thread_id = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);\n    int block_size = blockDim.x * blockDim.y * blockDim.z;\n\n    // Precompute output start/end index per input index on width dimension;\n    // Not doing this for height dimension, as that's our out-most loop.\n    for (index_t i = thread_id; i < isizeW; i+= block_size) {\n      ostartW_cached[i] = START_IND_INT(i, isizeW, osizeW);\n      oendW_cached[i] = END_IND_INT(i, isizeW, osizeW);\n    }\n\n    // Precompute pooling height/weight factor for each output element;\n    // This is used to weight output gradient when accumulate them on input\n    // gradient.\n    // Technically we don't have to compute it for the whole `osizeH`, since\n    // each cta only covers a consecutive portion of the entire output. But it's\n    // not going to save us from code divergence, and shared memory save is not\n    // an issue neither, so just leave it as is for now.\n    for (index_t i = thread_id; i < osizeH; i+= block_size) {\n      r_kH_cached[i] = scalar_t(1.0) / (END_IND_INT(i, osizeH, isizeH) - START_IND_INT(i, osizeH, isizeH));\n    }\n    for (index_t i = thread_id; i < osizeW; i+= block_size) {\n      r_kW_cached[i] = scalar_t(1.0) / (END_IND_INT(i, osizeW, isizeW) - START_IND_INT(i, osizeW, isizeW));\n    }\n\n    // each CTA handles a portion of a single slice on batch dimension;\n    int batch_id = blockIdx.x % sizeB;\n    int channel_id = blockIdx.x / sizeB;\n    int channel_offset = threadIdx.x + channel_id * blockDim.x;\n\n    // use shared memory to store temporary output value. This is simply to\n    // reduce register usage.\n    for (index_t i = thread_id; i < kernel_size_C*blockDim.x*blockDim.y*blockDim.z; i+= block_size) {\n      out_cached[i] = scalar_t(0.0);\n    }\n\n    __syncthreads();\n\n    // each CTA handles a portion of a single slice on batch dimension;\n    // We use gridDim.x to handle striding on C as well.\n    gradInput = gradInput + batch_id * isizeH * isizeW * sizeC;\n    gradOutput = gradOutput + batch_id * ostrideB;\n\n    // split out_cached and exclusively it assigned to each thread;\n    out_cached = &out_cached[(threadIdx.z * blockDim.y + threadIdx.y) * blockDim.x * kernel_size_C];\n\n    // iterate on input H & W.\n    // Each CTA handles a consecutive H & W section (TILE); Do NOT stride CTA on\n    // tile so there's a better chance to hit L1 cache.\n    index_t iH = (isizeH + gridDim.z-1) / gridDim.z;\n    index_t iW = (isizeW + gridDim.y-1) / gridDim.y;\n    index_t istartH = threadIdx.z + blockIdx.z*iH;\n    index_t iendH = ::min(istartH+iH, isizeH);\n    index_t istartW = threadIdx.y + blockIdx.y*iW;\n    index_t iendW = ::min(istartW+iW, isizeW);\n\n    // Stride for threads, each warp can reuse L1 as they go. So theoretically\n    // better chance to survive cache eviction.\n    for (index_t ih = istartH; ih < iendH; ih+=blockDim.z) {\n      index_t ostartH = START_IND_INT(ih, isizeH, osizeH);\n      index_t oendH = END_IND_INT(ih, isizeH, osizeH);\n      for (index_t iw = istartW; iw < iendW; iw+=blockDim.y) {\n        // loop on output: hierarchy h->w->c, so we could reuse weight factor f\n        // because it remains the same for given oh & ow\n        for(index_t oh = ostartH; oh < oendH; ++oh) {\n          for(index_t ow = ostartW_cached[iw]; ow < oendW_cached[iw]; ++ow) {\n            scalar_t f = r_kW_cached[ow] * r_kH_cached[oh];\n            const scalar_t* ptr_gradOutput = gradOutput + oh*ostrideH + ow*ostrideW;\n            int cached_index = threadIdx.x;\n            for (index_t c = channel_offset;\n                 c < sizeC;\n                 c += blockDim.x*kernel_stride_C) {\n              out_cached[cached_index] += ptr_gradOutput[c*ostrideC] * f;\n              cached_index += blockDim.x;\n            }\n          }\n        }\n        scalar_t *ptr_gradInput = gradInput + (ih * isizeW + iw) * sizeC;\n        int cached_index = threadIdx.x;\n        // write accumulated gradIput to global memory;\n        for (index_t c = channel_offset;\n             c < sizeC;\n             c += blockDim.x*kernel_stride_C) {\n          ptr_gradInput[c] = out_cached[cached_index];\n          out_cached[cached_index] = scalar_t(0.0);\n          cached_index += blockDim.x;\n        }\n        // no need to __syncthreads() since out_cached is not shared.\n      }\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest2d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest2d_out_frame(\n    const scalar_t* idata,\n    scalar_t* odata,\n    const size_t nc,\n    const size_t height1,\n    const size_t width1,\n    const size_t height2,\n    const size_t width2,\n    float height_scale,\n    float width_scale) {\n  size_t nc_iter = threadIdx.z + blockIdx.z * blockDim.z;\n  int64_t w2 = ((int64_t) threadIdx.x) + blockIdx.x * blockDim.x;\n  int64_t h2 = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (w2 >= width2 || h2 >= height2) {\n    return;\n  }\n\n  int64_t nc_stride = ((int64_t) blockDim.z) * gridDim.z;\n\n  const size_t h1 = height1 == height2\n      ? h2\n      : nn_compute_source_index_fn(height_scale, h2, height1);\n  const size_t w1 = width1 == width2\n      ? w2\n      : nn_compute_source_index_fn(width_scale, w2, width1);\n\n  size_t src_index = (nc_iter * height1 + h1) * width1 + w1;\n  size_t src_index_stride = nc_stride * width1 * height1;\n  size_t dst_index = (nc_iter * height2 + h2) * width2 + w2;\n  size_t dst_index_stride = nc_stride * width2 * height2;\n\n  // iterating over\n  while (nc_iter < nc) {\n    odata[dst_index] = idata[src_index];\n    dst_index += dst_index_stride;\n    src_index += src_index_stride;\n    nc_iter += nc_stride;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest2d_nhwc_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest2d_nhwc_out_frame(\n    const scalar_t* idata,\n    scalar_t* odata,\n    const size_t channels,\n    const size_t height1,\n    const size_t width1,\n    const size_t height2,\n    const size_t width2,\n    float height_scale,\n    float width_scale,\n    const size_t out_numel) {\n\n    const int64_t index = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n\n    if (index < out_numel) {\n    const auto c = index % channels;\n    const auto w2 = (index / channels) % width2;\n    const auto h2 = (index / channels / width2) % height2;\n    const auto n = index / channels / width2 / height2;\n\n    const size_t h1 = height1 == height2 ? h2 : nn_compute_source_index_fn(height_scale, h2, height1);\n    const size_t w1 = width1 == width2 ? w2 : nn_compute_source_index_fn(width_scale, w2, width1);\n\n    odata[index] = idata[idx_cl(n, h1, w1, c, height1, width1, channels)];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest2d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest2d_backward_out_frame(\n    const scalar_t* grad_o,\n    size_t dim_b,\n    size_t dim_c,\n    size_t src_dim_h,\n    size_t src_dim_w,\n    size_t dst_dim_h,\n    size_t dst_dim_w,\n    scalar_t* grad_i,\n    float height_scale,\n    float width_scale) {\n  int64_t dst_idx = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (dst_idx >= dim_c * dst_dim_h * dst_dim_w)\n    return;\n\n  int64_t dst_c_stride = dst_dim_h * dst_dim_w;\n  int64_t src_c_stride = src_dim_h * src_dim_w;\n\n  int c = (dst_idx / (dst_c_stride)) % dim_c;\n\n  int dst_y = (dst_idx / dst_dim_w) % dst_dim_h;\n  // note that we do not want to clamp src_y to src_dim_y, since we might\n  // intentionally want to skip in case of scale_factor < 1.0\n  int src_y =\n      nn_bw_compute_source_index_fn(height_scale, dst_y, src_dim_h);\n  int src_y_up = nn_bw_compute_source_index_fn(\n      height_scale, dst_y + 1, src_dim_h);\n\n  int dst_x = dst_idx % dst_dim_w;\n  // note that we do not want to clamp src_x to src_dim_w, since we might\n  // intentionally want to skip in case of scale_factor < 1.0\n  int src_x =\n      nn_bw_compute_source_index_fn(width_scale, dst_x, src_dim_w);\n  int src_x_up = nn_bw_compute_source_index_fn(\n      width_scale, dst_x + 1, src_dim_w);\n\n  for (int b = 0; b < dim_b; b++) {\n    accscalar_t grad = 0;\n    for (int y = src_y; y < src_y_up; y++) {\n      for (int x = src_x; x < src_x_up; x++) {\n        int64_t src_idx =\n            b * dim_c * src_c_stride + c * src_c_stride + y * src_dim_w + x;\n        grad += grad_o[src_idx];\n      }\n    }\n    grad_i[dst_idx] = grad;\n    dst_idx += dim_c * dst_c_stride;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest2d_backward_nhwc_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest2d_backward_nhwc_out_frame(\n    const scalar_t* go,\n    scalar_t* gi,\n    const size_t height1,\n    const size_t width1,\n    const size_t height2,\n    const size_t width2,\n    const size_t channels,\n    const float height_scale,\n    const float width_scale,\n    const size_t gi_numel) {\n\n  // 1 is for grad_output (src)\n  // 2 is for grad_input (dst)\n\n  const int64_t index = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n\n  if (index < gi_numel) {\n    const int c = index % channels;\n    const int w2 = (index / channels) % width2;\n    const int h2 = (index / channels / width2) % height2;\n    const int n = index / channels / width2 / height2;\n\n    int h1 = nn_bw_compute_source_index_fn(height_scale, h2, height1);\n    int h1_up = nn_bw_compute_source_index_fn(height_scale, h2 + 1, height1);\n\n    int w1 = nn_bw_compute_source_index_fn(width_scale, w2, width1);\n    int w1_up = nn_bw_compute_source_index_fn(width_scale, w2 + 1, width1);\n\n    accscalar_t grad = 0;\n    for (int ih = h1; ih < h1_up; ih++) {\n      for (int iw = w1; iw < w1_up; iw++) {\n        grad += go[idx_cl(n, ih, iw, c, height1, width1, channels)];\n      }\n    }\n    gi[index] = static_cast<scalar_t>(grad);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss2d_forward_no_reduce_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss2d_forward_no_reduce_kernel(\n  int64_t n_threads,\n  PackedTensorAccessor64<scalar_t, 4> input,\n  PackedTensorAccessor64<int64_t, 3> target,\n  PackedTensorAccessor64<scalar_t, 3> output,\n  const scalar_t* weight,\n  int64_t ignore_index\n) {\n  int64_t batch_size = input.size(0);\n  int64_t n_classes = input.size(1);\n  int64_t H = input.size(2);\n  int64_t W = input.size(3);\n\n  CUDA_KERNEL_LOOP(index, n_threads) {\n    const int64_t b = index % batch_size;\n    const int64_t h = (index / batch_size) % H;\n    const int64_t w = (index / (batch_size * H)) % W;\n\n    int64_t cur_target = target[b][h][w];\n    if (cur_target == ignore_index) {\n      output[b][h][w] = static_cast<scalar_t>(0);\n      continue;\n    }\n    CUDA_KERNEL_ASSERT(cur_target >= 0 && cur_target < n_classes);\n    scalar_t value = input[b][cur_target][h][w];\n    scalar_t cur_weight = weight != nullptr ? weight[cur_target] : static_cast<scalar_t>(1);\n    output[b][h][w] = -value * cur_weight;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss2d_forward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss2d_forward_kernel(\n  scalar_t* output,\n  scalar_t* total_weight,\n  const scalar_t* input,\n  const int64_t* target,\n  const scalar_t* weight,\n  int n_classes,\n  int map_nelem,\n  int blocks_per_sample,\n  int64_t ignore_index) {\n\n  scalar_t cur_weight;\n  accscalar_t input_sum = 0;\n  accscalar_t acc_weight = 0;\n\n  index_t sample = blockIdx.x / blocks_per_sample;\n  index_t toffset = sample * map_nelem;\n  index_t ioffset = sample * map_nelem * n_classes;\n  int step = blockDim.x * blocks_per_sample;\n  for (int i = (blockIdx.x % blocks_per_sample) * blockDim.x + threadIdx.x;\n       i < map_nelem;\n       i += step) {\n    index_t t = target[toffset + i];\n    if (t != ignore_index) {\n      CUDA_KERNEL_ASSERT(t >= 0 && t < n_classes);\n      cur_weight = weight != nullptr ? weight[t] : static_cast<scalar_t>(1);\n      const auto input_index = ioffset + i + map_nelem * t;\n      CUDA_KERNEL_ASSERT(input_index >= 0);\n      input_sum -= input[input_index] * cur_weight;\n      acc_weight += cur_weight;\n    }\n  }\n\n  __shared__ accscalar_t acc_weight_smem[CUDA_NUM_THREADS];\n  __shared__ accscalar_t input_sum_smem[CUDA_NUM_THREADS];\n\n  auto acc_weight_ = cuda_utils::BlockReduceSum(acc_weight, acc_weight_smem);\n  auto input_sum_ = cuda_utils::BlockReduceSum(input_sum, input_sum_smem);\n\n  if (threadIdx.x == 0) {\n    gpuAtomicAdd(total_weight, static_cast<scalar_t>(acc_weight_));\n    gpuAtomicAdd(output, static_cast<scalar_t>(input_sum_));\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss2d_forward_size_average_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss2d_forward_size_average_kernel(\n  scalar_t* output,\n  const scalar_t* total_weight\n) {\n  *output /= *total_weight;\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss2d_backward_no_reduce_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss2d_backward_no_reduce_kernel(\n  int64_t n_threads,\n  PackedTensorAccessor64<int64_t, 3> target,\n  PackedTensorAccessor64<scalar_t, 3> grad_output,\n  PackedTensorAccessor64<scalar_t, 4> grad_input,\n  const scalar_t* weight,\n  int64_t ignore_index\n) {\n  int64_t batch_size = target.size(0);\n  int64_t H = target.size(1);\n  int64_t W = target.size(2);\n\n  CUDA_KERNEL_LOOP(index, n_threads) {\n    const int64_t b = index % batch_size;\n    const int64_t h = (index / batch_size) % H;\n    const int64_t w = (index / (batch_size * H)) % W;\n\n    int64_t cur_target = target[b][h][w];\n    if (cur_target == ignore_index) {\n      continue;\n    }\n    scalar_t value = -(weight != nullptr ? weight[cur_target] : static_cast<scalar_t>(1));\n    grad_input[b][cur_target][h][w] = value * grad_output[b][h][w];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss2d_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss2d_backward_kernel(\n  scalar_t* grad_input,\n  const scalar_t* grad_output,\n  const int64_t* target,\n  const scalar_t* weights,\n  const scalar_t* total_weight,\n  bool size_average,\n  int n_classes,\n  int map_nelem,\n  int blocks_per_sample,\n  int64_t ignore_index\n) {\n  const auto grad = -(size_average ? *grad_output / *total_weight\n                                   : *grad_output);\n\n  const int sample = blockIdx.x / blocks_per_sample;\n  const int step = blockDim.x * blocks_per_sample;\n\n  const int toffset = sample * map_nelem;\n  const auto* const target_thread = target + toffset;\n\n  const int ioffset = sample * map_nelem * n_classes;\n  auto* const grad_input_thread = grad_input + ioffset;\n\n  for (int i = (blockIdx.x % blocks_per_sample) * blockDim.x + threadIdx.x;\n       i < map_nelem;\n       i += step) {\n    const int64_t t = target_thread[i];\n    if (t != ignore_index) {\n      CUDA_KERNEL_ASSERT(t >= 0 && t < n_classes);\n      const auto grad_input_index = i + map_nelem * t;\n      CUDA_KERNEL_ASSERT(grad_input_index >= 0);\n      grad_input_thread[i + map_nelem * t] = weights != nullptr ? weights[t] * grad\n                                                                : grad;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "conv_depthwise2d_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void conv_depthwise2d_backward_kernel(\n    const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> grad_output,\n    PackedTensorAccessor32<scalar_t, 4, DefaultPtrTraits> grad_input,\n    const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> weight,\n    index_t totalElements,\n    const int inputChannels,\n    const int depthwiseMultiplier,\n    const int outputChannels,\n    const int inputWidth, const int inputHeight,\n    const int outputWidth, const int outputHeight,\n    const int kernelWidth, const int kernelHeight,\n    const int strideWidth, const int strideHeight,\n    const int padWidth, const int padHeight,\n    const int dilationWidth, const int dilationHeight) {\n  using acc_t = at::acc_type<scalar_t, true>;\n  const int KW_LIMIT = (kSize != 0) ? kSize : kernelWidth;\n  const int KH_LIMIT = (kSize != 0) ? kSize : kernelHeight;\n  const int strideW = (stride != 0) ? stride : strideWidth;\n  const int strideH = (stride != 0) ? stride : strideHeight;\n\n  CUDA_KERNEL_LOOP_TYPE(linearIndex, totalElements, index_t) {\n    int indtmp1 = linearIndex/inputWidth;\n    const int w = linearIndex - indtmp1 * inputWidth;\n    int indtmp2 = indtmp1/inputHeight;\n    const int h = indtmp1 - indtmp2 * inputHeight;\n    indtmp1 = indtmp2;\n    indtmp2 = indtmp1/inputChannels;\n    const int c = indtmp1 - indtmp2 * inputChannels;\n    const int n = indtmp2;\n\n    acc_t value(0);\n\n    for (int multiplier = 0; multiplier < depthwiseMultiplier; ++multiplier) {\n      int och = (c * depthwiseMultiplier) + multiplier;\n      int weightOffset = och * kernelHeight * kernelWidth;\n      for (int kh = 0; kh < KH_LIMIT; ++kh) {\n#if !defined(USE_ROCM)\n#pragma unroll\n#endif\n        for (int kw = 0; kw < KW_LIMIT; ++kw) {\n          int h_out = h + padHeight - kh * dilationHeight;\n          int w_out = w + padWidth - kw * dilationWidth;\n          if ((h_out % strideH == 0) && (w_out % strideW == 0)) {\n            h_out = h_out / strideH;\n            w_out = w_out / strideW;\n\n            if ((h_out >= 0) && (h_out < outputHeight)\n                  && (w_out >= 0) && (w_out < outputWidth)) {\n\n              const int offset = ((n * outputChannels + och) * outputHeight + h_out)\n                    * outputWidth + w_out;\n              value += (static_cast<acc_t>(weight.data()[weightOffset]) *\n                        static_cast<acc_t>(grad_output.data()[offset]));\n            }\n          }\n          ++weightOffset;\n        }\n      }\n    }\n    grad_input.data()[linearIndex] = static_cast<scalar_t>(value);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "conv_depthwise2d_grad_weight_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void conv_depthwise2d_grad_weight_kernel(\n    const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> grad_output,\n    const PackedTensorAccessor32<const scalar_t, 4, DefaultPtrTraits> input,\n    PackedTensorAccessor32<scalar_t, 4, DefaultPtrTraits> grad_weight,\n    const int batchSize,\n    const int inputChannels,\n    const int kernelChannels,\n    const int depthwiseMultiplier,\n    const int inputWidth, const int inputHeight,\n    const int outputWidth, const int outputHeight,\n    const int kernelWidth, const int kernelHeight,\n    const int strideWidth, const int strideHeight,\n    const int padWidth, const int padHeight,\n    const int dilationWidth, const int dilationHeight) {\n  using acc_t = at::acc_type<scalar_t, true>;\n  const int channelStride = kernelWidth * kernelHeight;\n\n  // Each Block is responsible for accumulating over a permutation of\n  // (channels x kH x kW), use blockIdx to determine which one\n  int bidx = blockIdx.x;\n  int kW = bidx % kernelWidth;\n  int kH = (bidx / kernelWidth) % kernelHeight;\n  int ch = (bidx / channelStride);\n\n  // Need to calculate which input channel is associated with this filter\n  // channel\n  int inputCh = ch / depthwiseMultiplier;\n\n  acc_t grad(0);\n\n  const int laneId = threadIdx.x % C10_WARP_SIZE;\n  const int batch = threadIdx.x / C10_WARP_SIZE;\n  const int nwarps = blockDim.x / C10_WARP_SIZE;\n  const int imageElements = outputWidth * outputHeight;\n  // Use warp per item.  In the original kernel, a threadblock was used to sum over NHW.\n  // Here, we use a warp to sum values over HW dimension, and if batchSize is larger than the\n  // number of warps, a warp would loop over remaining batch items (e.g. if there are 8 warps,\n  // warp 0 would go over 0-8-16 etc image, warp 1 over 1-9-17 etc). Later in blockReduce,\n  // all the warps will be reduced anyway, thus the full reduction will be over NHW, like it\n  // should be. That allows to get rid of one modulo operation inside the loop (because n/batchIdx\n  // now does not have to be computed through modulo, you are just looping over it), and\n  // bring a nice speed-up.\n  for (int batchIdx = batch; batchIdx < batchSize; batchIdx += nwarps){\n    // Warp-stride loop over elements in a batch item\n    for (index_t idx = laneId; idx < imageElements; idx += C10_WARP_SIZE) {\n    // Need to calculate the following: batch position, and offset into the grad_output\n    // in height, and width. We can intuit the corresponding position in the input from\n    // the other parameters we have\n      int go_w_offset = idx % outputWidth;\n      int go_h_offset = (idx / outputWidth);\n\n      int i_w_offset = (go_w_offset * strideWidth) + (kW * dilationWidth) - padWidth;\n      int i_h_offset = (go_h_offset * strideHeight) + (kH * dilationHeight) - padHeight;\n\n      if (i_w_offset >= 0 && i_h_offset >= 0 && i_w_offset < inputWidth && i_h_offset < inputHeight) {\n        int inputOffset = ((batchIdx * inputChannels + inputCh) * inputHeight + i_h_offset) * inputWidth + i_w_offset;\n        int outputOffset = ((batchIdx * kernelChannels + ch) * outputHeight ) * outputWidth + idx;\n        grad += (static_cast<acc_t>(input.data()[inputOffset]) *\n                 static_cast<acc_t>(grad_output.data()[outputOffset]));\n      }\n    }\n  }\n\n  // At this point each thread in the block has a local gradient, which we need to\n  // accumulate prior to writing the global value\n  extern __shared__ char smem[];\n  acc_t* buf = reinterpret_cast<acc_t*>(smem);\n  acc_t tval = cuda_utils::BlockReduceSum(grad, buf);\n\n  // After reduction, first thread in the block has the gradient, so its responsible\n  // for writing it to grad_weight\n  if (threadIdx.x == 0) {\n    int weightOffset = kW + (kernelWidth * kH) + (kernelWidth * kernelHeight * ch);\n    grad_weight.data()[weightOffset] = static_cast<scalar_t>(tval);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "kernelHistogram1D",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void kernelHistogram1D(\n    detail::TensorInfo<output_t, IndexType> a, /* output */\n    detail::TensorInfo<output_t, IndexType> p, /* partial output */\n    detail::TensorInfo<const input_t, IndexType> b, /* input */\n    int64_t nbins,\n    at::acc_type<input_t, /*is_cuda=*/true> minvalue,\n    at::acc_type<input_t, /*is_cuda=*/true> maxvalue,\n    IndexType totalElements,\n    Op getOp) {\n  extern __shared__ unsigned char my_smem[];\n  output_t* smem = nullptr;\n\n  if (MemoryType == CUDAHistogramMemoryType::SHARED) {\n    ////////////////////////// Shared memory //////////////////////////\n    // atomically add to block specific shared memory\n    // then atomically add to the global output tensor\n    smem = reinterpret_cast<output_t*>(my_smem);\n    for (IndexType i = threadIdx.x; i < a.sizes[0]; i += blockDim.x) {\n      smem[i] = 0;\n    }\n    __syncthreads();\n    FOR_KERNEL_LOOP(linearIndex, totalElements) {\n      // Convert `linearIndex` into an offset of `b`\n      const IndexType bOffset =\n          detail::IndexToOffset<const input_t, IndexType, BDims>::get(linearIndex, b);\n      const auto bVal = b.data[bOffset];\n      if (bVal >= minvalue && bVal <= maxvalue) {\n        // Use value at `b` as an offset of `smem`\n        const IndexType bin =\n            getBin<input_t, IndexType>(bVal, minvalue, maxvalue, nbins);\n        gpuAtomicAddNoReturn(&smem[bin], getOp(linearIndex));\n      }\n    }\n    __syncthreads();\n    // NOTE: atomically update output bin count.\n    //   Atomic update is imp since __syncthread() will only synchronize threads\n    //   in a given block, not across blocks.\n    for (IndexType i = threadIdx.x; i < a.sizes[0]; i += blockDim.x) {\n      const IndexType aOffset =\n          detail::IndexToOffset<output_t, IndexType, ADims>::get(i, a);\n      gpuAtomicAddNoReturn(&a.data[aOffset], smem[i]);\n    }\n\n  } else {\n    ////////////////////////// Global memory //////////////////////////\n    // atomically add to the output tensor\n    // compute histogram for the block\n    FOR_KERNEL_LOOP(linearIndex, totalElements) {\n      // Convert `linearIndex` into an offset of `b`\n      const IndexType bOffset =\n          detail::IndexToOffset<const input_t, IndexType, BDims>::get(linearIndex, b);\n      const auto bVal = b.data[bOffset];\n      if (bVal >= minvalue && bVal <= maxvalue) {\n        // Use value at `b` as an offset of `a`\n        const IndexType bin =\n            getBin<input_t, IndexType>(bVal, minvalue, maxvalue, nbins);\n        const IndexType aOffset =\n            detail::IndexToOffset<output_t, IndexType, ADims>::get(bin, a);\n        gpuAtomicAddNoReturn(&a.data[aOffset], getOp(linearIndex));\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_pool_forward_nchw",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_pool_forward_nchw(const int nthreads, const scalar_t* bottom_data,\n    const int64_t channels, const int64_t height,\n    const int64_t width, const int pooled_height, const int pooled_width,\n    const int kernel_h, const int kernel_w, const int stride_h,\n    const int stride_w, const int pad_h, const int pad_w,\n    const int dilation_h, const int dilation_w, scalar_t* top_data,\n    int64_t* top_mask) {\n  CUDA_KERNEL_LOOP(index, nthreads) {\n    int pw = index % pooled_width;\n    int ph = (index / pooled_width) % pooled_height;\n    int c = (index / pooled_width / pooled_height) % channels;\n    int n = index / pooled_width / pooled_height / channels;\n    int hstart = ph * stride_h - pad_h;\n    int wstart = pw * stride_w - pad_w;\n    int hend = min(hstart + (kernel_h - 1) * dilation_h + 1, height);\n    int wend = min(wstart + (kernel_w - 1) * dilation_w + 1, width);\n    while(hstart < 0)\n      hstart += dilation_h;\n    while(wstart < 0)\n      wstart += dilation_w;\n    scalar_t maxval = at::numeric_limits<scalar_t>::lower_bound(); // -Infinity\n    int maxidx = hstart * width + wstart;\n    const scalar_t* btm_data = bottom_data + (n * channels + c) * height * width;\n    for (int h = hstart; h < hend; h += dilation_h) {\n      for (int w = wstart; w < wend; w += dilation_w) {\n        scalar_t val = btm_data[h * width + w];\n        if ((val > maxval) || at::_isnan(val)) {\n          maxidx = h * width + w;\n          maxval = val;\n        }\n      }\n    }\n    top_data[index] = maxval;\n    top_mask[index] = maxidx;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_pool_forward_nhwc",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_pool_forward_nhwc(const scalar_t* bottom_data, const int nbatch,\n                                   const int64_t channels, const int64_t height,\n                                   const int64_t width, const int pooled_height, const int pooled_width,\n                                   const int kernel_h, const int kernel_w, const int stride_h,\n                                   const int stride_w, const int pad_h, const int pad_w,\n                                   const int dilation_h, const int dilation_w,\n                                   const int in_stride_n, const int in_stride_c,\n                                   const int in_stride_h, const int in_stride_w,\n                                   const int kernel_stride_C, const int kernel_size_C,\n                                   scalar_t* top_data, int64_t* top_mask) {\n  extern __shared__ int smem[];\n  int *out_mask_cached = smem;\n  scalar_t *out_cached = reinterpret_cast<scalar_t*>(&out_mask_cached[kernel_size_C*blockDim.x*blockDim.y*blockDim.z]);\n\n  // flattening cta for pre-computation & smem initialization;\n  int thread_id = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);\n  int block_size = blockDim.x * blockDim.y * blockDim.z;\n\n  // use shared memory to store temporary output value. This is simply to\n  // reduce register usage.\n  for (int i = thread_id; i < kernel_size_C*blockDim.x*blockDim.y*blockDim.z; i+= block_size) {\n    out_cached[i] = at::numeric_limits<scalar_t>::lower_bound();\n    out_mask_cached[i] = 0;\n  }\n\n  __syncthreads();\n\n  int batch_id = blockIdx.x % nbatch;\n  int channel_id = blockIdx.x / nbatch;\n  int channel_offset = threadIdx.x + channel_id * blockDim.x;\n\n  top_data = top_data + batch_id * pooled_height * pooled_width * channels;\n  top_mask = top_mask + batch_id * pooled_height * pooled_width * channels;\n  bottom_data = bottom_data + batch_id * in_stride_n;\n\n  out_cached = &out_cached[(threadIdx.z * blockDim.y + threadIdx.y) * kernel_size_C*blockDim.x];\n  out_mask_cached = &out_mask_cached[(threadIdx.z * blockDim.y + threadIdx.y) * kernel_size_C*blockDim.x];\n\n  int oH = (pooled_height + gridDim.z-1) / gridDim.z;\n  int oW = (pooled_width + gridDim.y-1) / gridDim.y;\n  int ostartH = threadIdx.z + blockIdx.z*oH;\n  int oendH = ::min(ostartH+oH, pooled_height);\n  int ostartW = threadIdx.y + blockIdx.y*oW;\n  int oendW = ::min(ostartW+oW, pooled_width);\n\n  for (int oh = ostartH; oh < oendH; oh+=blockDim.z) {\n    int hstart = oh * stride_h - pad_h;\n    int hend = min(hstart + (kernel_h - 1) * dilation_h + 1, height);\n    for (int ow = ostartW; ow < oendW; ow+=blockDim.y) {\n      int wstart = ow * stride_w - pad_w;\n      int wend = min(wstart + (kernel_w - 1) * dilation_w + 1, width);\n      while(hstart < 0)\n        hstart += dilation_h;\n      while(wstart < 0)\n        wstart += dilation_w;\n      for (int ih = hstart; ih < hend; ih += dilation_h) {\n        for (int iw = wstart; iw < wend; iw += dilation_w) {\n          int cached_index = threadIdx.x;\n          const scalar_t *ptr_input = bottom_data + ih * in_stride_h + iw * in_stride_w;\n          for(int c = channel_offset; c < channels; c+= blockDim.x*kernel_stride_C) {\n            scalar_t val = ptr_input[c*in_stride_c];\n            if ((val > out_cached[cached_index]) || at::_isnan(val)) {\n              out_cached[cached_index] = val;\n              out_mask_cached[cached_index] = ih * width + iw;\n            }\n            cached_index += blockDim.x;\n          }\n        }\n      }\n      scalar_t *ptr_output_data = top_data + (oh * pooled_width + ow) * channels;\n      int64_t *ptr_output_mask = top_mask + (oh * pooled_width + ow) * channels;\n\n      int cached_index = threadIdx.x;\n      for(int c = channel_offset; c < channels; c+= blockDim.x*kernel_stride_C) {\n        ptr_output_data[c] = out_cached[cached_index];\n        ptr_output_mask[c] = out_mask_cached[cached_index];\n        out_cached[cached_index] = at::numeric_limits<scalar_t>::lower_bound();\n        out_mask_cached[cached_index] = 0;\n        cached_index += blockDim.x;\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_pool_backward_nchw",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_pool_backward_nchw(const scalar_t* top_diff,\n    const int64_t* top_mask, const int num, const int64_t channels,\n    const int64_t height, const int64_t width, const int pooled_height,\n    const int pooled_width, const int kernel_h, const int kernel_w,\n    const int stride_h, const int stride_w, const int pad_h, const int pad_w,\n    const int dilation_h, const int dilation_w,\n    scalar_t* bottom_diff) {\n  CUDA_KERNEL_LOOP(index, height*width) {\n    int h = index / width;\n    int w = index - h * width;\n    int phstart = p_start(h, pad_h, kernel_h, dilation_h, stride_h);\n    int phend = p_end(h, pad_h, pooled_height, stride_h);\n    int pwstart = p_start(w, pad_w, kernel_w, dilation_w, stride_w);\n    int pwend = p_end(w, pad_w, pooled_width, stride_w);\n    for (int n = blockIdx.y; n < num; n += gridDim.y) {\n      for (int c = blockIdx.z; c < channels; c+= gridDim.z) {\n        accscalar_t gradient = accscalar_t(0);\n        int offset = (n * channels + c) * pooled_height * pooled_width;\n        for (int ph = phstart; ph < phend; ++ph) {\n          for (int pw = pwstart; pw < pwend; ++pw) {\n            if (top_mask[ph * pooled_width + pw + offset] == h * width + w) {\n              gradient += static_cast<accscalar_t>(top_diff[ph * pooled_width + pw + offset]);\n            }\n          }\n        }\n        bottom_diff[(n*channels+c)*height*width+index] = static_cast<scalar_t>(gradient);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_pool_backward_nhwc",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_pool_backward_nhwc(const scalar_t* top_diff,\n                                    const int64_t* top_mask, const int nbatch, const int64_t channels,\n                                    const int64_t height, const int64_t width, const int pooled_height,\n                                    const int pooled_width, const int kernel_h, const int kernel_w,\n                                    const int stride_h, const int stride_w, const int pad_h, const int pad_w,\n                                    const int dilation_h, const int dilation_w,\n                                    const int out_stride_c, const int out_stride_h, const int out_stride_w,\n                                    const int kernel_stride_C, const int kernel_size_C,\n                                    scalar_t* bottom_diff) {\n  extern __shared__ int smem[];\n  accscalar_t *out_cached = reinterpret_cast<accscalar_t*>(smem);\n\n  int thread_id = threadIdx.x + blockDim.x * (threadIdx.y + blockDim.y * threadIdx.z);\n  int block_size = blockDim.x * blockDim.y * blockDim.z;\n\n  int batch_id = blockIdx.x % nbatch;\n  int channel_id = blockIdx.x / nbatch;\n  int channel_offset = threadIdx.x + channel_id * blockDim.x;\n\n  for (int i = thread_id; i < kernel_size_C*blockDim.x*blockDim.y*blockDim.z; i+= block_size) {\n    out_cached[i] = accscalar_t(0.0);\n  }\n\n  __syncthreads();\n\n  out_cached = &out_cached[(threadIdx.z * blockDim.y + threadIdx.y) * kernel_size_C*blockDim.x];\n\n  bottom_diff = bottom_diff + batch_id * height * width * channels;\n  top_mask = top_mask + batch_id * pooled_height * pooled_width * channels;\n  top_diff = top_diff + batch_id * pooled_height * pooled_width * channels;\n\n  int iH = (height + gridDim.z-1) / gridDim.z;\n  int iW = (width + gridDim.y-1) / gridDim.y;\n  int istartH = threadIdx.z + blockIdx.z*iH;\n  int iendH = ::min(static_cast<int64_t>(istartH)+iH, height);\n  int istartW = threadIdx.y + blockIdx.y*iW;\n  int iendW = ::min(static_cast<int64_t>(istartW)+iW, width);\n\n  for (int ih = istartH; ih < iendH; ih+=blockDim.z) {\n    int phstart = p_start(ih, pad_h, kernel_h, dilation_h, stride_h);\n    int phend = p_end(ih, pad_h, pooled_height, stride_h);\n    for (int iw = istartW; iw < iendW; iw+=blockDim.y) {\n      int pwstart = p_start(iw, pad_w, kernel_w, dilation_w, stride_w);\n      int pwend = p_end(iw, pad_w, pooled_width, stride_w);\n      int index_shift = ih * width + iw;\n      if ((phstart + 1 != phend) || (pwstart + 1 != pwend)) {\n        for(int oh = phstart; oh < phend; ++oh) {\n          for(int ow = pwstart; ow < pwend; ++ow) {\n            int cached_index = threadIdx.x;\n            const int64_t* ptr_top_mask = top_mask + oh * out_stride_h + ow * out_stride_w;\n            for (int c = channel_offset; c < channels; c += blockDim.x*kernel_stride_C) {\n              if (ptr_top_mask[c*out_stride_c] == index_shift) {\n                out_cached[cached_index] +=\n                  static_cast<accscalar_t>(top_diff[oh * out_stride_h + ow * out_stride_w + c*out_stride_c]);\n              }\n              cached_index += blockDim.x;\n            }\n          }\n        }\n        scalar_t *ptr_bottom_diff = bottom_diff + index_shift * channels;\n        int cached_index = threadIdx.x;\n        for (int c = channel_offset; c < channels; c += blockDim.x*kernel_stride_C) {\n          ptr_bottom_diff[c] = static_cast<scalar_t>(out_cached[cached_index]);\n          out_cached[cached_index] = accscalar_t(0.0);\n          cached_index += blockDim.x;\n        }\n      } else {\n        const int64_t* ptr_top_mask = top_mask + phstart * out_stride_h + pwstart * out_stride_w;\n        scalar_t *ptr_bottom_diff = bottom_diff + index_shift * channels;\n        int cached_index = threadIdx.x;\n        for (int c = channel_offset; c < channels; c += blockDim.x*kernel_stride_C) {\n          if (ptr_top_mask[c*out_stride_c] == index_shift) {\n            ptr_bottom_diff[c] =\n              static_cast<scalar_t>(top_diff[phstart * out_stride_h + pwstart * out_stride_w + c*out_stride_c]);\n          }\n          cached_index += blockDim.x;\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "_scatter_gather_elementwise_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _scatter_gather_elementwise_kernel(int N, func_t f) {\n  constexpr int nv = nt * vt;\n  int idx = nv * blockIdx.x + threadIdx.x;\n\n  #pragma unroll\n  for (int i = 0; i < vt; ++i) {\n    if (idx < N) {\n      f(idx);\n      idx += nt;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "kernel_pointwise_flip_apply2",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void kernel_pointwise_flip_apply2(\n    const cuda::detail::TensorInfo<scalar_t, IndexType> in_tensor_info,\n    cuda::detail::TensorInfo<scalar_t, IndexType> out_tensor_info,\n    IndexType N,\n    int flip_dim,\n    IndexType total_dims) {\n  for (IndexType linear_index = blockIdx.x * blockDim.x + threadIdx.x; linear_index < N; linear_index += gridDim.x * blockDim.x) {\n    IndexType dst_offset = 0;\n    if (flip_dim == 0) {\n      // flip 1st dim\n      dst_offset = (in_tensor_info.sizes[0] - 1 - linear_index / in_tensor_info.strides[0]) * in_tensor_info.strides[0] + linear_index % in_tensor_info.strides[0];\n    }\n    else {\n      // flip last dim\n      IndexType i = total_dims - 1;\n      dst_offset = linear_index / in_tensor_info.strides[0] * in_tensor_info.strides[0] + (in_tensor_info.sizes[i] - 1 - linear_index % in_tensor_info.strides[0]);\n    }\n    out_tensor_info.data[dst_offset] = in_tensor_info.data[linear_index];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "flip_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void flip_cuda_kernel(\n    scalar_t* in_tensor,\n    scalar_t* out_tensor,\n    int64_t N,\n    int64_t* flip_dims,\n    int64_t flip_dims_size,\n    int64_t* strides,\n    int64_t* strides_contiguous,\n    int64_t* shape,\n    int64_t total_dims) {\n  int64_t linear_index = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (linear_index >= N) {\n    return;\n  }\n\n  int64_t cur_indices = linear_index, rem = 0, dst_offset = 0;\n  for (int64_t i = 0; i < total_dims; i++) {\n    int64_t temp = cur_indices;\n    cur_indices = cur_indices / strides_contiguous[i];\n    rem = temp - cur_indices * strides_contiguous[i];\n    // flip the indices if it is in flip_dims\n    for (int64_t j = 0; j < flip_dims_size; j++) {\n      if (i == flip_dims[j]) {\n        cur_indices = shape[i] - 1 - cur_indices;\n      }\n    }\n    dst_offset += cur_indices * strides[i];\n    cur_indices = rem;\n  }\n  out_tensor[linear_index] = in_tensor[dst_offset];\n}",
      "disabled": true
    },
    {
      "kernel_name": "roll_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void roll_cuda_kernel(\n    const scalar_t* in_tensor,\n    scalar_t* out_tensor,\n    int64_t N,\n    int64_t roll_dim,\n    int64_t start,\n    int64_t size,\n    int64_t stride,\n    int64_t total_dims) {\n  int64_t linear_index = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  if (linear_index >= N) {\n    return;\n  }\n  // roll dim idx is the index of linear_index along the rolling dimension.\n  int64_t roll_dim_idx = linear_index % (stride * size) / stride;\n  // index into the source data to find appropriate value.\n  int64_t source_idx = 0;\n  if( roll_dim_idx >= (size - start) ) {\n    source_idx = linear_index - ((size - start) * stride);\n  } else {\n    source_idx = linear_index + (start * stride);\n  }\n  out_tensor[linear_index] = in_tensor[source_idx];\n}",
      "disabled": true
    },
    {
      "kernel_name": "embedding_backward_feature_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void embedding_backward_feature_kernel\n  (const index_t* indices,\n   const scalar_t* __restrict__ grad,\n   scalar_t* __restrict__ grad_weight,\n   int n, // OK to pass as int, we don't expect 2 billion+ samples in one shot\n   int64_t stride,\n   int padding_idx)\n{\n  extern __shared__ char buf[];\n  accscalar_t* smem = (accscalar_t*)buf;\n  accscalar_t* my_s = smem + C10_WARP_SIZE*threadIdx.y;\n  int* indices_batch = (int*)(buf + sizeof(accscalar_t)*C10_WARP_SIZE*blockDim.y);\n\n  const int s = (int)stride; // OK to make int, we don't expect 2 billion+ embedding row size\n\n  const int f = threadIdx.x + blockIdx.x*blockDim.x; // feature_dim\n\n  for(int batch_start = 0; batch_start < n; batch_start += blockDim.x*blockDim.y)\n  {\n    // Entire block cooperates to load a batch of 1024 indices to process\n    int tid = threadIdx.x + threadIdx.y*blockDim.x;\n    if(batch_start + tid < n)\n      indices_batch[tid] = (int)indices[batch_start + tid];\n\n    int batch_end = batch_start + blockDim.x*blockDim.y < n ?\n                    batch_start + blockDim.x*blockDim.y : n;\n\n    // Loop over the batch of <= 1024 loaded indices in chunks of blockDim.y = 32\n    for(int chunk_start = batch_start; chunk_start < batch_end; chunk_start += blockDim.y)\n    {\n      // This does double duty:  it makes sure indices_batch is ready, and it makes sure match-group\n      // leaders are done with their accumulates before other warps start loading again.\n      __syncthreads();\n\n      int n_this_chunk = (batch_end - chunk_start) < blockDim.y ?\n                         (batch_end - chunk_start) : blockDim.y;\n\n      int src_row = chunk_start + threadIdx.y;\n      int dst_row = indices_batch[src_row - batch_start]; // This warp's target row in grad_weight\n\n      // All warps load their smem segments with incoming grad data\n      if(src_row < n && f < s && dst_row != padding_idx)\n        my_s[threadIdx.x] = static_cast<accscalar_t>(grad[src_row*stride + f]);\n\n      __syncthreads();\n\n      // To ensure determinism, we can't just have each warp add its grad data to its dst_row.\n      // We need to check if any other warps pulled grad data targeting dst_row.\n      // If so, we elect the first warp in each matching group as the leader.\n      // Each leader warp serializes the accumulates targeting dst_row in shared memory,\n      // then finishes by adding the accumulated buffer to dst_row in grad_weight.\n      if(dst_row != padding_idx && src_row < n) // Per-warp exit condition, safe with ballot_sync\n      {\n        int match_found_this_thread = 0;\n        if(threadIdx.x < n_this_chunk)\n          match_found_this_thread = (dst_row == indices_batch[chunk_start - batch_start + threadIdx.x]);\n#if defined(USE_ROCM)\n        unsigned long long int matchmask = WARP_BALLOT(match_found_this_thread);\n        int first_remaining_peer = __ffsll(matchmask) - 1;\n#else\n        unsigned int matchmask = WARP_BALLOT(match_found_this_thread);\n        int first_remaining_peer = __ffs(matchmask) - 1;\n#endif\n\n        if(threadIdx.y == first_remaining_peer) // Nominate lowest-indexed warp as the leader\n        {\n          matchmask ^= (1 << first_remaining_peer);\n          while(matchmask)\n          {\n#if defined(USE_ROCM)\n            first_remaining_peer = __ffsll(matchmask) - 1;\n#else\n            first_remaining_peer = __ffs(matchmask) - 1;\n#endif\n            my_s[threadIdx.x] += smem[threadIdx.x + C10_WARP_SIZE*first_remaining_peer];\n            matchmask ^= (1 << first_remaining_peer);\n          }\n          if(f < s)\n            grad_weight[dst_row*stride + f] += static_cast<scalar_t>(my_s[threadIdx.x]);\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "embedding_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void embedding_backward_kernel(\n  index_t* input, index_t* indices, scalar_t* grad_output, scalar_t* grad_weight,\n  index_t* count, int64_t numel, int64_t stride, int padding_idx) {\n\n  using accscalar_t = acc_type<scalar_t, true>;\n  int idx = blockIdx.x * 4 + threadIdx.y;\n\n  // Each warp is responsible for an input into the LookupTable.\n  // If the preceding input has the same as this input, then the warp\n  // exits immediately. The warp also processes subsequent inputs with the\n  // same value.\n  //\n  // Input Warp\n  // 1     <warp 1>\n  // 1     <warp 1> (<warp 2> exits without doing any work)\n  // 5     <warp 3>\n  // 8     <warp 4>\n\n  // Number of values processed by each thread (grain size)\n  const int SZ = 4;\n\n  if (idx < numel\n      && (idx == 0 || input[idx] != input[idx - 1])\n      && input[idx] != padding_idx) {\n    do {\n      const int start_feature = threadIdx.x + blockIdx.y * blockDim.x * SZ;\n      const int weight_row = ((int) input[idx]) * stride;\n      const int grad_row = ((int) indices[idx]) * stride;\n      const accscalar_t scale = count ? (accscalar_t)1.0 / count[idx] : 1.0;\n\n      accscalar_t gradient[SZ];\n      accscalar_t weight[SZ];\n\n      #pragma unroll\n      for (int ii = 0; ii < SZ; ii++) {\n        int feature_dim = start_feature + ii * C10_WARP_SIZE;\n        if (feature_dim < stride) {\n          gradient[ii] = static_cast<accscalar_t>(grad_output[grad_row + feature_dim]);\n          weight[ii] = static_cast<accscalar_t>(grad_weight[weight_row + feature_dim]);\n        }\n      }\n\n      #pragma unroll\n      for (int ii = 0; ii < SZ; ii++) {\n        weight[ii] += gradient[ii] * scale;\n      }\n\n      #pragma unroll\n      for (int ii = 0; ii < SZ; ii++) {\n        int feature_dim = start_feature + ii * C10_WARP_SIZE;\n        if (feature_dim < stride) {\n            grad_weight[weight_row + feature_dim] = static_cast<scalar_t>(weight[ii]);\n        }\n      }\n\n      idx++;\n    } while (idx < numel && input[idx] == input[idx - 1]);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "renorm_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void renorm_kernel(\n    scalar_t* weights, index_t* indices, accscalar_t max_norm,\n    accscalar_t norm_type, int64_t dim,\n    int64_t weights_stride0, int64_t weights_stride1,\n    const int64_t *num_unique_indices) {\n  if (blockIdx.x >= *num_unique_indices) {\n    return;\n  }\n\n  // Some casting hacks since dynamic shared memory and templates don't work together:\n  extern __shared__ unsigned char smem[];\n  auto sdata = reinterpret_cast<accscalar_t*>(smem);\n\n  int tid = threadIdx.x;\n  int base_index = indices[blockIdx.x] * weights_stride0;\n\n  accscalar_t v = 0;\n  for (int i = tid; i < dim; i += blockDim.x) {\n    auto x = static_cast<accscalar_t>(weights[base_index + i * weights_stride1]);\n    if (norm_type == 1) {\n      v += std::abs(x);\n    } else if (norm_type == 2) {\n      v += x * x;\n    } else {\n      v += std::pow(x, norm_type);\n    }\n  }\n\n  v = cuda_utils::BlockReduceSum(v, sdata);\n\n  if (tid == 0) {\n    sdata[0] = std::pow(v, static_cast<accscalar_t>(1.0 / norm_type));\n  }\n  __syncthreads();\n\n  // now we renormalize the blocks that need it\n  if (sdata[0] > max_norm) {\n    auto factor = static_cast<scalar_t>(max_norm / (sdata[0] + 1e-7));\n    for (int i = tid; i < dim; i += blockDim.x) {\n      weights[base_index + i * weights_stride1] *= factor;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptivemaxpool",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptivemaxpool(const T *input, T *output, int64_t *indices,\n                        int isizeH, int isizeW,\n                        int osizeH, int osizeW,\n                        int64_t istrideD, int64_t istrideH, int64_t istrideW)\n{\n  // iterators\n  int oh, ow;\n\n  // compute offsets based on thread/block ID\n  int o_plane = blockIdx.x;\n  int i_plane = o_plane;\n\n  int ostartW = threadIdx.x;\n  int oendW = osizeW;\n  const int ostepW = blockDim.x;\n\n  int ostartH = blockDim.y*blockIdx.y + threadIdx.y;\n  int oendH = osizeH;\n  const int ostepH = blockDim.y*gridDim.y;\n  // select input/output plane\n  output = output + o_plane*osizeH*osizeW;\n  input = input + i_plane*istrideD;\n  indices = indices + o_plane*osizeH*osizeW;\n\n  // For all output pixels...\n  for(oh = ostartH; oh < oendH; oh += ostepH) {\n\n    int istartH = start_index(oh, osizeH, isizeH);\n    int iendH   = end_index(oh, osizeH, isizeH);\n    int kH = iendH - istartH;\n\n    for(ow = ostartW; ow < oendW; ow += ostepW) {\n      int istartW = start_index(ow, osizeW, isizeW);\n      int iendW   = end_index(ow, osizeW, isizeW);\n\n      int kW = iendW - istartW;\n\n      // Compute the mean of the input image...\n      const T *ptr_input = input + istartH*istrideH + istartW*istrideW;\n      T *ptr_output = output + oh*osizeW + ow;\n      int64_t *ptr_ind = indices + oh*osizeW + ow;\n      int argmax = istartH * isizeW + istartW;\n      T max = at::numeric_limits<T>::lower_bound(); // -Infinity\n      int ih, iw;\n      for(ih = 0; ih < kH; ih++) {\n        for(iw = 0; iw < kW; iw++) {\n          T val = ptr_input[iw*istrideW];\n          if ((val > max) || at::_isnan(val)) {\n            max = val;\n            argmax = (ih+istartH)*isizeW + iw+istartW;\n          }\n        }\n        ptr_input += istrideH; // next input line\n      }\n      // Update output and argmax\n      *ptr_output = max;\n      *ptr_ind = argmax;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptivemaxgradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptivemaxgradinput(T *gradInput, const T *gradOutput, const int64_t *indices,\n                             int isizeH, int isizeW,\n                             int osizeH, int osizeW)\n{\n  // iterators\n  int oh, ow;\n\n  // compute offsets based on thread/block ID\n  int o_plane = blockIdx.x;\n  int i_plane = o_plane;\n  //int k = blockIdx.x % sizeD;\n\n  int ostartW = threadIdx.x;\n  int oendW = osizeW;\n  int ostepW = blockDim.x;\n\n  int ostartH = blockDim.y*blockIdx.y + threadIdx.y;\n  int oendH = osizeH;\n  int ostepH = blockDim.y*gridDim.y;\n\n  // select input/output plane\n  gradOutput = gradOutput + o_plane*osizeH*osizeW;\n  gradInput = gradInput + i_plane*isizeH*isizeW;\n  indices = indices + o_plane*osizeH*osizeW;\n\n  // compute gradInput\n  for(oh = ostartH; oh < oendH; oh += ostepH) {\n\n    for(ow = ostartW; ow < oendW; ow += ostepW) {\n\n      const T *ptr_gradOutput = gradOutput + oh*osizeW + ow;\n      const int64_t *ptr_ind = indices + oh*osizeW + ow;\n      T z = *ptr_gradOutput;\n\n      int argmax = (*ptr_ind);\n\n      gradInput[argmax] += z;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "atomicadaptivemaxgradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void atomicadaptivemaxgradinput(\n  T *gradInput, const T *gradOutput, const int64_t *indices,\n  int isizeH, int isizeW, int osizeH, int osizeW\n)\n{\n  // iterators\n  int oh, ow;\n\n  // compute offsets based on thread/block ID\n  int o_plane = blockIdx.x;\n  int i_plane = o_plane;\n\n  int ostartW = threadIdx.x;\n  int oendW = osizeW;\n  int ostepW = blockDim.x;\n\n  int ostartH = blockDim.y*blockIdx.y + threadIdx.y;\n  int oendH = osizeH;\n  int ostepH = blockDim.y*gridDim.y;\n\n  // select input/output plane\n  gradOutput = gradOutput + o_plane*osizeH*osizeW;\n  gradInput = gradInput + i_plane*isizeH*isizeW;\n  indices = indices + o_plane*osizeH*osizeW;\n\n  // compute gradInput\n  for(oh = ostartH; oh < oendH; oh += ostepH) {\n\n    for(ow = ostartW; ow < oendW; ow += ostepW) {\n\n      const T *ptr_gradOutput = gradOutput + oh*osizeW + ow;\n      const int64_t *ptr_ind = indices + oh*osizeW + ow;\n      T z = *ptr_gradOutput;\n\n      int argmax = (*ptr_ind);\n\n      // atomic add since different threads could update same variable\n      gpuAtomicAddNoReturn(&(gradInput[argmax]), z);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_unpooling2d_forward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_unpooling2d_forward_kernel(\n    const int64_t numInputElements,\n    const T* input,\n    const int64_t* indices,\n    const int64_t numChannels,\n    const int64_t inputHeight,\n    const int64_t inputWidth,\n    const int64_t outputHeight,\n    const int64_t outputWidth,\n    T* output) {\n  int64_t outputImageSize = outputHeight * outputWidth;\n  CUDA_KERNEL_LOOP(linearIndex, numInputElements) {\n    int c = (linearIndex / inputWidth / inputHeight) % numChannels;\n    int n = linearIndex / inputWidth / inputHeight / numChannels;\n    output += (n * numChannels + c) * outputHeight * outputWidth;\n    int maxind = indices[linearIndex];\n    CUDA_KERNEL_ASSERT(maxind >= 0 && maxind < outputImageSize);\n    output[maxind] = input[linearIndex];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_unpooling3d_forward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_unpooling3d_forward_kernel(\n    PackedTensorAccessor64<const T, 4> input,\n    PackedTensorAccessor64<const int64_t, 4> indices,\n    T* output,\n    const int64_t oT,\n    const int64_t oH,\n    const int64_t oW,\n    const int64_t offsetZ) {\n  int64_t iColumn = ((int64_t) blockIdx.x) * blockDim.x + threadIdx.x;\n  int64_t iRow = blockIdx.y * blockDim.y + threadIdx.y;\n  int64_t iFrame = (blockIdx.z + offsetZ) % input.size(1); // input frame/time\n  int64_t slice = (blockIdx.z + offsetZ) / input.size(1); // input slice/feature\n  int64_t outputImageSize = oT * oH * oW;\n  if (iRow < input.size(2) && iColumn < input.size(3)) {\n    const T val = input[slice][iFrame][iRow][iColumn];\n    const int64_t index = indices[slice][iFrame][iRow][iColumn];\n    CUDA_KERNEL_ASSERT(index >= 0 && index < outputImageSize);\n    output[slice * oT * oH * oW + index] = val;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_unpooling2d_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_unpooling2d_backward_kernel(\n    const int64_t numInputElements,\n    const T* input,\n    const int64_t* indices,\n    const int64_t numChannels,\n    const int64_t inputHeight,\n    const int64_t inputWidth,\n    const int64_t outputHeight,\n    const int64_t outputWidth,\n    T* output) {\n  CUDA_KERNEL_LOOP(linearIndex, numInputElements) {\n    int c = (linearIndex / inputWidth / inputHeight) % numChannels;\n    int n = linearIndex / inputWidth / inputHeight / numChannels;\n    input += (n * numChannels + c) * outputHeight * outputWidth;\n    int maxind = indices[linearIndex];\n    output[linearIndex] = input[maxind];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "max_unpooling3d_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void max_unpooling3d_backward_kernel(\n    const T* gradOutputData,\n    int64_t oT,\n    int64_t oH,\n    int64_t oW,\n    PackedTensorAccessor64<int64_t, 4> indices,\n    PackedTensorAccessor64<T, 4> gradInput,\n    int offsetZ) {\n  int iColumn = blockIdx.x * blockDim.x + threadIdx.x;\n  int iRow = blockIdx.y * blockDim.y + threadIdx.y;\n  int iFrame = (blockIdx.z + offsetZ) % gradInput.size(1); // output frame/time\n  int slice =\n      (blockIdx.z + offsetZ) / gradInput.size(1); // output slice/feature\n\n  if (iRow < gradInput.size(2) && iColumn < gradInput.size(3)) {\n    int64_t index = indices[slice][iFrame][iRow][iColumn];\n    T grad_val = gradOutputData[slice * oT * oH * oW + index];\n    gradInput[slice][iFrame][iRow][iColumn] = grad_val;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "C10_LAUNCH_BOUNDS_2",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\nC10_LAUNCH_BOUNDS_2((std::is_same_v<scalar_t, float> ? 1024 : 896), 1)\nctc_loss_backward_log_beta_gpu_kernel(scalar_t* __restrict__ log_beta_data,\n                                      const scalar_t*log_probs_data, const int64_t* __restrict__ input_lengths, int64_t max_input_length,\n                                      const target_t* __restrict__ targets_data, const int64_t* __restrict__ target_lengths, int64_t max_target_length,\n                                      int64_t lp_input_stride, int64_t lp_batch_stride, int64_t lp_char_stride,\n                                      int64_t lb_batch_stride, int64_t lb_input_stride, int64_t lb_target_stride,\n                                      const int64_t* __restrict__ tg_batch_offsets, int64_t tg_target_stride,\n                                      int64_t batch_size, int64_t BLANK) {\n  constexpr scalar_t neginf = -INFINITY;\n\n  int64_t b = threadIdx.y + blockIdx.y * blockDim.y;\n\n  if (b >= batch_size)\n    return;\n\n  int64_t input_length = input_lengths[b];\n  int64_t target_length = target_lengths[b];\n  int64_t lp_batch_offset = b*lp_batch_stride;\n  int64_t lb_batch_offset = b*lb_batch_stride;\n  int64_t tg_batch_offset = tg_batch_offsets[b];\n\n  if (input_length == 0)\n    return;\n\n  // \"first\" row, the beta initialization before eq (10) (t=target_length - differes per batch)\n  for (int64_t block_s = 2*max_target_length - (2*max_target_length % blockDim.x); block_s >= 0; block_s -= blockDim.x) {\n    int64_t s = threadIdx.x + block_s;\n    scalar_t lb;\n    if (s == 2*target_length) {\n      lb = log_probs_data[lp_batch_offset + (input_length-1) * lp_input_stride + lp_char_stride * BLANK];\n    } else if (s == 2 * target_length - 1) { // false for target_length == 0\n      int64_t current_target_prime = get_target_prime(\n          targets_data,\n          tg_batch_offset,\n          tg_target_stride,\n          s,\n          BLANK);\n      lb = log_probs_data[lp_batch_offset + (input_length-1) * lp_input_stride + lp_char_stride * current_target_prime];\n    } else {\n      lb = neginf;\n    }\n    if (s < 2*max_target_length+1) {\n      log_beta_data[lb_batch_offset + (input_length-1) * lb_input_stride + lb_target_stride * s] = lb;\n    }\n  }\n\n  // go backward in s\n  for (int64_t block_s = 2*max_target_length - (2*max_target_length % blockDim.x); block_s >= 0; block_s -= blockDim.x) {\n    int64_t s = threadIdx.x + block_s;\n    int64_t current_target_prime;\n    bool have_three;\n    if (s < 2 * target_length + 1 && target_length > 0) {\n      current_target_prime = get_target_prime(\n          targets_data,\n          tg_batch_offset,\n          tg_target_stride,\n          s,\n          BLANK);\n      have_three =\n          ((s < 2 * target_length - 1) &&\n           (get_target_prime(\n                targets_data,\n                tg_batch_offset,\n                tg_target_stride,\n                s + 2,\n                BLANK) != current_target_prime));\n    } else {\n      current_target_prime = BLANK;\n      have_three = false;\n    }\n    // now go backward in t. Note that we need to skip the last timestep that we did above.\n    for (int64_t t=max_input_length-2; t>=0; t--) {\n      __syncthreads(); // on cuda 9 we might use partial synchronization of only the threads within the same batch item\n      if ((t < input_length - 1) && (s < 2 * target_length + 1)) {\n        scalar_t lb1 = log_beta_data[lb_batch_offset + lb_input_stride * (t+1) + lb_target_stride * s];\n        scalar_t lbmax = lb1;\n        scalar_t lb2, lb3;\n\n        if (s < 2*target_length) {\n          lb2 = log_beta_data[lb_batch_offset + lb_input_stride * (t+1) + lb_target_stride * (s+1)];\n          if (lb2 > lbmax)\n            lbmax = lb2;\n        } else {\n          lb2 = neginf;\n        }\n        if (have_three) {\n          lb3 = log_beta_data[lb_batch_offset + lb_input_stride * (t+1) + lb_target_stride * (s+2)];\n          if (lb3 > lbmax)\n            lbmax = lb3;\n        } else {\n          lb3 = neginf;\n        }\n        if (lbmax == neginf)\n          lbmax = 0;\n\n        scalar_t lb = std::log(std::exp(lb1-lbmax)+std::exp(lb2-lbmax)+std::exp(lb3-lbmax))+lbmax\n          + log_probs_data[lp_batch_offset + t * lp_input_stride + lp_char_stride * current_target_prime];\n\n        log_beta_data[lb_batch_offset + lb_input_stride * t + lb_target_stride * s] = lb;\n      } else if (\n          (s < 2 * max_target_length + 1) &&\n          (((target_length == 0) && (s > 0)) || (s >= 2 * target_length + 1) ||\n           (t >= input_length))) {\n        log_beta_data\n            [lb_batch_offset + lb_input_stride * t + lb_target_stride * s] =\n                neginf;\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_bicubic2d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_bicubic2d_out_frame(\n    const int num_elements,\n    const accscalar_t height_scale,\n    const accscalar_t width_scale,\n    const bool align_corners,\n    const PackedTensorAccessor64<const scalar_t, 4> idata,\n    PackedTensorAccessor64<scalar_t, 4> odata) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int input_height = idata.size(2);\n  const int input_width = idata.size(3);\n  const int output_height = odata.size(2);\n  const int output_width = odata.size(3);\n\n  if (index >= num_elements) {\n    return;\n  }\n\n  // Special case: input and output are the same size, just copy\n  const int output_x = index % output_width;\n  const int output_y = index / output_width;\n\n  if (input_height == output_height && input_width == output_width) {\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; c++) {\n        const scalar_t val = idata[n][c][output_y][output_x];\n        odata[n][c][output_y][output_x] = val;\n      }\n    }\n    return;\n  }\n\n  // Interpolation kernel\n  accscalar_t real_x = area_pixel_compute_source_index(\n      width_scale, output_x, align_corners, /*cubic=*/true);\n  int in_x = floorf(real_x);\n  accscalar_t t_x = real_x - in_x;\n\n  accscalar_t real_y = area_pixel_compute_source_index(\n      height_scale, output_y, align_corners, /*cubic=*/true);\n  int in_y = floorf(real_y);\n  accscalar_t t_y = real_y - in_y;\n\n  for (int n = 0; n < batchsize; n++) {\n    for (int c = 0; c < channels; c++) {\n      accscalar_t coefficients[4];\n\n      for (int k = 0; k < 4; k++) {\n        coefficients[k] = cubic_interp1d(\n            upsample_get_value_bounded<scalar_t>(\n                idata, n, c, input_height, input_width, in_y - 1 + k, in_x - 1),\n            upsample_get_value_bounded<scalar_t>(\n                idata, n, c, input_height, input_width, in_y - 1 + k, in_x + 0),\n            upsample_get_value_bounded<scalar_t>(\n                idata, n, c, input_height, input_width, in_y - 1 + k, in_x + 1),\n            upsample_get_value_bounded<scalar_t>(\n                idata, n, c, input_height, input_width, in_y - 1 + k, in_x + 2),\n            t_x);\n      }\n\n      odata[n][c][output_y][output_x] = static_cast<scalar_t>(cubic_interp1d(\n          coefficients[0],\n          coefficients[1],\n          coefficients[2],\n          coefficients[3],\n          t_y));\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_bicubic2d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_bicubic2d_backward_out_frame(\n    const int num_elements,\n    const accscalar_t height_scale,\n    const accscalar_t width_scale,\n    const bool align_corners,\n    PackedTensorAccessor64<scalar_t, 4> idata,\n    const PackedTensorAccessor64<const scalar_t, 4> odata) {\n  int index = threadIdx.x + blockIdx.x * blockDim.x;\n\n  const int batchsize = idata.size(0);\n  const int channels = idata.size(1);\n  const int input_height = idata.size(2);\n  const int input_width = idata.size(3);\n  const int output_height = odata.size(2);\n  const int output_width = odata.size(3);\n\n  if (index >= num_elements) {\n    return;\n  }\n\n  const int output_x = index % output_width;\n  const int output_y = index / output_width;\n  // special case: output_xust copy\n  if (input_height == output_height && input_width == output_width) {\n    for (int n = 0; n < batchsize; n++) {\n      for (int c = 0; c < channels; ++c) {\n        const scalar_t val = odata[n][c][output_y][output_x];\n        idata[n][c][output_y][output_x] = val;\n      }\n    }\n    return;\n  }\n\n  accscalar_t real_x = area_pixel_compute_source_index(\n      width_scale, output_x, align_corners, /*cubic=*/true);\n  int input_x = floorf(real_x);\n  accscalar_t t_x = real_x - input_x;\n\n  accscalar_t real_y = area_pixel_compute_source_index(\n      height_scale, output_y, align_corners, /*cubic=*/true);\n  int input_y = floorf(real_y);\n  accscalar_t t_y = real_y - input_y;\n\n  accscalar_t x_coeffs[4];\n  accscalar_t y_coeffs[4];\n\n  get_cubic_upsampling_coefficients(x_coeffs, t_x);\n  get_cubic_upsampling_coefficients(y_coeffs, t_y);\n\n  for (int n = 0; n < batchsize; n++) {\n    for (int c = 0; c < channels; ++c) {\n      scalar_t out_value = odata[n][c][output_y][output_x];\n      for (int i = 0; i < 4; i++) {\n        for (int j = 0; j < 4; j++) {\n          upsample_increment_value_bounded<scalar_t, accscalar_t>(\n              idata,\n              n,\n              c,\n              input_height,\n              input_width,\n              input_y - 1 + i,\n              input_x - 1 + j,\n              out_value * y_coeffs[i] * x_coeffs[j]);\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool2d_out_cuda_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool2d_out_cuda_frame(const int nthreads,\n    const scalar_t* const bottom_data, const int64_t channels,\n    const int64_t height, const int64_t width, const int64_t pooled_height,\n    const int pooled_width, const int kernel_h, const int kernel_w,\n    const int stride_h, const int stride_w, const int pad_h, const int pad_w,\n    scalar_t* const top_data, const int divisor_override,\n    const bool count_include_pad, const bool use_divisor) {\n  CUDA_KERNEL_LOOP(index, nthreads) {\n    const int pw = index % pooled_width;\n    const int ph = (index / pooled_width) % pooled_height;\n    const int c = (index / pooled_width / pooled_height) % channels;\n    const int n = index / pooled_width / pooled_height / channels;\n    int hstart = ph * stride_h - pad_h;\n    int wstart = pw * stride_w - pad_w;\n    int hend = min(hstart + kernel_h, height + pad_h);\n    int wend = min(wstart + kernel_w, width + pad_w);\n    const int pool_size = (hend - hstart) * (wend - wstart);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    hend = min(hend, height);\n    wend = min(wend, width);\n\n    if (hstart >= hend || wstart >= wend) {\n      top_data[index] = scalar_t(0);\n      continue;\n    }\n\n    accscalar_t aveval = accscalar_t(0);\n    const scalar_t* const bottom_slice = bottom_data + (n * channels + c) * height * width;\n    for (int h = hstart; h < hend; ++h) {\n      for (int w = wstart; w < wend; ++w) {\n        aveval += bottom_slice[h * width + w];\n      }\n    }\n    int divide_factor;\n    if (use_divisor) {\n      divide_factor = divisor_override;\n    } else {\n      if(count_include_pad) {\n        divide_factor = pool_size;\n      } else {\n        divide_factor = (hend - hstart) * (wend - wstart);\n      }\n    }\n    top_data[index] = static_cast<scalar_t>(aveval / divide_factor);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool2d_out_cuda_frame_nhwc",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool2d_out_cuda_frame_nhwc(const int nthreads,\n    const scalar_t* const bottom_data, const int64_t channels,\n    const int64_t height, const int64_t width, const int pooled_height,\n    const int pooled_width, const int kernel_h, const int kernel_w,\n    const int stride_h, const int stride_w, const int pad_h, const int pad_w,\n    scalar_t* const top_data, const int divisor_override,\n    const bool count_include_pad, const bool use_divisor) {\n  CUDA_KERNEL_LOOP(index, nthreads) {\n    const int c = index % channels;\n    const int pw = (index / channels) % pooled_width;\n    const int ph = (index / channels / pooled_width) % pooled_height;\n    const int n = index / channels / pooled_width / pooled_height;\n    int hstart = ph * stride_h - pad_h;\n    int wstart = pw * stride_w - pad_w;\n    int hend = min(hstart + kernel_h, height + pad_h);\n    int wend = min(wstart + kernel_w, width + pad_w);\n    const int pool_size = (hend - hstart) * (wend - wstart);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    hend = min(hend, height);\n    wend = min(wend, width);\n\n    if (hstart >= hend || wstart >= wend) {\n      top_data[index] = scalar_t(0);\n      continue;\n    }\n\n    accscalar_t aveval = accscalar_t(0);\n    const scalar_t* const bottom_slice = bottom_data + n * channels * height * width + c;\n    for (int h = hstart; h < hend; ++h) {\n      for (int w = wstart; w < wend; ++w) {\n        aveval += bottom_slice[(h * width + w) * channels];\n      }\n    }\n    int divide_factor;\n    if (use_divisor) {\n      divide_factor = divisor_override;\n    } else {\n      if(count_include_pad) {\n        divide_factor = pool_size;\n      } else {\n        divide_factor = (hend - hstart) * (wend - wstart);\n      }\n    }\n    top_data[index] = static_cast<scalar_t>(aveval / divide_factor);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool2d_backward_out_cuda_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool2d_backward_out_cuda_frame(const index_t nthreads, const scalar_t* const top_diff,\n    const int64_t channels, const int64_t height,\n    const int64_t width, const int64_t pooled_height, const int64_t pooled_width,\n    const int kernel_h, const int kernel_w, const int stride_h,\n    const int stride_w, const int pad_h, const int pad_w,\n    scalar_t* const bottom_diff, const int divisor_override,\n    bool count_include_pad, bool use_divisor) {\n  CUDA_KERNEL_LOOP_TYPE(index, nthreads, index_t) {\n    // find out the local index\n    // find out the local offset\n    const int w = index % width + pad_w;\n    const int h = (index / width) % height + pad_h;\n    const int c = (index / width / height) % channels;\n    const int n = index / width / height / channels;\n    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int phend = min(h / stride_h + 1, pooled_height);\n    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int pwend = min(w / stride_w + 1, pooled_width);\n    accscalar_t gradient = accscalar_t(0);\n    const scalar_t* const top_diff_slice =\n        top_diff + (n * channels + c) * pooled_height * pooled_width;\n    for (int ph = phstart; ph < phend; ++ph) {\n      for (int pw = pwstart; pw < pwend; ++pw) {\n        // figure out the pooling size\n        int hstart = ph * stride_h - pad_h;\n        int wstart = pw * stride_w - pad_w;\n        int hend = min(hstart + kernel_h, height + pad_h);\n        int wend = min(wstart + kernel_w, width + pad_w);\n        int pool_size = (hend - hstart) * (wend - wstart);\n        hstart = max(hstart, 0);\n        wstart = max(wstart, 0);\n        hend = min(hend, height);\n        wend = min(wend, width);\n\n        if (hstart >= hend || wstart >= wend) {\n          continue;\n        }\n\n        int divide_factor;\n        if (use_divisor) {\n          divide_factor = divisor_override;\n        } else {\n          if(count_include_pad) {\n            divide_factor = pool_size;\n          } else {\n            divide_factor = (hend - hstart) * (wend - wstart);\n          }\n        }\n        gradient += top_diff_slice[ph * pooled_width + pw] / divide_factor;\n      }\n    }\n    bottom_diff[index] = static_cast<scalar_t>(gradient);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool2d_backward_out_cuda_frame_nhwc",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool2d_backward_out_cuda_frame_nhwc(const index_t nthreads,\n    const scalar_t* const top_diff,\n    const int64_t channels, const int64_t height,\n    const int64_t width, const int pooled_height, const int pooled_width,\n    const int kernel_h, const int kernel_w, const int stride_h,\n    const int stride_w, const int pad_h, const int pad_w,\n    scalar_t* const bottom_diff, const int divisor_override,\n    bool count_include_pad, bool use_divisor) {\n  CUDA_KERNEL_LOOP_TYPE(index, nthreads, index_t) {\n    const int c = index % channels;\n    const int w = (index / channels) % width;\n    const int h = (index / channels / width) % height;\n    const int n = index / channels / width / height;\n\n    const int phstart = (h < kernel_h) ? 0 : (h - kernel_h) / stride_h + 1;\n    const int phend = min(h / stride_h + 1, pooled_height);\n    const int pwstart = (w < kernel_w) ? 0 : (w - kernel_w) / stride_w + 1;\n    const int pwend = min(w / stride_w + 1, pooled_width);\n    accscalar_t gradient = accscalar_t(0);\n    const scalar_t* const top_diff_slice = top_diff + n * channels * pooled_height * pooled_width + c;\n    for (int ph = phstart; ph < phend; ++ph) {\n      for (int pw = pwstart; pw < pwend; ++pw) {\n        // figure out the pooling size\n        int hstart = ph * stride_h - pad_h;\n        int wstart = pw * stride_w - pad_w;\n        int hend = min(hstart + kernel_h, height + pad_h);\n        int wend = min(wstart + kernel_w, width + pad_w);\n        int pool_size = (hend - hstart) * (wend - wstart);\n        hstart = max(hstart, 0);\n        wstart = max(wstart, 0);\n        hend = min(hend, height);\n        wend = min(wend, width);\n\n        if (hstart >= hend || wstart >= wend) {\n          continue;\n        }\n\n        int divide_factor;\n        if (use_divisor) {\n          divide_factor = divisor_override;\n        } else {\n          if(count_include_pad) {\n            divide_factor = pool_size;\n          } else {\n            divide_factor = (hend - hstart) * (wend - wstart);\n          }\n        }\n        gradient += top_diff_slice[(ph * pooled_width + pw) * channels] / divide_factor;\n      }\n    }\n    bottom_diff[index] = static_cast<scalar_t>(gradient);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool3d_cuda_update_output",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool3d_cuda_update_output(\n  PackedTensorAccessor64<const scalar_t, 4> input,\n  PackedTensorAccessor64<scalar_t, 4> output,\n  int kT, int kH, int kW,\n  int dT, int dH, int dW,\n  int padT, int padH, int padW,\n  bool count_include_pad,\n  int offsetZ, int divisor_override)\n{\n  int oCol   = blockIdx.x * blockDim.x + threadIdx.x;\n  int oRow   = blockIdx.y * blockDim.y + threadIdx.y;\n  int oFrame = (blockIdx.z + offsetZ) % output.size(1); // output frame/time\n  int slice  = (blockIdx.z + offsetZ) / output.size(1); // output slice/feature\n\n  if (oRow < output.size(2) && oCol < output.size(3))\n  {\n    accscalar_t sum = 0.0;\n\n    int tstart = oFrame * dT - padT;\n    int hstart = oRow   * dH - padH;\n    int wstart = oCol   * dW - padW;\n    int tend = min(tstart + kT, input.size(1) + padT);\n    int hend = min(hstart + kH, input.size(2) + padH);\n    int wend = min(wstart + kW, input.size(3) + padW);\n    int pool_size = (tend - tstart) * (hend - hstart) * (wend - wstart);\n    tstart = max(tstart, 0);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    tend = min(tend, input.size(1));\n    hend = min(hend, input.size(2));\n    wend = min(wend, input.size(3));\n\n    if (tstart >= tend || hstart >= hend || wstart >= wend) {\n      output[slice][oFrame][oRow][oCol] = scalar_t(0);\n      return;\n    }\n\n    accscalar_t divide_factor;\n    if (divisor_override) {\n      divide_factor = static_cast<accscalar_t>(divisor_override);\n    } else {\n      if(count_include_pad) {\n        divide_factor = static_cast<accscalar_t>(pool_size);\n      } else {\n        divide_factor = static_cast<accscalar_t>((tend - tstart) * (hend - hstart) * (wend - wstart));\n      }\n    }\n\n    int ti, hi, wi;\n    for (ti = tstart; ti < tend; ++ti)\n    {\n      for (hi = hstart; hi < hend; ++hi)\n      {\n        for (wi = wstart; wi < wend; ++wi)\n        {\n          const scalar_t val = input[slice][ti][hi][wi];\n          sum += val;\n        }\n      }\n    }\n\n    output[slice][oFrame][oRow][oCol] = static_cast<scalar_t>(sum / divide_factor);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool3d_cuda_update_output",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool3d_cuda_update_output(\n  PackedTensorAccessor64<const scalar_t, 4> input,\n  PackedTensorAccessor64<scalar_t, 4> output,\n  int kT, int kH,\n  int dT, int dH, int dW,\n  int padT, int padH, int padW,\n  bool count_include_pad,\n  int offsetZ, int divisor_override)\n{\n  int oCol   = blockIdx.x * blockDim.x + threadIdx.x;\n  int oRow   = blockIdx.y * blockDim.y + threadIdx.y;\n  int oFrame = (blockIdx.z + offsetZ) % output.size(1); // output frame/time\n  int slice  = (blockIdx.z + offsetZ) / output.size(1); // output slice/feature\n\n  if (oRow < output.size(2) && oCol < output.size(3))\n  {\n    accscalar_t sum = 0.0;\n\n    int tstart = oFrame * dT - padT;\n    int hstart = oRow   * dH - padH;\n    int wstart = oCol   * dW - padW;\n    int tend = min(tstart + kT, input.size(1) + padT);\n    int hend = min(hstart + kH, input.size(2) + padH);\n    int wend = min(wstart + KERNEL_WIDTH, input.size(3) + padW);\n    int pool_size = (tend - tstart) * (hend - hstart) * (wend - wstart);\n    tstart = max(tstart, 0);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    tend = min(tend, input.size(1));\n    hend = min(hend, input.size(2));\n    wend = min(wend, input.size(3));\n\n    if (tstart >= tend || hstart >= hend || wstart >= wend) {\n      output[slice][oFrame][oRow][oCol] = scalar_t(0);\n      return;\n    }\n\n    accscalar_t divide_factor;\n    if (divisor_override) {\n      divide_factor = static_cast<accscalar_t>(divisor_override);\n    } else {\n      if(count_include_pad) {\n        divide_factor = static_cast<accscalar_t>(pool_size);\n      } else {\n        divide_factor = static_cast<accscalar_t>((tend - tstart) * (hend - hstart) * (wend - wstart));\n      }\n    }\n\n    int ti, hi, wi;\n    for (ti = tstart; ti < tend; ++ti)\n    {\n      for (hi = hstart; hi < hend; ++hi)\n      {\n        for (wi = wstart; wi < wend; ++wi)\n        {\n          const scalar_t val = input[slice][ti][hi][wi];\n          sum += val;\n        }\n      }\n    }\n\n    output[slice][oFrame][oRow][oCol] = static_cast<scalar_t>(sum / divide_factor);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool3d_single_backward_out_frame_stride1",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool3d_single_backward_out_frame_stride1(\n  PackedTensorAccessor64<const scalar_t, 4> gradOutput,\n  PackedTensorAccessor64<scalar_t, 4> gradInput,\n  int kT, int kH, int kW,\n  accscalar_t normFactor,\n  int offsetZ)\n{\n  int iCol   = blockIdx.x * blockDim.x + threadIdx.x;\n  int iRow   = blockIdx.y * blockDim.y + threadIdx.y;\n  int iFrame = (blockIdx.z + offsetZ) % gradInput.size(1); // input frame/time\n  int slice  = (blockIdx.z + offsetZ) / gradInput.size(1); // input slice/feature\n\n  // guard against over-tiled threads\n  if (iRow < gradInput.size(2) && iCol < gradInput.size(3))\n  {\n    accscalar_t sum = 0.0;\n    const scalar_t *gOut = &gradOutput[slice][max(0, iFrame - kT + 1)]\n      [max(0, iRow - kH + 1)][max(0, iCol - kW + 1)];\n    int frameOffset = 0;\n    for (int oFrame  = max(0, iFrame - kT + 1);\n         oFrame < min(iFrame + 1, gradOutput.size(1));\n         ++oFrame)\n    {\n      int rowOffset = frameOffset;\n      for (int oRow = max(0, iRow - kH + 1);\n           oRow < min(iRow + 1, gradOutput.size(2));\n           ++oRow)\n      {\n        int colOffset = rowOffset;\n        for (int oCol = max(0, iCol - kW + 1);\n             oCol < min(iCol + 1, gradOutput.size(3));\n             ++oCol)\n        {\n          sum += gOut[colOffset];\n          ++colOffset;\n        }\n        rowOffset += gradOutput.size(3);\n      }\n      frameOffset += gradOutput.size(2) * gradOutput.size(3);\n    }\n    gradInput[slice][iFrame][iRow][iCol] = static_cast<scalar_t>(sum * normFactor);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool3d_cuda_update_grad_input_atomic",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool3d_cuda_update_grad_input_atomic(\n  PackedTensorAccessor64<const scalar_t, 4> gradOutput,\n  PackedTensorAccessor64<scalar_t, 4> gradInput,\n  int kT, int kH, int kW,\n  int dT, int dH, int dW,\n  int padT, int padH, int padW,\n  bool count_include_pad,\n  int offsetZ, int divisor_override, const int gradInput_numel)\n{\n  int oCol   = blockIdx.x * blockDim.x + threadIdx.x;\n  int oRow   = blockIdx.y * blockDim.y + threadIdx.y;\n  int oFrame = (blockIdx.z + offsetZ) % gradOutput.size(1); // gradOutput frame/time\n  int slice  = (blockIdx.z + offsetZ) / gradOutput.size(1); // gradOutput slice/feature\n\n  // guard against over-tiled threads\n  if (oRow < gradOutput.size(2) && oCol < gradOutput.size(3))\n  {\n    int tstart = oFrame * dT - padT;\n    int hstart = oRow   * dH - padH;\n    int wstart = oCol   * dW - padW;\n    int tend = min(tstart + kT, gradInput.size(1) + padT);\n    int hend = min(hstart + kH, gradInput.size(2) + padH);\n    int wend = min(wstart + kW, gradInput.size(3) + padW);\n    int pool_size = (tend - tstart) * (hend - hstart) * (wend - wstart);\n    tstart = max(tstart, 0);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    tend = min(tend, gradInput.size(1));\n    hend = min(hend, gradInput.size(2));\n    wend = min(wend, gradInput.size(3));\n\n    accscalar_t divide_factor;\n    if (divisor_override) {\n      divide_factor = static_cast<accscalar_t>(divisor_override);\n    } else {\n      if(count_include_pad) {\n        divide_factor = static_cast<accscalar_t>(pool_size);\n      } else {\n        divide_factor = static_cast<accscalar_t>((tend - tstart) * (hend - hstart) * (wend - wstart));\n      }\n    }\n\n    scalar_t val = static_cast<scalar_t>(\n      static_cast<accscalar_t>(gradOutput[slice][oFrame][oRow][oCol]) / divide_factor);\n    for (int iFrame = tstart; iFrame < tend; ++iFrame)\n    {\n      for (int iRow = hstart; iRow < hend; ++iRow)\n      {\n        for (int iCol = wstart; iCol < wend; ++iCol)\n        {\n          const int index = slice * gradInput.stride(0) + iFrame * gradInput.stride(1) + iRow * gradInput.stride(2) + iCol * gradInput.stride(3);\n          fastAtomicAdd(gradInput.data(), index, gradInput_numel, val, true);\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "avg_pool3d_cuda_update_grad_input",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void avg_pool3d_cuda_update_grad_input(\n  PackedTensorAccessor64<const scalar_t, 4> gradOutput,\n  PackedTensorAccessor64<scalar_t, 4> gradInput,\n  int kT, int kH, int kW,\n  int dT, int dH, int dW,\n  int padT, int padH, int padW,\n  bool count_include_pad, int offsetZ, int divisor_override)\n{\n  int oCol   = blockIdx.x * blockDim.x + threadIdx.x;\n  int oRow   = blockIdx.y * blockDim.y + threadIdx.y;\n  int oFrame = (blockIdx.z + offsetZ) % gradOutput.size(1); // gradOutput frame/time\n  int slice  = (blockIdx.z + offsetZ) / gradOutput.size(1); // gradOutput slice/feature\n\n  // guard against over-tiled threads\n  if (oRow < gradOutput.size(2) && oCol < gradOutput.size(3))\n  {\n    int tstart = oFrame * dT - padT;\n    int hstart = oRow   * dH - padH;\n    int wstart = oCol   * dW - padW;\n    int tend = min(tstart + kT, gradInput.size(1) + padT);\n    int hend = min(hstart + kH, gradInput.size(2) + padH);\n    int wend = min(wstart + kW, gradInput.size(3) + padW);\n    int pool_size = (tend - tstart) * (hend - hstart) * (wend - wstart);\n    tstart = max(tstart, 0);\n    hstart = max(hstart, 0);\n    wstart = max(wstart, 0);\n    tend = min(tend, gradInput.size(1));\n    hend = min(hend, gradInput.size(2));\n    wend = min(wend, gradInput.size(3));\n\n    accscalar_t divide_factor;\n    if (divisor_override) {\n      divide_factor = static_cast<accscalar_t>(divisor_override);\n    } else {\n      if(count_include_pad) {\n        divide_factor = static_cast<accscalar_t>(pool_size);\n      } else {\n        divide_factor = static_cast<accscalar_t>((tend - tstart) * (hend - hstart) * (wend - wstart));\n      }\n    }\n\n    scalar_t val = static_cast<scalar_t>(\n      static_cast<accscalar_t>(gradOutput[slice][oFrame][oRow][oCol]) / divide_factor);\n    for (int iFrame = tstart; iFrame < tend; ++iFrame)\n    {\n      for (int iRow = hstart; iRow < hend; ++iRow)\n      {\n        for (int iCol = wstart; iCol < wend; ++iCol)\n        {\n          gradInput[slice][iFrame][iRow][iCol] = val;\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "triu_tril_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void triu_tril_kernel(\n    cuda::detail::TensorInfo<scalar_t, IndexType> result_info,\n    const cuda::detail::TensorInfo<const scalar_t, IndexType> self_info,\n    const int64_t k,\n    const int64_t N_padded,\n    const IndexType last_dim_padded) {\n  int64_t linear_idx = (blockIdx.x * blockDim.x + threadIdx.x) * elements_per_thread;\n  if (linear_idx >= N_padded) {\n    return;\n  }\n\n  auto dims = self_info.dims;\n\n  // Compute column index amd row index\n  IndexType col = linear_idx % last_dim_padded;\n  linear_idx /= last_dim_padded;\n  IndexType row = linear_idx % self_info.sizes[dims - 2];\n\n  if constexpr (inplace) {\n    bool mask_all_true = upper ? (col - row >= k) : (col + elements_per_thread - row <= k);\n    if (mask_all_true)\n      return;\n  }\n\n  // Compute offset\n  IndexType self_offset = 0, result_offset = 0;\n  self_offset += self_info.strides[dims - 1] * col;\n  result_offset += result_info.strides[dims - 1] * col;\n  linear_idx /= self_info.sizes[dims - 2];\n  self_offset += self_info.strides[dims - 2] * row;\n  result_offset += result_info.strides[dims - 2] * row;\n\n  // Compute remaining offsets\n  IndexType running_index;\n  #pragma unroll\n  for (IndexType i = dims - 3; i >= 0; --i) {\n    running_index = linear_idx % self_info.sizes[i];\n    linear_idx /= self_info.sizes[i];\n    self_offset += running_index * self_info.strides[i];\n    result_offset += running_index * result_info.strides[i];\n  }\n\n  if constexpr (inplace) {\n    #pragma unroll\n    for (int i = 0; i < elements_per_thread && col + i < self_info.sizes[dims - 1]; i++) {\n      bool mask = upper ? (col + i - row >= k) : (col + i - row <= k);\n      if (!mask)\n        result_info.data[result_offset + i * result_info.strides[dims - 1]] = scalar_t(0);\n    }\n  } else {\n    scalar_t frag[elements_per_thread] = {};\n    bool has_mask = (upper && col + elements_per_thread - row >= k) || (!upper && col - row <= k);\n    if (has_mask) {\n      #pragma unroll\n      for (int i = 0; i < elements_per_thread && col + i < self_info.sizes[dims - 1]; i++)\n        frag[i] = self_info.data[self_offset + i * self_info.strides[dims - 1]];\n\n      #pragma unroll\n      for (int i = 0; i < elements_per_thread; i++) {\n        bool mask = upper ? (col + i - row >= k) : (col + i - row <= k);\n        frag[i] = mask ? frag[i] : scalar_t(0);\n      }\n    }\n\n    #pragma unroll\n    for (int i = 0; i < elements_per_thread && col + i < self_info.sizes[dims - 1]; i++)\n      result_info.data[result_offset + i * result_info.strides[dims - 1]] = frag[i];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "rrelu_with_noise_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void rrelu_with_noise_cuda_kernel(\n    int numel,\n    PhiloxCudaState philox_args,\n    scalar_t* output,\n    const scalar_t* input,\n    scalar_t* noise,\n    double lower,\n    double upper,\n    const F& random_func) {\n  auto seeds = at::cuda::philox::unpack(philox_args);\n  int idx = blockIdx.x * blockDim.x + threadIdx.x;\n  curandStatePhilox4_32_10_t state;\n  curand_init(std::get<0>(seeds),\n              idx,\n              std::get<1>(seeds),\n              &state);\n\n  int grid_stride = blockDim.x * gridDim.x * unroll_factor;\n  int rounded_size = ((numel - 1) / grid_stride + 1) * grid_stride;\n  double range = upper - lower;\n\n  for (int linear_index = idx; linear_index < rounded_size; linear_index += grid_stride) {\n    auto rand = random_func(&state);\n\n    // ensure that (&rand.x)[ii] is safe\n    static_assert(sizeof(rand)/sizeof(rand.x) == unroll_factor, \"\");\n\n    #pragma unroll\n    for (int ii = 0; ii < unroll_factor; ii++) {\n      int li = linear_index + blockDim.x * gridDim.x * ii;\n      if (li >= numel) {\n        continue;\n      }\n      scalar_t r = static_cast<scalar_t>((&rand.x)[ii]);\n      r = r * range + lower;\n      if (input[li] <= 0) {\n        output[li] = input[li] * r;\n        noise[li] = r;\n      } else {\n        output[li] = input[li];\n        noise[li] = static_cast<scalar_t>(1);\n      }\n    }\n    __syncthreads();\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "apply_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void apply_kernel(int n, loop_t loop) {\n  constexpr int nv = nt * vt;\n  int idx = nv * blockIdx.x + threadIdx.x;\n\n  #pragma unroll\n  for (int i = 0; i < vt; ++i) {\n    if (idx < n) {\n      loop(idx);\n      idx += nt;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexing_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexing_backward_kernel(\n  const int64_t* sorted_indices, const int64_t* indices, const scalar_t* grad_output, scalar_t* grad_weight,\n  int64_t numel, int64_t stride, int64_t stride_before, int64_t outer_dim, bool accumulate) {\n  using opmath_t = at::opmath_type<scalar_t>;\n\n  extern __shared__ unsigned char smem[];\n  auto smem_dups_cache = reinterpret_cast<int64_t*>(smem);\n\n  int smem_offset = threadIdx.y * C10_WARP_SIZE;\n\n  int laneIdx = threadIdx.x % C10_WARP_SIZE;\n  int64_t grad_row = 0;\n\n  for (int64_t z = blockIdx.z; z < outer_dim; z += gridDim.z) {\n    // Init duplicates every time we compute a new set of entries:\n    smem_dups_cache[smem_offset + laneIdx] = 0;\n    WARP_SYNC();\n\n    int64_t base_idx = blockIdx.x * blockDim.y * C10_WARP_SIZE + threadIdx.y * C10_WARP_SIZE;\n    int64_t idx = base_idx + laneIdx;\n\n    if (idx < numel) {\n      int64_t crnt_sorted_idx = sorted_indices[idx];\n\n      if (idx == 0 || crnt_sorted_idx != sorted_indices[idx - 1]) {\n        // Determine the number of duplicates in advance:\n        int64_t num_duplicates = 1;\n\n        // Lookahead in case there is a large number of duplicates. Once that is done, handle the tail.\n        while ((idx + num_duplicates + SKIP_SORTED_INDICES - 1) < numel) {\n          if (sorted_indices[idx + num_duplicates + SKIP_SORTED_INDICES - 1] != crnt_sorted_idx) break;\n            num_duplicates += SKIP_SORTED_INDICES;\n        }\n        while (((idx + num_duplicates) < numel) && (sorted_indices[idx + num_duplicates] == crnt_sorted_idx)) {\n          num_duplicates++;\n        }\n\n        smem_dups_cache[smem_offset + laneIdx] = num_duplicates;\n      }\n    }\n\n    WARP_SYNC();\n\n    // All lanes in the warp are still active here. Use them all to reduce duplicates when\n    // large number of duplicates are present:\n    for (int subwarp = 0; subwarp < C10_WARP_SIZE; subwarp++) {\n      // All lanes read the shared memory entry for number of duplicates\n      int64_t new_num_duplicates = smem_dups_cache[smem_offset + subwarp];\n\n      // Check if the original sub-warp had duplicates to eliminate, if not skip.\n      if (new_num_duplicates == 0)\n        continue;\n\n      // There are duplicates that need eliminating:\n      int64_t new_idx = base_idx + subwarp;\n      int64_t new_crnt_sorted_idx = sorted_indices[new_idx];\n      const int64_t new_weight_row = new_crnt_sorted_idx * stride + z * stride_before;\n\n      if (!accumulate) {\n        const int64_t grad_row = ((int64_t)indices[new_idx + new_num_duplicates - 1]) * stride + z * numel * stride;\n        int64_t feature_dim = blockIdx.y * blockDim.x + threadIdx.x;\n        while (feature_dim < stride) {\n          grad_weight[new_weight_row + feature_dim] = grad_output[grad_row + feature_dim];\n          feature_dim += gridDim.y * blockDim.x;\n        }\n        continue;\n      }\n\n      for (int dup = 0; dup < new_num_duplicates; dup++) {\n        const int64_t grad_row = ((int64_t) indices[new_idx + dup]) * stride + z * numel * stride;\n\n        // All lanes do the same thing up to here.\n        int64_t feature_dim = blockIdx.y * blockDim.x + threadIdx.x;\n\n        // Each lane has a different feature_dim.\n        while (feature_dim < stride) {\n          grad_weight[new_weight_row + feature_dim] += grad_output[grad_row + feature_dim];\n          feature_dim += gridDim.y * blockDim.x;\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexing_backward_kernel_stride_1",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexing_backward_kernel_stride_1(\n  const int64_t* sorted_indices, const int64_t* indices, const scalar_t* grad_output, scalar_t* grad_weight,\n  int64_t numel, int64_t stride, int64_t stride_before, int64_t outer_dim, bool accumulate) {\n  using opmath_t = at::opmath_type<scalar_t>;\n\n  int laneIdx = threadIdx.x % C10_WARP_SIZE;\n\n  const opmath_t scale = (opmath_t)1.0;\n  int64_t grad_row = 0;\n\n  extern __shared__ unsigned char smem[];\n  auto smem_dups_cache = reinterpret_cast<int64_t*>(smem);\n\n  // Each warp gets a different section of the share memory allocation:\n  int smem_offset = threadIdx.y * C10_WARP_SIZE;\n\n  // Number of values processed by each thread (grain size)\n  for (int64_t z = blockIdx.z; z < outer_dim; z += gridDim.z) {\n    // Init duplicates every time we compute a new set of entries:\n    smem_dups_cache[smem_offset + laneIdx] = 0;\n\n    int64_t base_idx = blockIdx.x * blockDim.y * C10_WARP_SIZE + threadIdx.y * C10_WARP_SIZE;\n    int64_t idx = base_idx + laneIdx;\n\n    // Each lane calculates the number of duplicates:\n    if (idx < numel) {\n      int64_t crnt_sorted_idx = sorted_indices[idx];\n\n      if (idx == 0 || crnt_sorted_idx != sorted_indices[idx - 1]) {\n        // Determine the number of duplicates in advance:\n        int64_t num_duplicates = 1;\n\n        // Lookahead in case there is a large number of duplicates. Once that is done, handle the tail.\n        while ((idx + num_duplicates + SKIP_SORTED_INDICES - 1) < numel) {\n          if (sorted_indices[idx + num_duplicates + SKIP_SORTED_INDICES - 1] != crnt_sorted_idx) break;\n            num_duplicates += SKIP_SORTED_INDICES;\n        }\n        while (((idx + num_duplicates) < numel) && (sorted_indices[idx + num_duplicates] == crnt_sorted_idx)) {\n          num_duplicates++;\n        }\n\n        if (!accumulate) {\n          const int64_t weight_row = crnt_sorted_idx * stride + z * stride_before;\n          grad_row = ((int64_t)indices[idx + num_duplicates - 1]) * stride + z * numel * stride;\n          grad_weight[weight_row] =\n            static_cast<scalar_t>(static_cast<opmath_t>(grad_output[grad_row]) * scale);\n          continue;\n        }\n\n        // Each lane sequentially handles the duplicate elimination:\n        if (num_duplicates < C10_WARP_SIZE) {\n          opmath_t gradient = (opmath_t)0.0;\n          const int64_t weight_row = crnt_sorted_idx * stride + z * stride_before;\n          for (int64_t i = 0; i < num_duplicates; ++i) {\n            grad_row = ((int64_t) indices[idx + i]) * stride + z * numel * stride;\n            gradient += static_cast<opmath_t>(grad_output[grad_row]) * scale;\n          }\n\n          grad_weight[weight_row] = static_cast<scalar_t>(static_cast<opmath_t>(grad_weight[weight_row]) + gradient);\n        } else {\n          // Add duplicate to the cache:\n          smem_dups_cache[smem_offset + laneIdx] = num_duplicates;\n        }\n      }\n    }\n\n    WARP_SYNC();\n\n    // All lanes in the warp are still active here. Use them all to reduce duplicates when\n    // large number of duplicates are present:\n    for (int subwarp = 0; subwarp < C10_WARP_SIZE; subwarp++) {\n      // All lanes read the shared memory entry for number of duplicates\n      int64_t new_num_duplicates = smem_dups_cache[smem_offset + subwarp];\n\n      // Check if the original sub-warp had duplicates to eliminate, if not skip.\n      if (new_num_duplicates == 0)\n        continue;\n\n      // There are duplicates that need eliminating:\n      int64_t new_idx = base_idx + subwarp;\n      int64_t new_crnt_sorted_idx = sorted_indices[new_idx];\n      const int64_t new_weight_row = new_crnt_sorted_idx * stride + z * stride_before;\n\n      // Result of the reduction will be in this variable:\n      opmath_t gradient = (opmath_t)0.0;\n\n      int64_t num_warp_passes = new_num_duplicates / C10_WARP_SIZE;\n      // Parallel reduction across the array of duplicates using all the lanes in the warp:\n      for (int64_t i = 0; i < num_warp_passes; ++i) {\n        grad_row = ((int64_t) indices[new_idx + i * C10_WARP_SIZE + laneIdx]) * stride + z * numel * stride;\n        gradient += static_cast<opmath_t>(grad_output[grad_row]) * scale;\n      }\n\n      // Reduce across the lanes of the warp:\n      WARP_SYNC();\n      for (int offset = C10_WARP_SIZE / 2; offset > 0; offset /= 2) {\n        gradient += WARP_SHFL_DOWN(gradient, offset);\n      }\n\n      if (laneIdx == 0) {\n        for (int64_t i = num_warp_passes * C10_WARP_SIZE; i < new_num_duplicates; ++i) {\n          grad_row = ((int64_t) indices[new_idx + i]) * stride + z * numel * stride;\n          gradient += static_cast<opmath_t>(grad_output[grad_row]) * scale;\n        }\n\n        grad_weight[new_weight_row] = static_cast<scalar_t>(static_cast<opmath_t>(grad_weight[new_weight_row]) + gradient);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexing_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexing_backward_kernel(\n  const int64_t* sorted_indices, const int64_t* indices, const scalar_t* grad_output, scalar_t* grad_weight,\n  int64_t numel, int64_t stride, int64_t stride_before, int64_t outer_dim, bool accumulate) {\n//numel is total number of flattened indices, not expanded to dimensions that are not indexed.\n//stride is the cumulative size of the not-indexed last dimensions\n//stride_before is the stride of the dimension immediately preceding first indexed dimension\n//if indexing starts from the 0th dimension, stride_before does not matter because blockIdx.z will be 0 in this case\n//outer_dim is number of elements in the first unindexed dimensions\n  using opmath_t = at::opmath_type<scalar_t>;\n\n  // Each warp is responsible for an input into the LookupTable.\n  // If the preceding input has the same destination index as this input, then the warp\n  // exits immediately. The warp also processes subsequent inputs with the\n  // same value.\n  //\n  // Input Warp\n  // 1     <warp 1>\n  // 1     <warp 1> (<warp 2> exits without doing any work)\n  // 5     <warp 3>\n  // 8     <warp 4>\n\n  // Number of values processed by each thread (grain size)\n  for (int64_t z = blockIdx.z; z < outer_dim; z += gridDim.z){\n    int64_t idx = blockIdx.x * blockDim.y + threadIdx.y;\n    if (idx < numel\n        && (idx == 0 || sorted_indices[idx] != sorted_indices[idx - 1])){\n      do {\n        int64_t start_feature = threadIdx.x + blockIdx.y * blockDim.x * SZ;\n        // if not accumulate, we only keep the last duplicate index so skip those before it\n        if (!accumulate && (idx < numel - 1) && sorted_indices[idx] == sorted_indices[idx + 1]) {\n          idx++;\n          continue;\n        }\n        const int64_t weight_row = ((int64_t) sorted_indices[idx]) * stride + z * stride_before;\n        const int64_t grad_row = ((int64_t) indices[idx]) * stride + z * numel * stride;\n        const opmath_t scale = (opmath_t)1.0;\n\n        opmath_t gradient[SZ];\n        opmath_t weight[SZ];\n\n        while (start_feature < stride) {\n          #pragma unroll\n          for (int ii = 0; ii < SZ; ii++) {\n            int64_t feature_dim = start_feature + ii * C10_WARP_SIZE;\n            if (feature_dim < stride) {\n              gradient[ii] = static_cast<opmath_t>(grad_output[grad_row + feature_dim]);\n              if (accumulate) {\n                weight[ii] = static_cast<opmath_t>(grad_weight[weight_row + feature_dim]);\n              }\n            }\n          }\n\n          #pragma unroll\n          for (int ii = 0; ii < SZ; ii++) {\n            if (accumulate) {\n              weight[ii] += gradient[ii] * scale;\n            } else {\n              weight[ii] = gradient[ii] * scale;\n            }\n          }\n\n          #pragma unroll\n          for (int ii = 0; ii < SZ; ii++) {\n            int64_t feature_dim = start_feature + ii * C10_WARP_SIZE;\n            if (feature_dim < stride) {\n                grad_weight[weight_row + feature_dim] = static_cast<scalar_t>(weight[ii]);\n            }\n          }\n          start_feature += gridDim.y * blockDim.x * SZ;\n        }\n\n        idx++;\n      } while (idx < numel && sorted_indices[idx] == sorted_indices[idx - 1]);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexing_backward_kernel_stride_1",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexing_backward_kernel_stride_1(\n  const int64_t* sorted_indices, const int64_t* indices, const scalar_t* grad_output, scalar_t* grad_weight,\n  int64_t numel, int64_t stride, int64_t stride_before, int64_t outer_dim, bool accumulate) {\n  using opmath_t = at::opmath_type<scalar_t>;\n\n  // Number of values processed by each thread (grain size)\n  for (int64_t z = blockIdx.z; z < outer_dim; z += gridDim.z){\n    int64_t idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int64_t crnt_sorted_idx = sorted_indices[idx];\n\n    if ((idx < numel) &&\n        (idx == 0 || crnt_sorted_idx != sorted_indices[idx - 1]))\n    {\n      // Determine the number of duplicates in advance\n      int64_t num_duplicates = 1;\n      while (((idx + num_duplicates) < numel) && (sorted_indices[idx + num_duplicates] == crnt_sorted_idx)) {\n        num_duplicates++;\n      }\n\n      // Continue computing weights\n      const int64_t weight_row = crnt_sorted_idx * stride + z * stride_before;\n      int64_t grad_row = 0;\n      const opmath_t scale = (opmath_t)1.0;\n\n      if (!accumulate) {\n        grad_row = ((int64_t)indices[idx + num_duplicates - 1]) * stride + z * numel * stride;\n        grad_weight[weight_row] =\n          static_cast<scalar_t>(static_cast<opmath_t>(grad_output[grad_row]) * scale);\n      } else {\n        opmath_t gradient = (opmath_t)0.0;\n\n        int laneIdx = threadIdx.x % C10_WARP_SIZE;\n        int64_t num_warp_passes = num_duplicates / C10_WARP_SIZE;\n        for (int64_t i = 0; i < num_warp_passes; ++i) {\n            grad_row = ((int64_t) indices[idx + i * C10_WARP_SIZE + laneIdx]) * stride + z * numel * stride;\n            gradient += static_cast<opmath_t>(grad_output[grad_row]) * scale;\n        }\n        WARP_SYNC();\n        for (int offset = C10_WARP_SIZE / 2; offset > 0; offset /= 2) {\n          gradient += WARP_SHFL_DOWN(gradient, offset);\n        }\n\n        if (laneIdx == 0) {\n          for (int64_t i = num_warp_passes * C10_WARP_SIZE; i < num_duplicates; ++i) {\n            grad_row = ((int64_t) indices[idx + i]) * stride + z * numel * stride;\n            gradient += static_cast<opmath_t>(grad_output[grad_row]) * scale;\n          }\n\n          grad_weight[weight_row] = static_cast<scalar_t>(static_cast<opmath_t>(grad_weight[weight_row]) + gradient);\n        }\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexing_backward_kernel_small_stride",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexing_backward_kernel_small_stride(\n  const int64_t* sorted_indices, const int64_t* indices, const scalar_t* grad_output, scalar_t* grad_weight,\n  int64_t numel, int64_t stride, int64_t stride_before, int64_t outer_dim, bool accumulate) {\n  using opmath_t = at::opmath_type<scalar_t>;\n\n  // Number of values processed by each thread (grain size)\n  for (int64_t z = blockIdx.z; z < outer_dim; z += gridDim.z){\n    int64_t idx = blockIdx.x * blockDim.y + threadIdx.y;\n    int64_t tidx = threadIdx.x;\n    int64_t crnt_sorted_idx = sorted_indices[idx];\n\n    if ((idx < numel) &&\n        (tidx < stride) &&\n        (idx == 0 || crnt_sorted_idx != sorted_indices[idx - 1]))\n    {\n      // Determine the number of duplicates in advance\n      int64_t num_duplicates = 1;\n      while (((idx + num_duplicates) < numel) && (sorted_indices[idx + num_duplicates] == crnt_sorted_idx)) {\n        num_duplicates++;\n      }\n\n      // Continue computing weights\n      const int64_t weight_row = crnt_sorted_idx * stride + z * stride_before;\n      int64_t grad_row = 0;\n      const opmath_t scale = (opmath_t)1.0;\n\n      if (!accumulate) {\n        grad_row = ((int64_t)indices[idx + num_duplicates - 1]) * stride + z * numel * stride;\n        grad_weight[weight_row + tidx] =\n          static_cast<scalar_t>(static_cast<opmath_t>(grad_output[grad_row + tidx]) * scale);\n      } else {\n        opmath_t gradient = (opmath_t)0.0;\n        for (int64_t i = 0; i < num_duplicates; ++i) {\n          grad_row = ((int64_t) indices[idx + i]) * stride + z * numel * stride;\n          gradient += static_cast<opmath_t>(grad_output[grad_row + tidx]) * scale;\n        }\n\n        grad_weight[weight_row + tidx] = static_cast<scalar_t>(static_cast<opmath_t>(grad_weight[weight_row + tidx]) + gradient);\n      }\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexing_backward_kernel_quantized",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexing_backward_kernel_quantized(\n  const int64_t* sorted_indices, const int64_t* indices, const float* grad_output, scalar_t* grad_weight,\n  int64_t numel, int64_t stride, int64_t stride_before, int64_t outer_dim,\n  float inv_scale, int zero_point, int64_t qmin, int64_t qmax) {\n\n  // This implementation is adopted from indexing_backward_kernel above.\n  using opmath_t = at::opmath_type<float>;\n  for (int64_t z = blockIdx.z; z < outer_dim; z += gridDim.z){\n    int64_t idx = blockIdx.x * blockDim.y + threadIdx.y;\n    if (idx < numel\n        && (idx == 0 || sorted_indices[idx] != sorted_indices[idx - 1])){\n      do {\n        int64_t start_feature = threadIdx.x + blockIdx.y * blockDim.x * SZ;\n        // we only keep the last duplicate index so skip those before it\n        if ((idx < numel - 1) && sorted_indices[idx] == sorted_indices[idx + 1]) {\n          idx++;\n          continue;\n        }\n        const int64_t weight_row = ((int64_t) sorted_indices[idx]) * stride + z * stride_before;\n        const int64_t grad_row = ((int64_t) indices[idx]) * stride + z * numel * stride;\n        const opmath_t scale = (opmath_t)1.0;\n\n        opmath_t gradient[SZ];\n        opmath_t weight[SZ];\n\n        while (start_feature < stride) {\n          #pragma unroll\n          for (int ii = 0; ii < SZ; ii++) {\n            int64_t feature_dim = start_feature + ii * C10_WARP_SIZE;\n            if (feature_dim < stride) {\n              gradient[ii] = static_cast<opmath_t>(grad_output[grad_row + feature_dim]);\n            }\n          }\n\n          #pragma unroll\n          for (int ii = 0; ii < SZ; ii++) {\n            weight[ii] = gradient[ii] * scale;\n          }\n\n          #pragma unroll\n          for (int ii = 0; ii < SZ; ii++) {\n            int64_t feature_dim = start_feature + ii * C10_WARP_SIZE;\n            if (feature_dim < stride) {\n                // we do quantization here\n                int64_t qvalue = static_cast<int64_t>(zero_point + nearbyintf(weight[ii]* inv_scale));\n                qvalue = min(max(qvalue, qmin), qmax);\n                grad_weight[weight_row + feature_dim] = static_cast<scalar_t>(qvalue);\n            }\n          }\n          start_feature += gridDim.y * blockDim.x * SZ;\n        }\n\n        idx++;\n      } while (idx < numel && sorted_indices[idx] == sorted_indices[idx - 1]);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexFuncSmallIndex",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexFuncSmallIndex(cuda::detail::TensorInfo<T, IndexType> dst,\n                                    cuda::detail::TensorInfo<const T, IndexType> src,\n                                    cuda::detail::TensorInfo<const IndicesType, IndexType> indices,\n                                    int dstAddDim,\n                                    int srcAddDim,\n                                    IndexType innerSize,\n                                    int64_t dstAddDimSize,\n                                    int64_t dstNumel,\n                                    const func_t& op,\n                                    T alpha) {\n  // In order to avoid reloading the index that we are copying, load\n  // it once to handle all of the points that are being selected, so\n  // it can be reused as much as possible. This kernel is chosen when\n  // this is a good choice (small number of chosen indices), since\n  // re-accessing indices in addition to src elements can be slow.\n  for (IndexType srcIndex = 0; srcIndex < indices.sizes[0]; ++srcIndex) {\n    // Lua indices begin at 1\n    IndexType dstIndex =\n        indices.data[cuda::detail::IndexToOffset<const IndicesType, IndexType, IdxDim>::get(srcIndex, indices)];\n    CUDA_KERNEL_ASSERT(dstIndex < dstAddDimSize);\n\n    // We stride over the output ignoring the indexed dimension\n    // (innerSize), whose offset calculation is handled differently\n    for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n         linearIndex < innerSize;\n         linearIndex += gridDim.x * blockDim.x) {\n      IndexType dstOffset =\n          cuda::detail::IndexToOffset<T, IndexType, DstDim>::get(linearIndex, dst);\n      dstOffset += dstIndex * dst.strides[dstAddDim];\n\n      IndexType srcOffset =\n          cuda::detail::IndexToOffset<const T, IndexType, SrcDim>::get(linearIndex, src);\n      srcOffset += srcIndex * src.strides[srcAddDim];\n\n      T val = src.data[srcOffset] * alpha;\n      op(dst.data, dstOffset, dstNumel, &val);\n    }\n\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexFuncLargeIndex",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexFuncLargeIndex(cuda::detail::TensorInfo<T, IndexType> dst,\n                                    cuda::detail::TensorInfo<const T, IndexType> src,\n                                    cuda::detail::TensorInfo<const IndicesType, IndexType> indices,\n                                    int dstAddDim,\n                                    int srcAddDim,\n                                    IndexType totalSize,\n                                    IndexType innerSize,\n                                    int64_t dstAddDimSize,\n                                    int64_t dstNumel,\n                                    const func_t& op,\n                                    T alpha) {\n  // We stride over the output including the indexed dimension\n  // (totalSize), and calculate the destination index point based on that\n  for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n       linearIndex < totalSize;\n       linearIndex += gridDim.x * blockDim.x) {\n    IndexType srcIndex, elementInSlice;\n    if (IndexIsMajor) {\n      srcIndex = linearIndex / innerSize;\n      elementInSlice = linearIndex % innerSize;\n    }\n    else {\n      elementInSlice = linearIndex / innerSize;\n      srcIndex = linearIndex % innerSize;\n    }\n\n    // Lua indices begin at 1\n    IndexType dstIndex =\n        indices.data[cuda::detail::IndexToOffset<const IndicesType, IndexType, IdxDim>::get(srcIndex, indices)];\n    CUDA_KERNEL_ASSERT(dstIndex < dstAddDimSize);\n\n    IndexType dstOffset =\n      cuda::detail::IndexToOffset<T, IndexType, DstDim>::get(elementInSlice, dst);\n    dstOffset += dstIndex * dst.strides[dstAddDim];\n\n    IndexType srcOffset =\n      cuda::detail::IndexToOffset<const T, IndexType, SrcDim>::get(elementInSlice, src);\n    srcOffset += srcIndex * src.strides[srcAddDim];\n\n    T val = src.data[srcOffset] * alpha;\n    op(dst.data, dstOffset, dstNumel, &val);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexSelectSmallIndex",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexSelectSmallIndex(cuda::detail::TensorInfo<T, IndexType> dst,\n                                      cuda::detail::TensorInfo<const T, IndexType> src,\n                                      cuda::detail::TensorInfo<const IndicesType, IndexType> indices,\n                                      int dstSelectDim,\n                                      int srcSelectDim,\n                                      IndexType innerSize,\n                                      int64_t srcSelectDimSize) {\n  // In order to avoid reloading the index that we are copying, load\n  // it once to handle all of the points that are being selected, so\n  // it can be reused as much as possible. This kernel is chosen when\n  // this is a good choice (small number of chosen indices), since\n  // re-accessing indices in addition to src elements can be slow.\n  for (IndexType dstIndex = 0; dstIndex < indices.sizes[0]; ++dstIndex) {\n    IndexType srcIndex =\n      indices.data[cuda::detail::IndexToOffset<const IndicesType, IndexType, IdxDim>::get(dstIndex, indices)];\n    CUDA_KERNEL_ASSERT(srcIndex < srcSelectDimSize);\n\n    // We stride over the output ignoring the indexed dimension\n    // (innerSize), whose offset calculation is handled differently\n    for (IndexType linearIndex = blockIdx.x * blockDim.x + threadIdx.x;\n         linearIndex < innerSize;\n         linearIndex += gridDim.x * blockDim.x) {\n      IndexType dstOffset =\n        cuda::detail::IndexToOffset<T, IndexType, DstDim>::get(linearIndex, dst);\n      dstOffset += dstIndex * dst.strides[dstSelectDim];\n\n      IndexType srcOffset =\n        cuda::detail::IndexToOffset<const T, IndexType, SrcDim>::get(linearIndex, src);\n      srcOffset += srcIndex * src.strides[srcSelectDim];\n\n      dst.data[dstOffset] = src.data[srcOffset];\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "indexSelectLargeIndex",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void indexSelectLargeIndex(cuda::detail::TensorInfo<T, IndexType> dst,\n                                      cuda::detail::TensorInfo<const T, IndexType> src,\n                                      cuda::detail::TensorInfo<const IndicesType, IndexType> indices,\n                                      int dstSelectDim,\n                                      int srcSelectDim,\n                                      IndexType totalSize,\n                                      IndexType innerSize,\n                                      int64_t srcSelectDimSize) {\n  // We stride over the output including the indexed dimension\n  // (totalSize), and calculate the destination index point based on that\n  constexpr bool kPack4 = (sizeof(T) == 2);\n  constexpr int kElemsPerVec = kPack4 ? 4 : 1;\n\n  for (IndexType vecLinear = (blockIdx.x * blockDim.x + threadIdx.x) * kElemsPerVec;\n       vecLinear < totalSize;\n       vecLinear += gridDim.x * blockDim.x * kElemsPerVec) {\n\n    IndexType linearIndex = vecLinear;\n    IndexType dstIndex, elementInSlice;\n    if constexpr (IndexIsMajor) {\n      dstIndex = linearIndex / innerSize;\n      elementInSlice = linearIndex % innerSize;\n    }\n    else {\n      elementInSlice = linearIndex / innerSize;\n      dstIndex = linearIndex % innerSize;\n    }\n\n    IndexType srcIndex =\n      indices.data[cuda::detail::IndexToOffset<const IndicesType, IndexType, IdxDim>::get(dstIndex, indices)];\n    CUDA_KERNEL_ASSERT(srcIndex < srcSelectDimSize);\n\n    IndexType dstOffset =\n      cuda::detail::IndexToOffset<T, IndexType, DstDim>::get(elementInSlice, dst);\n    dstOffset += dstIndex * dst.strides[dstSelectDim];\n\n    IndexType srcOffset =\n      cuda::detail::IndexToOffset<const T, IndexType, SrcDim>::get(elementInSlice, src);\n    srcOffset += srcIndex * src.strides[srcSelectDim];\n    if constexpr (kPack4) {\n      IndexType srcNextOffset = cuda::detail::IndexToOffset<const T, IndexType, SrcDim>::get(elementInSlice + 1, src) + srcIndex * src.strides[srcSelectDim];\n      IndexType dstNextOffset = cuda::detail::IndexToOffset<T, IndexType, DstDim>::get(elementInSlice + 1, dst) + dstIndex * dst.strides[dstSelectDim];\n\n      bool inner_contiguous = (srcNextOffset - srcOffset == 1) && (dstNextOffset - dstOffset == 1);\n      bool slic_has_4 = (elementInSlice + 3 < innerSize);\n      bool aligned = (((uintptr_t)(dst.data + dstOffset) & 7)==0) && (((uintptr_t)(src.data + srcOffset) & 7)==0);\n\n      bool can_vectorize = IndexIsMajor && inner_contiguous && slic_has_4 && aligned;\n      #if defined(__CUDA_ARCH__)\n        bool warp_fast = __all_sync(0xffffffffu, can_vectorize);\n      #elif defined(__HIP_DEVICE_COMPILE__)\n        unsigned long long mask = __ballot(can_vectorize);\n        bool warp_fast = (mask == 0xffffffff);\n      #else\n        bool warp_fast = can_vectorize;\n      #endif\n\n      if (warp_fast) {\n        uint64_t tmp;\n        memcpy(&tmp, src.data + srcOffset, 8);\n        memcpy(dst.data + dstOffset, &tmp, 8);\n      } else {\n      #pragma unroll\n      for (int i = 0; i < kElemsPerVec; ++i) {\n        IndexType li = linearIndex + i;\n        if (li >= totalSize) break;\n\n        IndexType dstIndex2, elem2;\n        if constexpr (IndexIsMajor) {\n          dstIndex2 = li / innerSize;\n          elem2 = li % innerSize;\n        } else {\n          elem2 = li / innerSize;\n          dstIndex2 = li % innerSize;\n        }\n\n        IndexType srcIndex2 = indices.data[cuda::detail::IndexToOffset<const IndicesType, IndexType, IdxDim>::get(dstIndex2, indices)];\n        CUDA_KERNEL_ASSERT(srcIndex2 < srcSelectDimSize);\n\n        IndexType dstOffset2 = cuda::detail::IndexToOffset<T, IndexType, DstDim>::get(elem2, dst) + dstIndex2 * dst.strides[dstSelectDim];\n        IndexType srcOffset2 = cuda::detail::IndexToOffset<const T, IndexType, SrcDim>::get(elem2, src) + srcIndex2 * src.strides[srcSelectDim];\n        dst.data[dstOffset2] = src.data[srcOffset2];\n        }\n      }\n    }\n    else {\n      dst.data[dstOffset] = src.data[srcOffset];\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest1d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest1d_out_frame(\n    const scalar_t* input,\n    size_t dim_b,\n    size_t dim_c,\n    size_t src_dim_w,\n    size_t dst_dim_w,\n    scalar_t* output,\n    float scale_factor) {\n  int dst_idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (dst_idx >= dim_c * dst_dim_w)\n    return;\n\n  int c = (dst_idx / dst_dim_w) % dim_c;\n\n  int dst_x = dst_idx % dst_dim_w;\n  int src_x = nn_compute_source_index_fn(scale_factor, dst_x, src_dim_w);\n\n  int src_idx = c * src_dim_w + src_x;\n  int src_stride = dim_c * src_dim_w;\n  int dst_stride = dim_c * dst_dim_w;\n\n  for (int b = 0; b < dim_b; b++) {\n    output[dst_idx] = input[src_idx];\n    src_idx += src_stride;\n    dst_idx += dst_stride;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "upsample_nearest1d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void upsample_nearest1d_backward_out_frame(\n    const scalar_t* grad_o,\n    size_t dim_b,\n    size_t dim_c,\n    size_t src_dim_w,\n    size_t dst_dim_w,\n    scalar_t* grad_i,\n    float scale_factor) {\n\n  int dst_idx = blockIdx.x * blockDim.x + threadIdx.x;\n  if (dst_idx >= dim_c * dst_dim_w)\n    return;\n\n  int c = (dst_idx / (dst_dim_w)) % dim_c;\n\n  int dst_x = dst_idx % dst_dim_w;\n  // note that we do not want to clamp src_x to src_dim_w, since we might\n  // intentionally want to skip in case of scale_factor < 1.0\n  int src_x = nn_bw_compute_source_index_fn(scale_factor, dst_x, src_dim_w);\n  int src_x_up = nn_bw_compute_source_index_fn(scale_factor, dst_x+1, src_dim_w);\n\n  for (int b = 0; b < dim_b; b++) {\n    accscalar_t grad = 0;\n    int src_idx = b * dim_c * src_dim_w + c * src_dim_w + src_x;\n    for (int x = src_x; x < src_x_up; x++) {\n      grad += grad_o[src_idx++];\n    }\n    grad_i[dst_idx] = grad;\n    dst_idx += dim_c * dst_dim_w;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "fused_dropout_kernel_vec",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\nfused_dropout_kernel_vec(at::cuda::detail::TensorInfo<const scalar_t, IndexType> a,\n                         at::cuda::detail::TensorInfo<scalar_t, IndexType> b,\n                         at::cuda::detail::TensorInfo<mask_t, IndexType> c,\n                         IndexType totalElements, accscalar_t p,\n                         PhiloxCudaState philox_args) {\n  using LoadT = memory::aligned_vector<scalar_t, VEC>;\n  using MaskLoadT = memory::aligned_vector<mask_t, VEC>;\n\n  auto [seed, offset] = at::cuda::philox::unpack(philox_args);\n  IndexType idx = blockIdx.x * blockDim.x + threadIdx.x;\n  curandStatePhilox4_32_10_t state;\n  curand_init(seed, idx, offset, &state);\n\n  // Helps align the total number of times curand_uniform4 is called by each thread for the same totalElements\n  // in the vec=2 and vec=4 cases.\n  bool gridxvec_loop_state = 0;\n  accscalar_t scale = 1.0 / p;\n\n  constexpr int RAND_SIZE = (VEC + 4 - 1) / 4;\n  float4 rand[RAND_SIZE];\n\n  // Note: Vectorized loads means we'll stride each thread by an additional VEC factor, as we'll load VEC elements at a time\n  for (IndexType linearIndex = idx * VEC;\n      linearIndex < totalElements;\n      linearIndex += gridDim.x * blockDim.x * VEC) {\n    // local storage\n    scalar_t src[VEC];\n    // We'll use this to actually cause vectorized loads later\n    LoadT *value = reinterpret_cast<LoadT*>(&src);\n\n    //curand_uniform_double was pure evil anyway, not doing what it promises, and there's nothing for halfs, so generate float for everything\n    // Note: need a new set of random values per 4 elements -- we'll handle VEC elements in this thread, so need ceil(VEC / 4)\n    // sets of rand.\n    if ((VEC >= 4) || (gridxvec_loop_state == 0)) {\n      #pragma unroll\n      for (int ii = 0; ii < RAND_SIZE; ii++) {\n        rand[ii] = curand_uniform4(&state);\n      }\n    } else {\n      // sets up the last two values we generated last iteration to be used this iteration.\n      rand[0].x = rand[0].z;\n      rand[0].y = rand[0].w;\n      gridxvec_loop_state ^= 1;\n    }\n\n    rand[0].x = rand[0].x < p;\n    rand[0].y = rand[0].y < p;\n    if constexpr (VEC >= 4) {\n      rand[0].z = rand[0].z < p;\n      rand[0].w = rand[0].w < p;\n    }\n\n    #pragma unroll\n    for (int ii = 1; ii < RAND_SIZE; ii++) {\n      rand[ii].x = rand[ii].x < p;\n      rand[ii].y = rand[ii].y < p;\n      rand[ii].z = rand[ii].z < p;\n      rand[ii].w = rand[ii].w < p;\n    }\n\n    // Note: We explicitly check for is_contiguous() before launching the vectorized kernel\n    // and replace IndexToOffset call with linearIndex to allow vectorization of NHWC (or other)\n    // ordering.\n    // Single vectorized load\n    *value = *reinterpret_cast<const LoadT*>(&a.data[linearIndex]);\n\n    scalar_t r[VEC];\n    mask_t mask[VEC];\n\n    // Perform the actual computation\n    #pragma unroll\n    for (int jj = 0; jj < RAND_SIZE; jj++) {\n      #pragma unroll\n      for (int ii = 0; ii < std::min(VEC, 4); ii++) {\n        r[jj * 4 + ii] = src[jj * 4 + ii]*(&rand[jj].x)[ii]*scale;\n        mask[jj * 4 + ii] = (mask_t)(&rand[jj].x)[ii];\n      }\n    }\n\n    // Vectorized writes for both mask & result\n    *(reinterpret_cast<LoadT*>(&b.data[linearIndex])) = *reinterpret_cast<LoadT*>(&r[0]);\n    *(reinterpret_cast<MaskLoadT*>(&c.data[linearIndex])) = *reinterpret_cast<MaskLoadT*>(&mask[0]);\n\n    __syncthreads();\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "fused_dropout_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void\nfused_dropout_kernel(cuda::detail::TensorInfo<const scalar_t, IndexType> a,\n                     cuda::detail::TensorInfo<scalar_t, IndexType> b,\n                     cuda::detail::TensorInfo<mask_t, IndexType> c,\n                     IndexType totalElements, accscalar_t p,\n                     PhiloxCudaState philox_args) {\n  auto [seed, offset] = at::cuda::philox::unpack(philox_args);\n  IndexType idx = blockIdx.x * blockDim.x + threadIdx.x;\n  curandStatePhilox4_32_10_t state;\n  curand_init(seed, idx, offset, &state);\n  accscalar_t scale = 1.0 / p;\n\n  IndexType rounded_size = ((totalElements - 1)/(blockDim.x * gridDim.x * UNROLL)+1) *\n        blockDim.x * gridDim.x * UNROLL;\n  for (IndexType linearIndex = idx;\n       linearIndex < rounded_size;\n       linearIndex += gridDim.x * blockDim.x*UNROLL) {\n//curand_uniform_double was pure evil anyway, not doing what it promises, and there's nothing for halfs, so generate float for everything\n       float4 rand = curand_uniform4(&state);\n       scalar_t src[UNROLL];\n       rand.x = rand.x < p;\n       rand.y = rand.y < p;\n       rand.z = rand.z < p;\n       rand.w = rand.w < p;\n       for (int ii = 0; ii < UNROLL; ii++) {\n           IndexType li = linearIndex + blockDim.x * gridDim.x * ii;\n           if (li < totalElements) {\n    // Convert `linearIndex` into an offset of `a`\n               const IndexType aOffset =\n                   cuda::detail::IndexToOffset<const scalar_t, IndexType, ADims>::get(li, a);\n               src[ii] = a.data[aOffset];\n           }\n       }\n       for (int ii = 0; ii < UNROLL; ii++) {\n           IndexType li = linearIndex + blockDim.x * gridDim.x * ii;\n           if (li < totalElements) {\n    // Convert `linearIndex` into an offset of `b`\n               const IndexType bOffset =\n                   cuda::detail::IndexToOffset<scalar_t, IndexType, BDims>::get(li, b);\n               b.data[bOffset] = src[ii]*(&rand.x)[ii]*scale;\n               c.data[bOffset] = (mask_t)(&rand.x)[ii];\n           }\n       }\n       __syncthreads();\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "MultiMarginLoss_forward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void MultiMarginLoss_forward_kernel(\n    scalar_t *output, const scalar_t *input, const int64_t *target, const scalar_t *weights,\n    int nframe, int dim, bool sizeAverage, scalar_t margin) {\n  using acc_t = at::acc_type<scalar_t, true>;\n  __shared__ acc_t buffer[MULTIMARGIN_THREADS];\n  int k = blockIdx.x;\n  const scalar_t *input_k = input + k*dim;\n  scalar_t *output_k = output + k;\n  int target_k = static_cast<int>(target[k]);\n  CUDA_KERNEL_ASSERT(target_k >= 0 && target_k < dim && \"target index is out of bounds\");\n  scalar_t input_target_k = input_k[target_k];\n\n  int i_start = threadIdx.x;\n  int i_end = dim;\n  int i_step = blockDim.x;\n\n  buffer[threadIdx.x] = 0;\n  for (int i = i_start; i < i_end; i += i_step) {\n    scalar_t z = margin - input_target_k + input_k[i];\n    if (i == target_k) {\n      continue;\n    }\n\n    if (z > 0) {\n      scalar_t h = (P==1) ? z : z*z;\n      if (weights) {\n        h *= weights[target_k];\n      }\n      buffer[threadIdx.x] += h;\n    }\n  }\n  __syncthreads();\n\n  // reduce\n  if (threadIdx.x == 0) {\n    acc_t sum = 0;\n    for (int i=0; i < blockDim.x; i++)\n      sum += buffer[i];\n\n    const int denom = sizeAverage ? nframe * dim : dim;\n    *output_k = static_cast<scalar_t>(sum / denom);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "MultiMarginLoss_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void MultiMarginLoss_backward_kernel(\n    scalar_t *gradInput, const scalar_t *gradOutput, const scalar_t *input, const int64_t *target,\n    const scalar_t *weights, int nframe, int dim, bool sizeAverage, scalar_t margin,\n    bool reduce) {\n  using acc_t = at::acc_type<scalar_t, true>;\n  __shared__ acc_t buffer[MULTIMARGIN_THREADS];\n  int k = blockIdx.x;\n  const scalar_t *input_k = input + k*dim;\n  scalar_t *gradInput_k = gradInput + k*dim;\n  int target_k = static_cast<int>(target[k]);\n  scalar_t input_target_k = input_k[target_k];\n\n  const scalar_t *gradOutput_k = gradOutput;\n  if (!reduce) {\n    gradOutput_k += k;\n  }\n\n  const int denom = sizeAverage && reduce ? nframe * dim : dim;\n  const acc_t g = acc_t(1) / static_cast<acc_t>(denom);\n\n  int i_start = threadIdx.x;\n  int i_end = dim;\n  int i_step = blockDim.x;\n\n  buffer[threadIdx.x] = 0;\n  for (int i=i_start; i<i_end; i+=i_step) {\n    scalar_t z = margin - input_target_k + input_k[i];\n    if (i == target_k) {\n      continue;\n    }\n\n    if (z > 0) {\n      acc_t h = (P == 1) ? g : 2*g*z;\n      if (weights) {\n        h *= weights[target_k];\n      }\n\n      buffer[threadIdx.x] -= static_cast<scalar_t>(h);\n      gradInput_k[i] = static_cast<scalar_t>(h);\n    } else {\n      gradInput_k[i] = static_cast<scalar_t>(0);\n    }\n  }\n\n  __syncthreads();\n\n  // reduce\n  if (threadIdx.x == 0) {\n    acc_t gradInput_target_k = 0;\n    for (int i=0; i<blockDim.x; i++) {\n      gradInput_target_k += buffer[i];\n    }\n    gradInput_k[target_k] = static_cast<scalar_t>(gradInput_target_k);\n  }\n\n  for (int i=i_start; i<i_end; i+= i_step) {\n    gradInput_k[i] *= * gradOutput_k;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "weight_norm_fwd_first_dim_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void weight_norm_fwd_first_dim_kernel\n  (scalar_t* __restrict__ w,\n   accscalar_t* __restrict__ norms,\n   const scalar_t* __restrict__ v,\n   const scalar_t* __restrict__ g,\n   const int rowSize)\n{\n  // We are norming each slowest-dim row of the tensor separately.\n  // For now, assign one block to each row.\n  const int tid = threadIdx.x;\n  const int row = blockIdx.x;\n  const int stride = blockDim.x;\n\n  // Logical index offset for this flattened row\n  const int rowStart = row*rowSize;\n\n  // Hack to get around nvcc complaining when an smem array is declared with the same name\n  // but different types in different kernels (in this case different instantiations)\n  // extern __shared__ accscalar_t s[]; // error: declaration is incompatible with previous \"s\"\n  extern __shared__ char buf[];\n  accscalar_t* s = (accscalar_t*)buf;\n\n  accscalar_t thread_sum = 0.f;\n  for(int i = tid; i < rowSize; i += stride )\n  {\n    accscalar_t val_f = static_cast<accscalar_t>(v[i+rowStart]);\n    thread_sum += val_f*val_f; // AccumOp, could do Kahan here\n  }\n\n  reduce_block_into_lanes(s, thread_sum, 1, ReduceAdd<accscalar_t>());\n  accscalar_t result = s[0];\n\n  result = sqrtf(result);\n\n  if(tid == 0)\n    norms[row] = result;\n\n  // Broadcast load, could use shared memory instead.\n  accscalar_t g_this_row = static_cast<accscalar_t>(g[row]);\n\n  accscalar_t rnorm = 1.f/result; // for consistency with backward kernel\n\n  // Write data to output\n  for(int i = tid; i < rowSize; i += stride )\n  {\n    accscalar_t val_f = static_cast<accscalar_t>(v[i+rowStart]);\n    w[i+rowStart] = static_cast<scalar_t>(g_this_row*val_f*rnorm);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "weight_norm_fwd_last_dim_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void weight_norm_fwd_last_dim_kernel\n(\n  scalar_t* __restrict__ w,\n  accscalar_t* __restrict__ norms,\n  const scalar_t* __restrict__ v,\n  const scalar_t* __restrict__ g,\n  const int fast_dim_size,\n  const int slower_dims_size\n)\n{\n  const int fast_dim_location = threadIdx.x + blockIdx.x*blockDim.x;\n\n  extern __shared__ char buf[];\n  accscalar_t* alloc = (accscalar_t*)buf;\n  accscalar_t* s = &alloc[0];\n  accscalar_t* rnorms_this_block = &alloc[blockDim.x*blockDim.y];\n\n  accscalar_t thread_sum = 0.f;\n\n  int slower_dims_location = threadIdx.y;\n  int currentIdx = fast_dim_location + fast_dim_size*slower_dims_location;\n  if(fast_dim_location < fast_dim_size)\n    while(slower_dims_location < slower_dims_size)\n    {\n      accscalar_t val_f = static_cast<accscalar_t>(v[currentIdx]);\n      thread_sum += val_f*val_f; // AccumOp, could do Kahan here\n      currentIdx += blockDim.y*fast_dim_size;\n      slower_dims_location += blockDim.y;\n    }\n\n  reduce_block_into_lanes(s, thread_sum, blockDim.x, ReduceAdd<accscalar_t>());\n\n  // Better to pass an EpilogueOp to reduce_block_into_lanes?\n  if(threadIdx.y == 0)\n  {\n    accscalar_t result = s[threadIdx.x];\n    accscalar_t norm_this_col = sqrtf(result);\n    norms[fast_dim_location] = norm_this_col;\n    rnorms_this_block[threadIdx.x] = 1.f/norm_this_col;\n  }\n\n  __syncthreads();\n\n  accscalar_t g_this_col = static_cast<accscalar_t>(g[fast_dim_location]);\n  accscalar_t rnorm = rnorms_this_block[threadIdx.x];\n\n  slower_dims_location = threadIdx.y;\n  currentIdx = fast_dim_location + fast_dim_size*slower_dims_location;\n  if(fast_dim_location < fast_dim_size)\n    while(slower_dims_location < slower_dims_size)\n    {\n      accscalar_t val_f = static_cast<accscalar_t>(v[currentIdx]);\n      w[currentIdx] = static_cast<scalar_t>(g_this_col*val_f*rnorm);\n      currentIdx += blockDim.y*fast_dim_size;\n      slower_dims_location += blockDim.y;\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "weight_norm_bwd_first_dim_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void weight_norm_bwd_first_dim_kernel\n  (scalar_t* __restrict__ grad_v,\n   scalar_t* __restrict__ grad_g,\n   const scalar_t* __restrict__ grad_w,\n   const scalar_t* __restrict__ saved_v,\n   const scalar_t* __restrict__ saved_g,\n   const accscalar_t* __restrict__ saved_norms,\n   const int rowSize)\n{\n  // For now, assign one block to each row.\n  const int tid = threadIdx.x;\n  const int row = blockIdx.x;\n  const int stride = blockDim.x;\n\n  // Logical index offset for this flattened row\n  const int rowStart = row*rowSize;\n\n  // Hack to get around nvcc complaining when an smem array is declared with the same name\n  // but different types in different kernels (in this case different instantiations)\n  // extern __shared__ accscalar_t s[]; // error: declaration is incompatible with previous \"s\"\n  extern __shared__ char buf[];\n  accscalar_t* s = (accscalar_t*)buf;\n\n  accscalar_t thread_sum = 0.f;\n  for(int i = tid; i < rowSize; i += stride )\n  {\n    accscalar_t grad_wi = static_cast<accscalar_t>(grad_w[i+rowStart]);\n    accscalar_t saved_vi = static_cast<accscalar_t>(saved_v[i+rowStart]);\n    thread_sum += grad_wi*saved_vi; // AccumOp, could do Kahan here\n  }\n\n  reduce_block_into_lanes(s, thread_sum, 1, ReduceAdd<accscalar_t>());\n  accscalar_t result = s[0];\n\n  // Could choose to save reciprocal of norm instead I suppose, but norms is probably\n  // more handy to keep around.\n  // Broadcast load; could use shared memory instead.\n  accscalar_t rnorm = 1.f/saved_norms[row];\n  accscalar_t rnorm3 = rnorm*rnorm*rnorm;\n\n  // Write g gradients.\n  if(tid == 0)\n    grad_g[row] = static_cast<scalar_t>(result*rnorm);\n\n  // Broadcast load, could use shared memory instead.\n  accscalar_t g_this_row = static_cast<accscalar_t>(saved_g[row]);\n\n  // Write v gradients.  We are reusing values that were loaded earlier, so there\n  // is an optimization opportunity here (store values persistently).\n  for(int j = tid; j < rowSize; j += stride )\n  {\n    accscalar_t grad_wj = static_cast<accscalar_t>(grad_w[j+rowStart]);\n    accscalar_t saved_vj = static_cast<accscalar_t>(saved_v[j+rowStart]);\n    accscalar_t grad_vj = g_this_row*(rnorm*grad_wj - rnorm3*saved_vj*result);\n    grad_v[j+rowStart] = static_cast<scalar_t>(grad_vj);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "weight_norm_bwd_last_dim_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void weight_norm_bwd_last_dim_kernel\n  (scalar_t* __restrict__ grad_v,\n   scalar_t* __restrict__ grad_g,\n   const scalar_t* __restrict__ grad_w,\n   const scalar_t* __restrict__ saved_v,\n   const scalar_t* __restrict__ saved_g,\n   const accscalar_t* __restrict__ saved_norms,\n   const int fast_dim_size,\n   const int slower_dims_size)\n{\n  const int fast_dim_location = threadIdx.x + blockIdx.x*blockDim.x;\n\n  extern __shared__ char buf[];\n  accscalar_t* s = (accscalar_t*)buf;\n\n  accscalar_t thread_sum = 0.f;\n\n  int slower_dims_location = threadIdx.y;\n  int currentIdx = fast_dim_location + fast_dim_size*slower_dims_location;\n  if(fast_dim_location < fast_dim_size)\n    while(slower_dims_location < slower_dims_size)\n    {\n      accscalar_t grad_wi = static_cast<accscalar_t>(grad_w[currentIdx]);\n      accscalar_t saved_vi = static_cast<accscalar_t>(saved_v[currentIdx]);\n      thread_sum += grad_wi*saved_vi; // AccumOp, could do Kahan here\n      currentIdx += blockDim.y*fast_dim_size;\n      slower_dims_location += blockDim.y;\n    }\n\n  reduce_block_into_lanes(s, thread_sum, blockDim.x, ReduceAdd<accscalar_t>());\n  accscalar_t result = s[threadIdx.x];\n\n  // Broadcast load; could use shared memory instead.\n  accscalar_t rnorm = 1.f/saved_norms[fast_dim_location];\n  accscalar_t rnorm3 = rnorm*rnorm*rnorm;\n\n  // Write g gradients.\n  if(threadIdx.y == 0)\n    grad_g[fast_dim_location] = static_cast<scalar_t>(result*rnorm);\n\n  // Entire block pulls these values, could use shared memory instead.\n  accscalar_t g_this_col = static_cast<accscalar_t>(saved_g[fast_dim_location]);\n\n  // Write v gradients.\n  slower_dims_location = threadIdx.y;\n  currentIdx = fast_dim_location + fast_dim_size*slower_dims_location;\n  if(fast_dim_location < fast_dim_size)\n    while(slower_dims_location < slower_dims_size)\n    {\n      accscalar_t grad_wj = static_cast<accscalar_t>(grad_w[currentIdx]);\n      accscalar_t saved_vj = static_cast<accscalar_t>(saved_v[currentIdx]);\n      accscalar_t grad_vj = g_this_col*(rnorm*grad_wj - rnorm3*saved_vj*result);\n      grad_v[currentIdx] = static_cast<scalar_t>(grad_vj);\n      currentIdx += blockDim.y*fast_dim_size;\n      slower_dims_location += blockDim.y;\n    }\n}",
      "disabled": true
    },
    {
      "kernel_name": "EmbeddingBag_updateOutputKernel_max",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void EmbeddingBag_updateOutputKernel_max(\n    const index_t *input, const index_t *offsets, const scalar_t *weight, scalar_t *output,\n    index_t *offset2bag, int64_t numIndices, int64_t numBags,\n    int64_t featureSize, int64_t weight_stride0, int64_t weight_stride1,\n    index_t *bag_size, index_t *max_indices,\n    index_t padding_idx, int64_t numRows) {\n\n  // the strategy here is that each bag x feature is handled by a single thread\n\n  int64_t chunksPerBag = ceil_div(featureSize, (int64_t)blockDim.x);\n  int64_t numChunks = numBags * chunksPerBag;\n  int64_t chunkOffset = blockIdx.x * blockDim.y + threadIdx.y;\n  int64_t chunkStride = gridDim.x * blockDim.y;\n\n  for (int64_t chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n    int64_t featureDim = (chunk % chunksPerBag) * blockDim.x + threadIdx.x;\n    if (featureDim < featureSize) {\n      int64_t bag = chunk / chunksPerBag;\n      const scalar_t *weightFeat = weight + featureDim * weight_stride1;\n      int64_t begin = bag == 0 ? 0 : offsets[bag]; // forces first offset to be 0 instead of asserting on it\n      int64_t end = (bag < numBags - 1) ? (offsets[bag + 1]) : numIndices;\n      CUDA_KERNEL_ASSERT(end >= begin);\n      scalar_t weightFeatMax = 0;\n      int64_t bag_size_ = 0;\n      int64_t maxWord = -1;\n      for (int64_t emb = begin; emb < end; emb++) {\n        bool pad = (input[emb] == padding_idx);\n        CUDA_KERNEL_ASSERT(input[emb] < numRows);\n        const int64_t weightRow = input[emb] * weight_stride0;\n        scalar_t weightValue = weightFeat[weightRow];\n        if (bag_size_ == 0 || weightValue > weightFeatMax) {\n          weightFeatMax = pad ? weightFeatMax : weightValue;\n          maxWord = pad ? maxWord : input[emb];\n        }\n        bag_size_ += pad ? 0 : 1;\n\n        if (featureDim == 0) {\n          offset2bag[emb] = bag;\n        }\n      }\n      bag_size[bag] = bag_size_;\n      max_indices[bag * featureSize + featureDim] = maxWord;\n      output[bag * featureSize + featureDim] = weightFeatMax;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "EmbeddingBag_updateOutputKernel_sum_mean",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void EmbeddingBag_updateOutputKernel_sum_mean(\n    const index_t *input, const index_t *offsets, const scalar_t *weight, scalar_t *output,\n    index_t *offset2bag, int64_t numIndices, int64_t numBags,\n    int64_t featureSize, int64_t weight_stride0, int64_t weight_stride1,\n    int mode, index_t *bag_size,\n    const scalar_t* per_sample_weights, int64_t per_sample_weights_stride,\n    index_t padding_idx, int64_t numRows) {\n\n  // the strategy here is that each bag x feature is handled by a single thread\n\n  using accscalar_t = acc_type<scalar_t, true>;\n  int64_t chunksPerBag = ceil_div(featureSize, (int64_t)blockDim.x);\n  int64_t numChunks = numBags * chunksPerBag;\n  int64_t chunkOffset = blockIdx.x * blockDim.y + threadIdx.y;\n  int64_t chunkStride = gridDim.x * blockDim.y;\n\n  for (int64_t chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n    int64_t featureDim = (chunk % chunksPerBag) * blockDim.x + threadIdx.x;\n    if (featureDim < featureSize) {\n      int64_t bag = chunk / chunksPerBag;\n      const scalar_t *weightFeat = weight + featureDim * weight_stride1;\n      int64_t begin = bag == 0 ? 0 : offsets[bag]; // forces first offset to be 0 instead of asserting on it\n      int64_t end = (bag < numBags - 1) ? (offsets[bag + 1]) : numIndices;\n      CUDA_KERNEL_ASSERT(end >= begin);\n      accscalar_t weightFeatSum = 0;\n      int64_t bag_size_ = 0;\n      for (int64_t emb = begin; emb < end; emb++) {\n        index_t input_idx = input[emb];\n        bool pad = (input_idx == padding_idx);\n        CUDA_KERNEL_ASSERT(0 <= input_idx && input_idx < numRows);\n        const int64_t weightRow = input_idx * weight_stride0;\n        scalar_t weightValue = weightFeat[weightRow];\n        weightValue = pad ? static_cast<scalar_t>(0) : weightValue;\n        if (per_sample_weights) {\n          accscalar_t scaleWeightBy = static_cast<accscalar_t>(\n              per_sample_weights[emb * per_sample_weights_stride]);\n          weightFeatSum += scaleWeightBy * static_cast<accscalar_t>(weightValue);\n        } else {\n          weightFeatSum += static_cast<accscalar_t>(weightValue);\n        }\n        bag_size_ += pad ? 0 : 1;\n\n        if (featureDim == 0) {\n          offset2bag[emb] = bag;\n        }\n      }\n      if (mode == static_cast<int64_t>(EmbeddingBagMode::MEAN)) {\n        if (bag_size_ != 0) {\n          weightFeatSum = weightFeatSum / static_cast<accscalar_t>(bag_size_);\n        }\n      }\n      bag_size[bag] = bag_size_;\n      output[bag * featureSize + featureDim] = static_cast<scalar_t>(weightFeatSum);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "EmbeddingBag_accGradParametersKernel_max",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void EmbeddingBag_accGradParametersKernel_max(\n    const index_t *max_indices, const scalar_t *gradOutput,\n    scalar_t *gradWeight, int64_t stride, int64_t numBags,\n    index_t padding_idx, const index_t numel) {\n\n  using accscalar_t = acc_type<scalar_t, true>;\n\n  int64_t chunksPerBag = ceil_div(stride, (int64_t)blockDim.x);\n  int64_t numChunks = numBags * chunksPerBag;\n  int64_t chunkOffset = blockIdx.x * blockDim.y + threadIdx.y;\n  int64_t chunkStride = gridDim.x * blockDim.y;\n\n  for (int64_t chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n    int64_t featureDim = (chunk % chunksPerBag) * blockDim.x + threadIdx.x;\n    if (featureDim < stride) {\n      int64_t bag = chunk / chunksPerBag;\n\n      index_t word_idx = max_indices[bag * stride + featureDim];\n      if (word_idx >= 0 && word_idx != padding_idx) {\n        // If bag is empty, we have max_indices[idx] set to -1 in forward.\n        fastAtomicAdd(\n            gradWeight, static_cast<index_t>(word_idx * stride + featureDim),\n            numel, gradOutput[bag * stride + featureDim], true);\n      }\n    }\n  }\n}",
      "disabled": false
    },
    {
      "kernel_name": "fractional_max_pool3d_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fractional_max_pool3d_out_frame(\n  PackedTensorAccessor64<const scalar_t, 5> input,\n  PackedTensorAccessor64<scalar_t, 5> output,\n  PackedTensorAccessor64<int64_t, 5> indices,\n  PackedTensorAccessor64<const scalar_t, 3> samples,\n  int64_t poolSizeT, int64_t poolSizeH, int64_t poolSizeW) {\n    using accscalar_t = at::acc_type<scalar_t, /*is_cuda=*/true>;\n    // Output (t, h, w) point that this thread is responsible for\n    int64_t ourOutputPoint = threadIdx.x + ((int64_t) blockIdx.x) * blockDim.x;\n    int64_t plane = blockIdx.y;\n    int64_t batch = blockIdx.z;\n    // Each thread generates a specific output point\n    if (ourOutputPoint < output.size(2) * output.size(3) *\n      output.size(4)){\n      int64_t outputT = ourOutputPoint / (output.size(3) *\n                    output.size(4));\n      int64_t outputH = (ourOutputPoint / output.size(4)) %\n                    output.size(3);\n      int64_t outputW = ourOutputPoint % output.size(4);\n\n      int64_t poolT = get_intervals<scalar_t,accscalar_t>(\n        static_cast<accscalar_t>(samples[batch][plane][0]),\n        outputT, input.size(2), output.size(2), poolSizeT);\n      int64_t poolH = get_intervals<scalar_t, accscalar_t>(\n        static_cast<accscalar_t>(samples[batch][plane][1]),\n        outputH, input.size(3), output.size(3), poolSizeH);\n      int64_t poolW = get_intervals<scalar_t, accscalar_t>(\n        static_cast<accscalar_t>(samples[batch][plane][2]),\n        outputW, input.size(4), output.size(4), poolSizeW);\n\n      scalar_t maxVal = at::numeric_limits<scalar_t>::lower_bound();\n      int64_t maxIndex = poolT * input.size(3) * input.size(4) + poolH * input.size(4) + poolW;\n\n      for(int64_t t = poolT; t < poolT + poolSizeT; ++ t) {\n        for (int64_t h = poolH; h < poolH + poolSizeH; ++h) {\n          if(poolSizeW < 2 || poolSizeW > 7) {\n            for (int64_t w = poolW; w < poolW + poolSizeW; ++w) {\n              scalar_t val = input[batch][plane][t][h][w];\n              // for consistency with THNN, favor the first max\n              if (val > maxVal || at::_isnan(val)) {\n                maxIndex = t * input.size(3) *\n                  input.size(4) + h * input.size(4) + w;\n                maxVal = val;\n              }\n            }\n          } else {\n            for (int64_t i = 0; i < poolSizeW; ++i) {\n              int64_t w = i + poolW;\n              scalar_t val = input[batch][plane][t][h][w];\n              // for consistency with THNN, favor the first max\n              if (val > maxVal || at::_isnan(val)) {\n                maxIndex = t * input.size(3) * input.size(4) +\n                  h * input.size(4) + w;\n                maxVal = val;\n              }\n            }\n          }\n        }\n      }\n\n      indices[batch][plane][outputT][outputH][outputW] = maxIndex;\n      output[batch][plane][outputT][outputH][outputW] = maxVal;\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "fractional_max_pool3d_backward_out_frame",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void fractional_max_pool3d_backward_out_frame(\n  PackedTensorAccessor64<scalar_t, 5> gradInput,\n  PackedTensorAccessor64<const scalar_t, 5> gradOutput,\n  PackedTensorAccessor64<const int64_t, 5> indices) {\n  // Output (h, w) point that this thread is responsible for\n  int64_t ourOutputPoint = threadIdx.x + ((int64_t) blockIdx.x) * blockDim.x;\n  int64_t plane = blockIdx.y;\n  int64_t batch = blockIdx.z;\n\n  // Each thread generates a specific output point\n  if (ourOutputPoint < gradOutput.size(2) *\n    gradOutput.size(3) * gradOutput.size(4)) {\n    int64_t outputW = ourOutputPoint % gradOutput.size(4);\n    int64_t outputH = (ourOutputPoint / gradOutput.size(4)) %\n                      gradOutput.size(3);\n    int64_t outputT = ourOutputPoint / (gradOutput.size(3) *\n                      gradOutput.size(4));\n\n    int64_t index = indices[batch][plane][outputT][outputH][outputW];\n    CUDA_KERNEL_ASSERT(index >= 0);\n    int64_t inputW = index % gradInput.size(4);\n    int64_t inputH = (index / gradInput.size(4)) %\n      gradInput.size(3);\n    int64_t inputT = index / (gradInput.size(3) *\n      gradInput.size(4));\n    CUDA_KERNEL_ASSERT(inputT < gradInput.size(2));\n\n    gpuAtomicAddNoReturn(\n      &gradInput[batch][plane][inputT][inputH][inputW],\n      gradOutput[batch][plane][outputT][outputH][outputW]\n      );\n    }\n  }",
      "disabled": true
    },
    {
      "kernel_name": "multilabel_margin_loss_forward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void multilabel_margin_loss_forward_kernel(\n    scalar_t* output,\n    const scalar_t* input,\n    const int64_t* target,\n    scalar_t* is_target,\n    int nframe,\n    int dim,\n    bool size_average) {\n\n  // vectors:\n  int k = blockIdx.x;\n  const scalar_t* input_k = input + k * dim;\n  const int64_t* target_k = target + k * dim;\n  scalar_t* output_k = output + k;\n  scalar_t* is_target_k = is_target + k * dim;\n\n  // zero is_target\n  for (int d = threadIdx.x; d < dim; d += blockDim.x) {\n    is_target_k[d] = static_cast<scalar_t>(0);\n  }\n  __syncthreads();\n\n  // mark targets in is_target\n  if (threadIdx.x == 0) {\n    for (int dt = 0; dt < dim; dt++) {\n      int target_idx = target_k[dt];\n      if (target_idx < 0) {\n        break;\n      }\n      is_target_k[target_idx] = static_cast<scalar_t>(1);\n    }\n  }\n  __syncthreads();\n\n  // iterate over targets\n  accscalar_t sum = 0;\n  for (int dt = 0; dt < dim; dt++) {\n    // next target:\n    int target_idx = target_k[dt];\n    if (target_idx < 0) {\n      break;\n    }\n\n    // current value for target\n    scalar_t input_target_k = input_k[target_idx];\n\n    // compare to all inputs (multithreaded):\n    for (int d = threadIdx.x; d < dim; d += blockDim.x) {\n      // contribute to loss only if not a target\n      if (!static_cast<int>(is_target_k[d])) {\n        scalar_t z = 1 - input_target_k + input_k[d];\n        if (z > 0) {\n          sum += z;\n        }\n      }\n    }\n  }\n\n  // Temporary sums (for mapreduce)\n  __shared__ accscalar_t smem[MULTILABELMARGIN_THREADS];\n  accscalar_t total_sum = cuda_utils::BlockReduceSum(sum, smem);\n  if (threadIdx.x == 0) {\n    if (size_average) {\n      *output_k = static_cast<scalar_t>((total_sum / dim) / nframe);\n    } else {\n      *output_k = static_cast<scalar_t>(total_sum / dim);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "multilabel_margin_loss_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void multilabel_margin_loss_backward_kernel(\n    scalar_t* grad_input,\n    const scalar_t* grad_output,\n    const scalar_t* input,\n    const int64_t* target,\n    const scalar_t* is_target,\n    int nframe,\n    int dim,\n    bool size_average,\n    bool reduce) {\n\n  int k = blockIdx.x;\n  const scalar_t* input_k = input + k * dim;\n  scalar_t* grad_input_k = grad_input + k * dim;\n  const int64_t* target_k = target + k * dim;\n  const scalar_t* is_target_k = is_target + k * dim;\n\n  const scalar_t* grad_output_k = grad_output;\n  if (!reduce) {\n    grad_output_k += k;\n  }\n\n  // gain:\n  scalar_t g = static_cast<scalar_t>(\n      size_average && reduce ? 1. / static_cast<accscalar_t>(nframe * dim)\n                             : 1. / static_cast<accscalar_t>(dim));\n\n  // zero gradients:\n  for (int d = threadIdx.x; d < dim; d += blockDim.x) {\n    grad_input_k[d] = static_cast<scalar_t>(0);\n  }\n  __syncthreads();\n\n  // iterate over targets\n  for (int dt = 0; dt < dim; dt++) {\n    // next target:\n    int target_idx = static_cast<int>(target_k[dt]);\n    if (target_idx < 0) {\n      break;\n    }\n\n    // current value for target\n    scalar_t input_target_k = input_k[target_idx];\n\n    // compare to all inputs (multithreaded):\n    accscalar_t sum = 0;\n    for (int d = threadIdx.x; d < dim; d += blockDim.x) {\n      // contribute to loss only if not a target\n      if (!static_cast<int>(is_target_k[d])) {\n        scalar_t z = 1 - input_target_k + input_k[d];\n        if (z > 0) {\n          sum -= g;\n          grad_input_k[d] += g;\n        }\n      }\n    }\n    __syncthreads();\n\n    // Temporary sums (for mapreduce)\n    __shared__ accscalar_t smem[MULTILABELMARGIN_THREADS];\n    accscalar_t total_sum = cuda_utils::BlockReduceSum(sum, smem);\n    if (threadIdx.x == 0) {\n      grad_input_k[target_idx] += static_cast<scalar_t>(total_sum);\n    }\n  }\n\n  for (int d = threadIdx.x; d < dim; d += blockDim.x) {\n    grad_input_k[d] *= *grad_output_k;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss_forward_no_reduce_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss_forward_no_reduce_cuda_kernel(\n    int64_t batch_size,\n    PackedTensorAccessor64<scalar_t, 2> input,\n    const index_t* target,\n    scalar_t* output,\n    const scalar_t* weights,\n    int64_t n_classes,\n    int64_t ignore_index) {\n  CUDA_KERNEL_LOOP(index, batch_size) {\n    index_t cur_target = target[index];\n    if (cur_target == ignore_index) {\n      output[index] = static_cast<scalar_t>(0);\n      continue;\n    }\n    CHECK_INDEX_IN_CLASS(cur_target, n_classes);\n    auto cur_weight =\n        weights != nullptr ? weights[cur_target] : static_cast<scalar_t>(1);\n    output[index] = -cur_weight * input[index][cur_target];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss_forward_reduce_cuda_kernel_1d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss_forward_reduce_cuda_kernel_1d(\n    scalar_t* output,\n    scalar_t* total_weight,\n    const scalar_t* input,\n    const index_t* target,\n    const scalar_t* weights,\n    bool size_average,\n    int64_t n_classes,\n    int64_t ignore_index) {\n  CUDA_KERNEL_ASSERT(threadIdx.x == 0 && threadIdx.y == 0 && threadIdx.z == 0);\n\n  const index_t t = *target;\n  if (t != ignore_index) {\n    CHECK_INDEX_IN_CLASS(t, n_classes);\n    const auto cur_weight = weights != nullptr ? weights[t] : scalar_t{1};\n    *total_weight = cur_weight;\n\n    if (size_average) {\n      // If we try to normalize a zero then we return a NaN\n      if (cur_weight == 0) {\n        *output = std::numeric_limits<scalar_t>::quiet_NaN();\n      } else {\n        *output = -input[t];\n      }\n    } else {\n      *output = -cur_weight * input[t];\n    }\n  } else {\n    // If the only element was omitted, we get 0. See the discussion in\n    // https://github.com/pytorch/pytorch/pull/64572#issuecomment-926504162\n    *output = scalar_t{0};\n    *total_weight = scalar_t{0};\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss_forward_reduce_cuda_kernel_2d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss_forward_reduce_cuda_kernel_2d(\n    scalar_t* output,\n    scalar_t* total_weight,\n    const scalar_t* input,\n    const index_t* target,\n    const scalar_t* weights,\n    bool size_average,\n    int64_t nframe,\n    int64_t ndim,\n    int64_t n_classes,\n    int64_t ignore_index) {\n  // NOLINTNEXTLINE(cppcoreguidelines-init-variables)\n  extern __shared__ unsigned char shmem[];\n  accscalar_t* sh_inputs = reinterpret_cast<accscalar_t*>(shmem);\n  accscalar_t* acc_weight = reinterpret_cast<accscalar_t*>(shmem + blockDim.x * sizeof(accscalar_t));\n\n  sh_inputs[threadIdx.x] = static_cast<accscalar_t>(0);\n  acc_weight[threadIdx.x] = static_cast<accscalar_t>(0);\n  for (int i = threadIdx.x; i < nframe; i += blockDim.x) {\n    index_t t = target[i];\n    if (t != ignore_index) {\n      CHECK_INDEX_IN_CLASS(t, n_classes);\n      scalar_t cur_weight =\n          weights != nullptr ? weights[t] : static_cast<scalar_t>(1);\n      sh_inputs[threadIdx.x] -= input[i * ndim + t] * cur_weight;\n      acc_weight[threadIdx.x] += cur_weight;\n    }\n  }\n\n  __syncthreads();\n\n  for (int stride = blockDim.x/2; stride > 0; stride >>= 1) {\n    if (threadIdx.x < stride) {\n      sh_inputs[threadIdx.x] += sh_inputs[threadIdx.x + stride];\n      acc_weight[threadIdx.x] += acc_weight[threadIdx.x + stride];\n    }\n    __syncthreads();\n  }\n\n  if (threadIdx.x == 0) {\n    *total_weight = static_cast<scalar_t>(acc_weight[0]);\n    if (size_average) {\n      *output = static_cast<scalar_t>(sh_inputs[0] / acc_weight[0]);\n    } else {\n      *output = static_cast<scalar_t>(sh_inputs[0]);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss_backward_no_reduce_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss_backward_no_reduce_cuda_kernel(\n  int batch_size,\n  const index_t *target,\n  PackedTensorAccessor64<const scalar_t, 1> grad_output,\n  PackedTensorAccessor64<scalar_t, 2> grad_input,\n  const scalar_t *weights,\n  int64_t n_classes,\n  int64_t ignore_index) {\n\n  CUDA_KERNEL_LOOP(index, batch_size) {\n    index_t cur_target = target[index];\n    if (cur_target == ignore_index) {\n      continue;\n    }\n    CHECK_INDEX_IN_CLASS(cur_target, n_classes);\n    scalar_t weight = weights != nullptr ? weights[cur_target] : static_cast<scalar_t>(1);\n    grad_input[index][cur_target] = -weight * grad_output[index];\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss_backward_reduce_cuda_kernel_1d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss_backward_reduce_cuda_kernel_1d(\n  scalar_t *grad_input,\n  const scalar_t *grad_output,\n  const scalar_t *weights,\n  const index_t *target,\n  const scalar_t *total_weight,\n  bool size_average,\n  int64_t n_classes,\n  int64_t ignore_index\n) {\n  const index_t t = *target;\n  if (t != ignore_index) {\n    CHECK_INDEX_IN_CLASS(t, n_classes);\n    const auto grad = -(size_average ? *grad_output / *total_weight : *grad_output);\n    grad_input[t] = weights != nullptr ? weights[t] * grad : grad;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "nll_loss_backward_reduce_cuda_kernel_2d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void nll_loss_backward_reduce_cuda_kernel_2d(\n    scalar_t* grad_input,\n    const scalar_t* grad_output,\n    const index_t* target,\n    const scalar_t* weights,\n    const scalar_t* total_weight,\n    bool size_average,\n    int nframe,\n    int ndim,\n    int64_t n_classes,\n    int64_t ignore_index) {\n  using bwd_index_t = typename bwd_index_type<index_t>::type;\n  const auto grad = -(size_average ? *grad_output / *total_weight\n                                   : *grad_output);\n\n  for (int i = threadIdx.x; i < nframe; i += blockDim.x) {\n    const index_t t = target[i];\n    if (t != ignore_index) {\n      CHECK_INDEX_IN_CLASS(t, n_classes);\n      // NOTE(crcrpar): this index could overflow in int64_t as `t` itself can be close to the max.\n      const bwd_index_t index = static_cast<bwd_index_t>(i) * ndim + t;\n      if constexpr(!std::is_unsigned_v<decltype(index)>) {\n        CUDA_KERNEL_ASSERT(index >= 0);\n      }\n      grad_input[index] = weights != nullptr ? weights[t] * grad : grad;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "_assert_async_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _assert_async_cuda_kernel(const scalar_t* input, Msg msg) {\n  CUDA_KERNEL_ASSERT_MSG(input[0] != 0, msg.msg);\n}",
      "disabled": true
    },
    {
      "kernel_name": "_assert_async_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _assert_async_cuda_kernel(const c10::complex<float>* input, Msg msg) {\n  CUDA_KERNEL_ASSERT_MSG(input[0] != c10::complex<float>(0, 0), msg.msg);\n}",
      "disabled": true
    },
    {
      "kernel_name": "_assert_async_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _assert_async_cuda_kernel(const c10::complex<double>* input, Msg msg) {\n  CUDA_KERNEL_ASSERT_MSG(input[0] != c10::complex<double>(0, 0), msg.msg);\n}",
      "disabled": true
    },
    {
      "kernel_name": "_fft_conjugate_copy_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void _fft_conjugate_copy_kernel(\n    int64_t numel, scalar_t * out_data, const scalar_t * in_data,\n    inp_calc_t ic, out_calc_t oc) {\n  CUDA_KERNEL_LOOP_TYPE(index, numel, int64_t) {\n    auto in_offset = ic.get(index)[0];\n    auto out_offset = oc.get(index)[0];\n    out_data[out_offset] = std::conj(in_data[in_offset]);\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "amp_update_scale_cuda_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void amp_update_scale_cuda_kernel(float* current_scale,\n                                             int* growth_tracker,\n                                             const float* found_inf,\n                                             double growth_factor,\n                                             double backoff_factor,\n                                             int growth_interval)\n{\n  if (*found_inf) {\n    *current_scale = (*current_scale)*backoff_factor;\n    *growth_tracker = 0;\n  } else {\n    // Entering this branch means we just carried out a successful step,\n    // so growth_tracker is incremented before comparing to growth_interval.\n    auto successful = (*growth_tracker) + 1;\n    if (successful == growth_interval) {\n      auto new_scale = static_cast<float>((*current_scale)*growth_factor);\n      // Do not grow the scale past fp32 bounds to inf.\n      if (isfinite_ensure_cuda_math(new_scale)) {\n          *current_scale = new_scale;\n      }\n      *growth_tracker = 0;\n    } else {\n      *growth_tracker = successful;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "replication_pad_forward_kernel1d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void replication_pad_forward_kernel1d(\n    PackedTensorAccessor64<const scalar_t, 3> input,\n    PackedTensorAccessor64<scalar_t, 3> output,\n    const int padL,\n    const int y_shift,\n    const int z_shift) {\n  const int64_t outputPointId = threadIdx.x + ((int64_t) blockIdx.x) * blockDim.x;\n  const int64_t plane = blockIdx.y + y_shift;\n  const int64_t batch = blockIdx.z + z_shift;\n  if (outputPointId >= output.size(2)) {\n    return;\n  }\n  const auto outputPointX = outputPointId % output.size(2);\n\n  const int iStartX = imax(0, -padL);\n  const int oStartX = imax(0, padL);\n\n  const auto inputPointX = imin(imax(padL, outputPointX), input.size(2) + padL - 1) - oStartX + iStartX;\n\n  scalar_t valueToCopy = input[batch][plane][inputPointX];\n  output[batch][plane][outputPointX] = valueToCopy;\n}",
      "disabled": true
    },
    {
      "kernel_name": "replication_pad_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void replication_pad_backward_kernel(\n    PackedTensorAccessor64<scalar_t, 3> gradInput,\n    PackedTensorAccessor64<const scalar_t, 3> gradOutput,\n    const int padL,\n    const int y_shift,\n    const int z_shift) {\n  const int64_t outputPointId = threadIdx.x + ((int64_t) blockIdx.x) * blockDim.x;\n  const int64_t plane = blockIdx.y + y_shift;\n  const int64_t batch = blockIdx.z + z_shift;\n  if (outputPointId >= gradOutput.size(2)) {\n    return;\n  }\n  const auto outputPointX = outputPointId % gradOutput.size(2);\n\n  const int iStartX = imax(0, -padL);\n  const int oStartX = imax(0, padL);\n\n  const auto inputPointX = imin(imax(padL, outputPointX), gradInput.size(2) + padL - 1) - oStartX + iStartX;\n\n  scalar_t valueToCopy = gradOutput[batch][plane][outputPointX];\n  gpuAtomicAddNoReturn(&gradInput[batch][plane][inputPointX], valueToCopy);\n}",
      "disabled": true
    },
    {
      "kernel_name": "replication_pad_forward_kernel2d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void replication_pad_forward_kernel2d(\n    PackedTensorAccessor64<const scalar_t, 4> input,\n    PackedTensorAccessor64<scalar_t, 4> output,\n    const int padT,\n    const int padL,\n    const int y_shift,\n    const int z_shift) {\n  const int outputPointId = threadIdx.x + blockIdx.x * blockDim.x;\n  const int plane = blockIdx.y + y_shift;\n  const int batch = blockIdx.z + z_shift;\n  if (outputPointId >= output.size(2) * output.size(3)) {\n    return;\n  }\n  const int outputPointX = outputPointId % output.size(3);\n  const int outputPointY = outputPointId / output.size(3);\n\n  const int iStartX = imax(0, -padL);\n  const int iStartY = imax(0, -padT);\n  const int oStartX = imax(0, padL);\n  const int oStartY = imax(0, padT);\n\n  const int inputPointX = imin(imax(padL, outputPointX), input.size(3) + padL - 1) - oStartX + iStartX;\n  const int inputPointY = imin(imax(padT, outputPointY), input.size(2) + padT - 1) - oStartY + iStartY;\n\n  scalar_t valueToCopy = input[batch][plane][inputPointY][inputPointX];\n  output[batch][plane][outputPointY][outputPointX] = valueToCopy;\n}",
      "disabled": true
    },
    {
      "kernel_name": "replication_pad_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void replication_pad_backward_kernel(\n    PackedTensorAccessor64<scalar_t, 4> gradInput,\n    PackedTensorAccessor64<const scalar_t, 4> gradOutput,\n    const int padT,\n    const int padL,\n    const int y_shift,\n    const int z_shift) {\n  const int outputPointId = threadIdx.x + blockIdx.x * blockDim.x;\n  const int plane = blockIdx.y + y_shift;\n  const int batch = blockIdx.z + z_shift;\n  if (outputPointId >= gradOutput.size(2) * gradOutput.size(3)) {\n    return;\n  }\n  const int outputPointX = outputPointId % gradOutput.size(3);\n  const int outputPointY = outputPointId / gradOutput.size(3);\n\n  const int iStartX = imax(0, -padL);\n  const int iStartY = imax(0, -padT);\n  const int oStartX = imax(0, padL);\n  const int oStartY = imax(0, padT);\n\n  const int inputPointX = imin(imax(padL, outputPointX), gradInput.size(3) + padL - 1) - oStartX + iStartX;\n  const int inputPointY = imin(imax(padT, outputPointY), gradInput.size(2) + padT - 1) - oStartY + iStartY;\n\n  scalar_t valueToCopy = gradOutput[batch][plane][outputPointY][outputPointX];\n  gpuAtomicAddNoReturn(&gradInput[batch][plane][inputPointY][inputPointX], valueToCopy);\n}",
      "disabled": true
    },
    {
      "kernel_name": "replication_pad_forward_kernel3d",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void replication_pad_forward_kernel3d(\n    PackedTensorAccessor64<const scalar_t, 5> input,\n    PackedTensorAccessor64<scalar_t, 5> output,\n    const int pfront,\n    const int ptop,\n    const int pleft,\n    const int y_shift,\n    const int z_shift) {\n  const int outputPointId = threadIdx.x + blockIdx.x * blockDim.x;\n  const int plane = blockIdx.y + y_shift;\n  const int batch = blockIdx.z + z_shift;\n  if (outputPointId >= (output.size(2) * output.size(3) *\n        output.size(4))) {\n    return;\n  }\n  const int outputPointX = outputPointId % output.size(4);\n  const int outputPointY = (outputPointId / output.size(4)) % output.size(3);\n  const int outputPointZ = outputPointId / (output.size(3) * output.size(4));\n\n  const int iStartX = imax(0, -pleft);\n  const int iStartY = imax(0, -ptop);\n  const int iStartZ = imax(0, -pfront);\n  const int oStartX = imax(0, pleft);\n  const int oStartY = imax(0, ptop);\n  const int oStartZ = imax(0, pfront);\n\n  const int inputPointX = imin(imax(pleft, outputPointX),\n      input.size(4) + pleft - 1) - oStartX + iStartX;\n  const int inputPointY = imin(imax(ptop, outputPointY),\n      input.size(3) + ptop - 1) - oStartY + iStartY;\n  const int inputPointZ = imin(imax(pfront, outputPointZ),\n      input.size(2) + pfront - 1) - oStartZ + iStartZ;\n\n  scalar_t valueToCopy =\n    input[batch][plane][inputPointZ][inputPointY][inputPointX];\n  output[batch][plane][outputPointZ][outputPointY][outputPointX] = valueToCopy;\n}",
      "disabled": true
    },
    {
      "kernel_name": "replication_pad_backward_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void replication_pad_backward_kernel(\n    PackedTensorAccessor64<scalar_t, 5> gradInput,\n    PackedTensorAccessor64<const scalar_t, 5> gradOutput,\n    const int pfront,\n    const int ptop,\n    const int pleft,\n    const int y_shift,\n    const int z_shift) {\n  const int outputPointId = threadIdx.x + blockIdx.x * blockDim.x;\n  const int plane = blockIdx.y + y_shift;\n  const int batch = blockIdx.z + z_shift;\n\n  if (outputPointId >= (gradOutput.size(2) * gradOutput.size(3) *\n        gradOutput.size(4))) {\n    return;\n  }\n  const int outputPointX = outputPointId % gradOutput.size(4);\n  const int outputPointY = (outputPointId / gradOutput.size(4)) %\n    gradOutput.size(3);\n  const int outputPointZ = outputPointId / (gradOutput.size(3) *\n      gradOutput.size(4));\n\n  const int iStartX = imax(0, -pleft);\n  const int iStartY = imax(0, -ptop);\n  const int iStartZ = imax(0, -pfront);\n  const int oStartX = imax(0, pleft);\n  const int oStartY = imax(0, ptop);\n  const int oStartZ = imax(0, pfront);\n\n  const int inputPointX = imin(imax(pleft, outputPointX),\n      gradInput.size(4) + pleft - 1) - oStartX + iStartX;\n  const int inputPointY = imin(imax(ptop, outputPointY),\n      gradInput.size(3) + ptop - 1) - oStartY + iStartY;\n  const int inputPointZ = imin(imax(pfront, outputPointZ),\n      gradInput.size(2) + pfront - 1) - oStartZ + iStartZ;\n\n  scalar_t valueToCopy =\n    gradOutput[batch][plane][outputPointZ][outputPointY][outputPointX];\n  gpuAtomicAddNoReturn(&gradInput[batch][plane][inputPointZ][inputPointY][inputPointX],\n      valueToCopy);\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptivemaxpool",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptivemaxpool(\n                        const T *input, T *output, int64_t *indices,\n                        int isizeT, int isizeH, int isizeW,\n                        int osizeT, int osizeH, int osizeW,\n                        int64_t istrideD,\n                        int64_t istrideT, int64_t istrideH, int64_t istrideW,\n                        int64_t offsetZ)\n{\n  // iterators on output pixels\n  int ot, oh, ow;\n\n  // compute offsets based on thread/block ID\n  int ostartH = blockIdx.y * blockDim.y + threadIdx.y;\n  int oendH   = osizeH;\n  int ostepH  = gridDim.y * blockDim.y;\n  int ostartW = threadIdx.x;\n  int oendW   = osizeW;\n  int ostepW  = blockDim.x;\n\n  // select output plane\n  int64_t o_plane = blockIdx.x + offsetZ;\n  ot = o_plane % osizeT;     // output frame/time\n  int d = o_plane / osizeT;  // slice/feature\n\n  // input frame/time ramge is fixed.\n  int istartT = start_index(ot, osizeT, isizeT);\n  int iendT = end_index(ot, osizeT, isizeT);\n  int kT = iendT - istartT;\n\n  // input offset by slice/feature and earliest relevant frame/time\n  const T *input_dt = input + d*istrideD + istartT*istrideT;\n  // output offset by slice/feature and frame/time\n  T *output_dt = output + o_plane*osizeH*osizeW;\n  // indices offset by slice/feature and frame/time\n  int64_t *indices_dt = indices + o_plane*osizeH*osizeW;\n\n  // For all output pixels...\n  for(oh = ostartH; oh < oendH; oh += ostepH) {\n\n    int istartH = start_index(oh, osizeH, isizeH);\n    int iendH   = end_index(oh, osizeH, isizeH);\n    int kH = iendH - istartH;\n\n    for(ow = ostartW; ow < oendW; ow += ostepW) {\n\n      int istartW = start_index(ow, osizeW, isizeW);\n      int iendW   = end_index(ow, osizeW, isizeW);\n      int kW = iendW - istartW;\n\n      // Compute the average pooling from corresponding input pixels\n      const T *ptr_input = input_dt + istartH*istrideH + istartW*istrideW;\n      T *ptr_output = output_dt + oh*osizeW + ow;\n      int64_t *ptr_ind = indices_dt + oh*osizeW + ow;\n      int64_t argmax = istartT*isizeH*isizeW + istartH*isizeW + istartW;\n      T max = at::numeric_limits<T>::lower_bound(); // -Infinity\n\n      int it, ih, iw;\n      for(it = 0; it < kT; ++it) {\n        for(ih = 0; ih < kH; ++ih) {\n          for(iw = 0; iw < kW; ++iw) {\n            T val = ptr_input[ih*istrideH + iw*istrideW];\n            if ((val > max) || at::_isnan(val)) {\n              max = val;\n              argmax = (it+istartT)*isizeH*isizeW + (ih+istartH)*isizeW + iw+istartW;\n            }\n          }\n        }\n        ptr_input += istrideT;   // next input frame\n      }\n      // Update output and argmax\n      *ptr_output = max;\n      *ptr_ind = argmax;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "adaptivemaxgradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void adaptivemaxgradinput(\n  T *gradInput, const T *gradOutput, const int64_t *indices,\n  int isizeT, int isizeH, int isizeW,\n  int osizeT, int osizeH, int osizeW,\n  int64_t offsetZ\n)\n{\n  // iterators on output pixels\n  int oh, ow;\n\n  // compute offsets based on thread/block ID\n  int ostartH = blockIdx.y * blockDim.y + threadIdx.y;\n  int oendH   = osizeH;\n  int ostepH  = gridDim.y * blockDim.y;\n  int ostartW = threadIdx.x;\n  int oendW   = osizeW;\n  int ostepW  = blockDim.x;\n\n  // select output plane\n  int64_t o_plane = blockIdx.x + offsetZ;\n  int d = o_plane / osizeT;     // output slice/feature\n\n  // gradInput offset by slice/feature\n  T *gradInput_d = gradInput + d*isizeT*isizeH*isizeW;\n  // gradOutput offset by slice/feature and frame/otme\n  const T *gradOutput_dt = gradOutput + o_plane*osizeH*osizeW;\n  // indices offset by slice/feature and frame/otme\n  const int64_t *indices_dt = indices + o_plane*osizeH*osizeW;\n\n  // For all output pixels...\n  for(oh = ostartH; oh < oendH; oh += ostepH) {\n    for(ow = ostartW; ow < oendW; ow += ostepW) {\n      // Compute the gradients for the argmax input pixel\n      const T *ptr_gradOutput = gradOutput_dt + oh*osizeW + ow;\n      const int64_t *ptr_ind = indices_dt + oh*osizeW + ow;\n      T grad_delta = *ptr_gradOutput;\n      int argmax = (*ptr_ind);\n      gradInput_d[argmax] += grad_delta;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "atomicadaptivemaxgradinput",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void atomicadaptivemaxgradinput(\n  T *gradInput, const T *gradOutput, const int64_t *indices,\n  int isizeT, int isizeH, int isizeW,\n  int osizeT, int osizeH, int osizeW,\n  int64_t offsetZ\n)\n{\n  // iterators on output pixels\n  int oh, ow;\n\n  // compute offsets based on thread/block ID\n  int ostartH = blockIdx.y * blockDim.y + threadIdx.y;\n  int oendH   = osizeH;\n  int ostepH  = gridDim.y * blockDim.y;\n  int ostartW = threadIdx.x;\n  int oendW   = osizeW;\n  int ostepW  = blockDim.x;\n\n  // select output plane\n  int64_t o_plane = blockIdx.x + offsetZ;\n  int d = o_plane / osizeT;     // output slice/feature\n\n  // gradInput offset by slice/feature\n  T *gradInput_d = gradInput + d*isizeT*isizeH*isizeW;\n  // gradOutput offset by slice/feature and frame/otme\n  const T *gradOutput_dt = gradOutput + o_plane*osizeH*osizeW;\n  // indices offset by slice/feature and frame/otme\n  const int64_t *indices_dt = indices + o_plane*osizeH*osizeW;\n\n  // For all output pixels...\n  for(oh = ostartH; oh < oendH; oh += ostepH) {\n    for(ow = ostartW; ow < oendW; ow += ostepW) {\n      // Compute the gradients for the argmax input pixel\n      const T *ptr_gradOutput = gradOutput_dt + oh*osizeW + ow;\n      const int64_t *ptr_ind = indices_dt + oh*osizeW + ow;\n      T grad_delta = *ptr_gradOutput;\n      int64_t argmax = (*ptr_ind);\n      gpuAtomicAddNoReturn(&(gradInput_d[argmax]), grad_delta);\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "write_indices",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void write_indices(\n    int64_t* inp,\n    TensorDims<index_t> dims,\n    int ndim,\n    index_t n,\n    int64_t * total = nullptr,\n    int64_t fill_value = -1) {\n  auto index = threadIdx.x + (int64_t)blockIdx.x * blockDim.x;\n  bool cond = (total == nullptr || index < *total);\n  if (index < n && cond) {\n    index_t div = 1;\n    int64_t idx_flat = inp[index];\n#pragma unroll\n    for (int dim = MAX_DIMS; dim >= 0; dim--) {\n      if (dim > ndim - 1)\n        continue;\n      auto dim_size = dims.sizes[dim];\n      inp[index + dim * n] = (idx_flat / div) % dim_size;\n      div *= dim_size;\n    }\n  } else if (index < n) {\n    // 0th dim has correct values already\n    for (int dim = ndim - 1; dim > 0; dim--) {\n      inp[index + dim * n] = fill_value;\n    }\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "write_fill_value",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void write_fill_value(int64_t * inp, int64_t * total, int64_t fill_value, int64_t n){\n  int64_t total_val = *total;\n  // not aiming for vectorized stores\n\n  for (int64_t idx = total_val + (int64_t)blockIdx.x * blockDim.x + threadIdx.x; idx < n; idx += blockDim.x * gridDim.x) {\n      inp[idx] = fill_value;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "compute_agg",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void compute_agg(int32_t * agg, int64_t * agg_cum, uint32_t n_blocks) {\n\n  using BlockScanT = ROCM_HIPCUB(at_cuda_detail::cub)::BlockScan<int64_t, BLOCK_THREADS, ROCM_HIPCUB(at_cuda_detail::cub)::BLOCK_SCAN_WARP_SCANS>;\n  __shared__ typename BlockScanT::TempStorage temp_storage;\n  int agg_data;\n  int64_t agg_cum_data;\n  agg_data = threadIdx.x < n_blocks ? agg[threadIdx.x] : 0;\n  BlockScanT(temp_storage).InclusiveSum(agg_data, agg_cum_data);\n  if (threadIdx.x < n_blocks) {\n    agg_cum[threadIdx.x] = agg_cum_data;\n  }\n}",
      "disabled": true
    },
    {
      "kernel_name": "flag_kernel",
      "task": "Write {gpu_software} kernel that implements the cuda kernel provided below {code}. The signature of the {gpu_software} kernel should match the signature of the cuda kernel.",
      "code": "__global__ void flag_kernel(const T* d_in, int64_t * d_out, const int64_t * agg, int64_t input_nelem, int64_t output_nelem, int iters_per_cta) {\n  int64_t start_idx = BLOCK_THREADS * ITEMS_PER_THREAD * iters_per_cta * (int64_t)blockIdx.x;\n  if (start_idx >= input_nelem) return;\n  d_in += start_idx;\n\n  using BlockLoadT = ROCM_HIPCUB(at_cuda_detail::cub)::BlockLoad<int, BLOCK_THREADS, ITEMS_PER_THREAD, ROCM_HIPCUB(at_cuda_detail::cub)::BLOCK_LOAD_WARP_TRANSPOSE>;\n\n  // Specialize BlockScan type for our thread block\n  using BlockScanT = ROCM_HIPCUB(at_cuda_detail::cub)::BlockScan<int, BLOCK_THREADS, ROCM_HIPCUB(at_cuda_detail::cub)::BLOCK_SCAN_WARP_SCANS>;\n  using TransformInputIteratorT = ROCM_HIPCUB(at_cuda_detail::cub)::TransformInputIterator<int, NonZeroOp<T>, const T*>;\n  using BlockExchangeT =  ROCM_HIPCUB(at_cuda_detail::cub)::BlockExchange<int, BLOCK_THREADS, ITEMS_PER_THREAD>;\n\n  // Shared memory\n  __shared__ union TempStorage\n  {\n    typename BlockLoadT::TempStorage load;\n    typename BlockScanT::TempStorage scan;\n    typename BlockExchangeT::TempStorage exchange;\n  } temp_storage;\n\n  int64_t aggregate = blockIdx.x == 0 ? 0 : agg[blockIdx.x - 1];\n  d_out += aggregate;\n\n  TransformInputIteratorT t_input_itr(d_in, NonZeroOp<T>());\n\n  // Per-thread tile data\n  int data[ITEMS_PER_THREAD];\n  int out_indices[ITEMS_PER_THREAD];\n\n  int64_t remaining =  input_nelem - start_idx;\n  int64_t out_remaining = output_nelem - aggregate;\n  for (int i=0; i<iters_per_cta; i++){\n\n  // Load items into a blocked arrangement\n    if (remaining >= BLOCK_THREADS * ITEMS_PER_THREAD) {\n      BlockLoadT(temp_storage.load).Load(t_input_itr, data);\n    } else {\n      BlockLoadT(temp_storage.load).Load(t_input_itr, data, remaining, int(0));\n    }\n\n    // Barrier for smem reuse\n    __syncthreads();\n\n    // Compute inclusive prefix sum\n    int aggregate;\n    __shared__ int aggregate_sh;\n    BlockScanT(temp_storage.scan).ExclusiveSum(data, out_indices, aggregate);\n\n    if (threadIdx.x == 0){\n      aggregate_sh = aggregate;\n    }\n\n    // Barrier for smem reuse\n    __syncthreads();\n    // striped arrangement will provide a slightly better\n    // coalescing for writes (although it's still bad because it's indirect indexing)\n    BlockExchangeT(temp_storage.exchange).BlockedToStriped(data);\n    __syncthreads();\n    BlockExchangeT(temp_storage.exchange).BlockedToStriped(out_indices);\n    for (int ii=0; ii<ITEMS_PER_THREAD; ii++){\n      if (data[ii] != 0 && out_indices[ii] < out_remaining) {\n        int64_t inp_idx = start_idx + threadIdx.x + blockDim.x * ii;\n        d_out[out_indices[ii]] = inp_idx;\n      }\n    }\n\n    out_remaining -= aggregate_sh;\n    remaining -= BLOCK_THREADS * ITEMS_PER_THREAD;\n    if (remaining <= 0 || out_remaining <= 0) return;\n    d_out += aggregate_sh;\n    t_input_itr += BLOCK_THREADS * ITEMS_PER_THREAD;\n    start_idx += BLOCK_THREADS * ITEMS_PER_THREAD;\n    __syncthreads();\n  }\n\n}",
      "disabled": true
    }
  ]
}
