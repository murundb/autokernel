{
  "task": "Write OpenCL kernel that implements the cuda kernel provided below __global__ void renormRowsL1(scalar_t* dist, long rows, long cols) {\n  extern __shared__  unsigned char my_smem[];\n  scalar_t *smem = reinterpret_cast<scalar_t *>(my_smem);\n  scalar_t zero = static_cast<scalar_t>(0);\n  scalar_t val;\n  for (int64_t row = blockIdx.x; row < rows; row += gridDim.x) {\n    scalar_t sum = static_cast<scalar_t>(0);\n    for (int64_t col = threadIdx.x; col < cols; col += blockDim.x) {\n      val = dist[row * cols + col];\n      CUDA_KERNEL_ASSERT(!(val < zero)); // ! < 0 for NaN handling\n      sum = sum + val;\n    }\n\n    sum = cuda_utils::BlockReduceSum(sum, smem);\n    if (threadIdx.x == 0) {\n      CUDA_KERNEL_ASSERT(!(val < zero)); // ! < 0 for NaN handling\n      smem[0] = sum;\n    }\n    __syncthreads();\n\n    sum = smem[0];\n    if (sum > zero) {\n      for (int64_t col = threadIdx.x; col < cols; col += blockDim.x) {\n        dist[row * cols + col] = dist[row * cols + col] / sum;\n      }\n    }\n  }\n}. The signature of the OpenCL kernel should match the signature of the cuda kernel.",
  "task_name": "renormRowsL1",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # OpenCL kernel code optimized for NVIDIA GPU\n    kernel_code = \"\"\"\n    // Main kernel for float type\n    __kernel void renormRowsL1(__global float* dist, const long rows, const long cols) {\n        __local float local_sum[1024]; // Local memory for reduction\n        const float zero = 0.0f;\n        float val;\n        \n        // Process one row per work group - matches CUDA blockIdx.x pattern\n        for (long row = get_group_id(0); row < rows; row += get_num_groups(0)) {\n            float sum = zero;\n            \n            // Each thread calculates partial sum for this row - matches CUDA threadIdx.x pattern\n            for (long col = get_local_id(0); col < cols; col += get_local_size(0)) {\n                val = dist[row * cols + col];\n                // Equivalent to CUDA_KERNEL_ASSERT(!(val < zero)) for NaN handling\n                if (!(val >= zero)) {\n                    // In production code, this would abort\n                    // For this implementation, we handle it gracefully\n                    val = zero;\n                }\n                sum += val;\n            }\n            \n            // Store in local memory for reduction - equivalent to BlockReduceSum\n            local_sum[get_local_id(0)] = sum;\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Perform reduction in shared memory\n            for (unsigned int stride = get_local_size(0)/2; stride > 0; stride >>= 1) {\n                if (get_local_id(0) < stride) {\n                    local_sum[get_local_id(0)] += local_sum[get_local_id(0) + stride];\n                }\n                barrier(CLK_LOCAL_MEM_FENCE);\n            }\n            \n            // Thread 0 has the final sum - verify and save\n            if (get_local_id(0) == 0) {\n                // Equivalent to CUDA_KERNEL_ASSERT(!(val < zero))\n                if (!(local_sum[0] >= zero)) {\n                    local_sum[0] = zero;\n                }\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // All threads read the validated sum\n            sum = local_sum[0];\n            \n            // Normalize the row if sum is positive\n            if (sum > zero) {\n                for (long col = get_local_id(0); col < cols; col += get_local_size(0)) {\n                    dist[row * cols + col] = dist[row * cols + col] / sum;\n                }\n            }\n        }\n    }\n    \n    // Same kernel for double precision\n    __kernel void renormRowsL1_double(__global double* dist, const long rows, const long cols) {\n        __local double local_sum[1024]; // Local memory for reduction\n        const double zero = 0.0;\n        double val;\n        \n        // Process one row per work group - matches CUDA blockIdx.x pattern\n        for (long row = get_group_id(0); row < rows; row += get_num_groups(0)) {\n            double sum = zero;\n            \n            // Each thread calculates partial sum for this row - matches CUDA threadIdx.x pattern\n            for (long col = get_local_id(0); col < cols; col += get_local_size(0)) {\n                val = dist[row * cols + col];\n                // Equivalent to CUDA_KERNEL_ASSERT(!(val < zero)) for NaN handling\n                if (!(val >= zero)) {\n                    // In production code, this would abort\n                    // For this implementation, we handle it gracefully\n                    val = zero;\n                }\n                sum += val;\n            }\n            \n            // Store in local memory for reduction - equivalent to BlockReduceSum\n            local_sum[get_local_id(0)] = sum;\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Perform reduction in shared memory\n            for (unsigned int stride = get_local_size(0)/2; stride > 0; stride >>= 1) {\n                if (get_local_id(0) < stride) {\n                    local_sum[get_local_id(0)] += local_sum[get_local_id(0) + stride];\n                }\n                barrier(CLK_LOCAL_MEM_FENCE);\n            }\n            \n            // Thread 0 has the final sum - verify and save\n            if (get_local_id(0) == 0) {\n                // Equivalent to CUDA_KERNEL_ASSERT(!(val < zero))\n                if (!(local_sum[0] >= zero)) {\n                    local_sum[0] = zero;\n                }\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // All threads read the validated sum\n            sum = local_sum[0];\n            \n            // Normalize the row if sum is positive\n            if (sum > zero) {\n                for (long col = get_local_id(0); col < cols; col += get_local_size(0)) {\n                    dist[row * cols + col] = dist[row * cols + col] / sum;\n                }\n            }\n        }\n    }\n    \"\"\"\n\n    # Initialize data\n    rows = 1024\n    cols = 1024\n    dtype = np.float32\n    \n    # Create random data ensuring no negative values\n    data_host = np.random.rand(rows, cols).astype(dtype)\n    \n    # Keep a copy for verification\n    data_copy = data_host.copy()\n    \n    # Get platform, device, context, queue\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    platform = platforms[0]\n    devices = platform.get_devices(device_type=cl.device_type.GPU)\n    if not devices:\n        raise RuntimeError(\"No GPU devices found.\")\n    device = devices[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n    \n    # Create OpenCL buffer\n    mf = cl.mem_flags\n    data_buf = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=data_host)\n    \n    # Build program\n    program = cl.Program(ctx, kernel_code).build()\n    kernel_func = program.renormRowsL1\n    \n    # Define work sizes optimized for NVIDIA RTX GPU\n    # For NVIDIA GPUs, using 256 threads per workgroup is generally good\n    local_size = 256  \n    # Limit total workgroups based on rows\n    max_groups = min(rows, 256)  # Adjust based on GPU compute units and rows\n    global_size = max_groups * local_size\n    \n    # Define verification function\n    def verification_fn():\n        # Get the result from GPU\n        result_host = np.empty_like(data_host)\n        cl.enqueue_copy(queue, result_host, data_buf)\n        \n        # Compute the expected result on CPU\n        expected = data_copy.copy()\n        row_sums = expected.sum(axis=1, keepdims=True)\n        # Only normalize rows where sum > 0\n        mask = (row_sums > 0).flatten()\n        expected[mask] = expected[mask] / row_sums[mask]\n        \n        # Check if results match\n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n    \n    # Run the kernel\n    num_iterations = 10\n    execution_times = []\n    \n    for i in range(num_iterations):\n        queue.finish()\n        start_time = time.time()\n        \n        # Launch the kernel\n        kernel_func(queue, (global_size,), (local_size,), data_buf, np.int64(rows), np.int64(cols))\n        \n        queue.finish()\n        end_time = time.time()\n        elapsed_time = (end_time - start_time) * 1000  # ms\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n    \n    is_correct, msg = verification_fn()\n    \n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"local_size\": local_size,\n        \"global_size\": global_size\n    }\n    \n    return timing_result",
  "timing_info": {
    "iterations": 10,
    "average_ms": 0.0849008560180664,
    "min_ms": 0.027418136596679688,
    "max_ms": 0.5879402160644531,
    "all_times_ms": [
      0.5879402160644531,
      0.039577484130859375,
      0.028371810913085938,
      0.027418136596679688,
      0.027418136596679688,
      0.027894973754882812,
      0.027418136596679688,
      0.027418136596679688,
      0.027894973754882812,
      0.02765655517578125
    ],
    "correct_result": true,
    "verification_feedback": "Results match!",
    "local_size": 256,
    "global_size": 65536
  }
}