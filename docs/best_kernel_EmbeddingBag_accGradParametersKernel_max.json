{
  "task": "Write OpenCL kernel that implements the cuda kernel provided below __global__ void EmbeddingBag_accGradParametersKernel_max(\n    const index_t *max_indices, const scalar_t *gradOutput,\n    scalar_t *gradWeight, int64_t stride, int64_t numBags,\n    index_t padding_idx, const index_t numel) {\n\n  using accscalar_t = acc_type<scalar_t, true>;\n\n  int64_t chunksPerBag = ceil_div(stride, (int64_t)blockDim.x);\n  int64_t numChunks = numBags * chunksPerBag;\n  int64_t chunkOffset = blockIdx.x * blockDim.y + threadIdx.y;\n  int64_t chunkStride = gridDim.x * blockDim.y;\n\n  for (int64_t chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n    int64_t featureDim = (chunk % chunksPerBag) * blockDim.x + threadIdx.x;\n    if (featureDim < stride) {\n      int64_t bag = chunk / chunksPerBag;\n\n      index_t word_idx = max_indices[bag * stride + featureDim];\n      if (word_idx >= 0 && word_idx != padding_idx) {\n        // If bag is empty, we have max_indices[idx] set to -1 in forward.\n        fastAtomicAdd(\n            gradWeight, static_cast<index_t>(word_idx * stride + featureDim),\n            numel, gradOutput[bag * stride + featureDim], true);\n      }\n    }\n  }\n}. The signature of the OpenCL kernel should match the signature of the cuda kernel.",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # OpenCL kernel code\n    opencl_code = \"\"\"\n    inline long ceil_div(long a, long b) {\n        return (a + b - 1) / b;\n    }\n\n    // Custom atomic add implementation for float\n    inline void atomic_add_float(__global float* address, float val) {\n        union {\n            unsigned int intVal;\n            float floatVal;\n        } newVal, prevVal, curVal;\n        \n        do {\n            prevVal.floatVal = *address;\n            newVal.floatVal = prevVal.floatVal + val;\n            curVal.intVal = atom_cmpxchg((volatile __global unsigned int*)address, \n                                          prevVal.intVal, newVal.intVal);\n        } while (curVal.intVal != prevVal.intVal);\n    }\n\n    __kernel void EmbeddingBag_accGradParametersKernel_max(\n        __global const int* restrict max_indices,\n        __global const float* restrict gradOutput,\n        __global float* restrict gradWeight,\n        const long stride, \n        const long numBags,\n        const int padding_idx, \n        const int numel) {\n\n        // Calculate chunking parameters\n        const long chunksPerBag = ceil_div(stride, (long)get_local_size(0));\n        const long numChunks = numBags * chunksPerBag;\n        \n        // Calculate this thread's chunk\n        const long chunkOffset = get_group_id(0) * get_local_size(1) + get_local_id(1);\n        const long chunkStride = get_num_groups(0) * get_local_size(1);\n\n        // Process chunks in a grid-strided loop\n        for (long chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n            const long featureDim = (chunk % chunksPerBag) * get_local_size(0) + get_local_id(0);\n            if (featureDim < stride) {\n                const long bag = chunk / chunksPerBag;\n                const long idx = bag * stride + featureDim;\n                \n                const int word_idx = max_indices[idx];\n                \n                // Only update if word index is valid\n                if (word_idx >= 0 && word_idx != padding_idx) {\n                    const long weight_idx = word_idx * stride + featureDim;\n                    \n                    // Ensure we're not writing out of bounds\n                    if (weight_idx < numel) {\n                        atomic_add_float(&gradWeight[weight_idx], gradOutput[idx]);\n                    }\n                }\n            }\n        }\n    }\n    \"\"\"\n\n    # Test parameters\n    stride = 512\n    numBags = 256\n    vocab_size = 1000\n    padding_idx = -1\n    num_iterations = 10\n\n    # Function to verify the results\n    def verification_fn():\n        result_host = np.zeros_like(gradWeight_host)\n        cl.enqueue_copy(queue, result_host, gradWeight_buf)\n        \n        # CPU implementation for verification\n        expected = np.zeros_like(gradWeight_host)\n        for bag in range(numBags):\n            for feat in range(stride):\n                idx = bag * stride + feat\n                word_idx = max_indices_host[idx]\n                if word_idx >= 0 and word_idx != padding_idx:\n                    weight_idx = word_idx * stride + feat\n                    if weight_idx < gradWeight_host.size:\n                        expected[weight_idx] += gradOutput_host[idx]\n        \n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n\n    # Initialize test data\n    np.random.seed(42)\n    max_indices_host = np.random.randint(-1, vocab_size, size=numBags * stride).astype(np.int32)\n    gradOutput_host = np.random.randn(numBags * stride).astype(np.float32)\n    gradWeight_host = np.zeros(vocab_size * stride, dtype=np.float32)\n    \n    # Create OpenCL context and queue\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    \n    platform = platforms[0]\n    devices = platform.get_devices()\n    if not devices:\n        raise RuntimeError(\"No OpenCL devices found.\")\n    \n    device = devices[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n    \n    # Create buffers\n    mf = cl.mem_flags\n    max_indices_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=max_indices_host)\n    gradOutput_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=gradOutput_host)\n    \n    # Grid and block sizes optimized for NVIDIA GPU\n    block_size = (32, 8)  # Local work size (X, Y) - multiple of 32 for NVIDIA warp\n    grid_size = (32, 4)   # Number of work groups\n    \n    # Compile and run the kernel\n    program = cl.Program(ctx, opencl_code).build()\n    kernel = program.EmbeddingBag_accGradParametersKernel_max\n    \n    execution_times = []\n    \n    for i in range(num_iterations):\n        # Reset gradWeight buffer for each iteration\n        gradWeight_buf = cl.Buffer(ctx, mf.WRITE_ONLY | mf.COPY_HOST_PTR, hostbuf=gradWeight_host)\n        \n        # Set kernel arguments\n        args = (\n            max_indices_buf, \n            gradOutput_buf, \n            gradWeight_buf, \n            np.int64(stride), \n            np.int64(numBags), \n            np.int32(padding_idx), \n            np.int32(gradWeight_host.size)\n        )\n        \n        # Define global and local work sizes\n        global_size = (block_size[0] * grid_size[0], block_size[1] * grid_size[1])\n        local_size = block_size\n        \n        # Ensure queue is finished before timing\n        queue.finish()\n        start_time = time.time()\n        \n        # Launch kernel\n        kernel(queue, global_size, local_size, *args)\n        \n        # Wait for completion before measurement\n        queue.finish()\n        end_time = time.time()\n        \n        elapsed_time = (end_time - start_time) * 1000  # Convert to ms\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    # Calculate statistics\n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n    \n    # Verify results\n    is_correct, msg = verification_fn()\n    \n    # Return timing results\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"block_size\": block_size,\n        \"grid_size\": grid_size\n    }\n    \n    return timing_result",
  "timing_info": {
    "iterations": 10,
    "average_ms": 2.5388240814208984,
    "min_ms": 2.3381710052490234,
    "max_ms": 2.833127975463867,
    "all_times_ms": [
      2.833127975463867,
      2.3512840270996094,
      2.3381710052490234,
      2.495288848876953,
      2.6655197143554688,
      2.5925636291503906,
      2.6597976684570312,
      2.5687217712402344,
      2.44903564453125,
      2.4347305297851562
    ],
    "correct_result": false,
    "verification_feedback": "Results do NOT match!",
    "block_size": [
      32,
      8
    ],
    "grid_size": [
      32,
      4
    ]
  }
}