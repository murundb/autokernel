{
  "task": "Write OpenCL kernel that implements the cuda kernel provided below __global__ void EmbeddingBag_accGradParametersKernel_max(\n    const index_t *max_indices, const scalar_t *gradOutput,\n    scalar_t *gradWeight, int64_t stride, int64_t numBags,\n    index_t padding_idx, const index_t numel) {\n\n  using accscalar_t = acc_type<scalar_t, true>;\n\n  int64_t chunksPerBag = ceil_div(stride, (int64_t)blockDim.x);\n  int64_t numChunks = numBags * chunksPerBag;\n  int64_t chunkOffset = blockIdx.x * blockDim.y + threadIdx.y;\n  int64_t chunkStride = gridDim.x * blockDim.y;\n\n  for (int64_t chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n    int64_t featureDim = (chunk % chunksPerBag) * blockDim.x + threadIdx.x;\n    if (featureDim < stride) {\n      int64_t bag = chunk / chunksPerBag;\n\n      index_t word_idx = max_indices[bag * stride + featureDim];\n      if (word_idx >= 0 && word_idx != padding_idx) {\n        // If bag is empty, we have max_indices[idx] set to -1 in forward.\n        fastAtomicAdd(\n            gradWeight, static_cast<index_t>(word_idx * stride + featureDim),\n            numel, gradOutput[bag * stride + featureDim], true);\n      }\n    }\n  }\n}. The signature of the OpenCL kernel should match the signature of the cuda kernel.",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n    \n    # OpenCL kernel code\n    opencl_code = \"\"\"\n    // Ceiling division function\n    inline int ceil_div(int a, int b) {\n        return (a + b - 1) / b;\n    }\n    \n    typedef float scalar_t;\n    typedef float accscalar_t;\n    typedef int index_t;\n    \n    // Custom atomic add for float values using compare-and-swap\n    // Optimized for NVIDIA GPUs\n    inline void atomic_add_float(__global float* addr, const float val) {\n        union {\n            unsigned int u32;\n            float f32;\n        } next, expected, current;\n        \n        current.f32 = *addr;\n        \n        do {\n            expected.f32 = current.f32;\n            next.f32 = expected.f32 + val;\n            current.u32 = atomic_cmpxchg((volatile __global unsigned int*)addr, \n                                         expected.u32, next.u32);\n        } while (current.u32 != expected.u32);\n    }\n    \n    // Helper function for atomic add with bounds checking\n    inline void fastAtomicAdd(\n        __global scalar_t* restrict address,\n        index_t index,\n        index_t numel,\n        scalar_t val,\n        bool ignore_overflow) {\n        \n        if (index < numel) {\n            atomic_add_float(&address[index], val);\n        }\n    }\n    \n    __kernel void EmbeddingBag_accGradParametersKernel_max(\n        __global const index_t* restrict max_indices,\n        __global const scalar_t* restrict gradOutput,\n        __global scalar_t* restrict gradWeight,\n        const long stride,\n        const long numBags,\n        const index_t padding_idx,\n        const index_t numel) {\n        \n        // Calculate thread and block indices\n        const int local_x = get_local_id(0);\n        const int local_y = get_local_id(1);\n        const int group_id = get_group_id(0);\n        const int local_size_x = get_local_size(0);\n        const int local_size_y = get_local_size(1);\n        \n        // Calculate chunksPerBag similar to CUDA version\n        long chunksPerBag = ceil_div(stride, local_size_x);\n        long numChunks = numBags * chunksPerBag;\n        long chunkOffset = group_id * local_size_y + local_y;\n        long chunkStride = get_num_groups(0) * local_size_y;\n        \n        // Process chunks in strided fashion for better memory access patterns\n        for (long chunk = chunkOffset; chunk < numChunks; chunk += chunkStride) {\n            long featureDim = (chunk % chunksPerBag) * local_size_x + local_x;\n            if (featureDim < stride) {\n                long bag = chunk / chunksPerBag;\n                \n                // Calculate index for max_indices and gradOutput once\n                long idx = bag * stride + featureDim;\n                \n                index_t word_idx = max_indices[idx];\n                if (word_idx >= 0 && word_idx != padding_idx) {\n                    // Prefetch gradient value to minimize global memory accesses\n                    scalar_t grad_val = gradOutput[idx];\n                    // Use atomic add for thread safety\n                    fastAtomicAdd(\n                        gradWeight, (index_t)(word_idx * stride + featureDim),\n                        numel, grad_val, true);\n                }\n            }\n        }\n    }\n    \"\"\"\n    \n    # Test parameters\n    num_iterations = 5\n    num_bags = 1024\n    stride = 256\n    num_embeddings = 2048\n    padding_idx = -1\n    \n    # Initialize test data\n    max_indices = np.random.randint(-1, num_embeddings, size=(num_bags, stride)).astype(np.int32)\n    # Set some indices to padding_idx\n    max_indices.ravel()[np.random.choice(max_indices.size, size=max_indices.size//10, replace=False)] = padding_idx\n    \n    gradOutput = np.random.rand(num_bags, stride).astype(np.float32)\n    gradWeight = np.zeros((num_embeddings, stride), dtype=np.float32)\n    \n    # Create a reference implementation for verification\n    def reference_impl(max_indices, gradOutput, num_embeddings, stride, padding_idx):\n        ref_gradWeight = np.zeros((num_embeddings, stride), dtype=np.float32)\n        for bag in range(max_indices.shape[0]):\n            for feat in range(max_indices.shape[1]):\n                word_idx = max_indices[bag, feat]\n                if word_idx >= 0 and word_idx != padding_idx:\n                    ref_gradWeight[word_idx, feat] += gradOutput[bag, feat]\n        return ref_gradWeight\n    \n    # Function to verify the results\n    def verification_fn():\n        result_host = np.empty_like(gradWeight)\n        cl.enqueue_copy(queue, result_host, gradWeight_buf)\n        expected = reference_impl(max_indices, gradOutput, num_embeddings, stride, padding_idx)\n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n    \n    # Set up OpenCL context\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    platform = platforms[0]\n    devices = platform.get_devices(device_type=cl.device_type.GPU)\n    if not devices:\n        devices = platform.get_devices()\n    if not devices:\n        raise RuntimeError(\"No OpenCL devices found.\")\n    device = devices[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n    \n    # Create OpenCL buffers\n    mf = cl.mem_flags\n    max_indices_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=max_indices)\n    gradOutput_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=gradOutput)\n    gradWeight_buf = cl.Buffer(ctx, mf.READ_WRITE | mf.COPY_HOST_PTR, hostbuf=gradWeight)\n    \n    # Build the program\n    program = cl.Program(ctx, opencl_code).build()\n    kernel = program.EmbeddingBag_accGradParametersKernel_max\n    \n    # Set up kernel execution parameters - optimized for NVIDIA\n    # Use 32x16=512 threads per block (half max workgroup size for better occupancy)\n    # Max workgroup size is 1024, we use half to improve occupancy\n    block_size = (32, 16)  \n    # Use 32 blocks to ensure all 5 compute units are fully utilized\n    grid_size = (32, 1)    \n    \n    # Prepare global and local work sizes\n    global_size = (block_size[0] * grid_size[0], block_size[1] * grid_size[1])\n    local_size = block_size\n    \n    # Arguments for the kernel\n    kernel_args = (\n        max_indices_buf, \n        gradOutput_buf, \n        gradWeight_buf, \n        np.int64(stride), \n        np.int64(num_bags), \n        np.int32(padding_idx), \n        np.int32(num_embeddings * stride)\n    )\n    \n    # Execute the kernel and measure time\n    execution_times = []\n    for i in range(num_iterations):\n        # Reset gradWeight for each iteration\n        cl.enqueue_copy(queue, gradWeight_buf, gradWeight)\n        \n        queue.finish()\n        start_time = time.time()\n        \n        kernel(queue, global_size, local_size, *kernel_args)\n        \n        queue.finish()\n        end_time = time.time()\n        elapsed_time = (end_time - start_time) * 1000  # Convert to ms\n        \n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    # Compute statistics\n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n    \n    # Verify results\n    is_correct, msg = verification_fn()\n    \n    # Return timing results\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"block_size\": block_size,\n        \"grid_size\": grid_size\n    }\n    \n    return timing_result",
  "timing_info": {
    "iterations": 5,
    "average_ms": 0.7465839385986328,
    "min_ms": 0.6465911865234375,
    "max_ms": 1.0924339294433594,
    "all_times_ms": [
      1.0924339294433594,
      0.6585121154785156,
      0.6830692291259766,
      0.652313232421875,
      0.6465911865234375
    ],
    "correct_result": true,
    "verification_feedback": "Results match!",
    "block_size": [
      32,
      16
    ],
    "grid_size": [
      32,
      1
    ]
  }
}