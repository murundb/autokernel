{
  "task": "Write OpenCL kernel that implements the torch model provided below import torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\n\nN = 2048\n\n\ndef get_inputs():\n    A = torch.randn(N, N)\n    B = torch.randn(N, N)\n    return [A, B]\n\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n. Use the input specifications provided above for any validation and timings. A sample cuda kernel is provided here #include <torch/extension.h>\n\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <c10/cuda/CUDAException.h>\n\n#define TILE_SIZE 16\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n\n__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n\n    int row = blockIdx.y * TILE_SIZE + ty;\n    int col = blockIdx.x * TILE_SIZE + tx;\n\n    float C_value = 0.0f;\n\n    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n        // Load tiles into shared memory\n        if (row < N && m * TILE_SIZE + tx < N)\n            As[ty][tx] = A[row * N + m * TILE_SIZE + tx];\n        else\n            As[ty][tx] = 0.0f;\n\n        if (col < N && m * TILE_SIZE + ty < N)\n            Bs[ty][tx] = B[(m * TILE_SIZE + ty) * N + col];\n        else\n            Bs[ty][tx] = 0.0f;\n\n        __syncthreads();\n\n        // Compute partial product\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            C_value += As[ty][k] * Bs[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    // Write the result\n    if (row < N && col < N)\n        C[row * N + col] = C_value;\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    CHECK_INPUT(A);\n    CHECK_INPUT(B);\n    CHECK_FLOAT(A);\n    CHECK_FLOAT(B);\n\n    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n\n    int64_t N = A.size(0);\n\n    auto C = torch::zeros({N, N}, A.options());\n\n    const float* A_data = A.data_ptr<float>();\n    const float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n\n    // Check for kernel launch errors\n    C10_CUDA_CHECK(cudaGetLastError());\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication kernel (CUDA)\");\n}. The signature of the OpenCL kernel should match the signature of the cuda sample kernel.",
  "task_name": "1_Square_matrix_multiplication_",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # Add the kernel code into a python string\n    opencl_code = \"\"\"\n    // For NVidia RTX 4070 with 48KB local memory, we can use larger tiles\n    #define TILE_SIZE 32\n    \n    __kernel void matmul_tiled(__global const float* restrict A,\n                             __global const float* restrict B,\n                             __global float* restrict C,\n                             const int N) {\n        // Local memory for the tiles with padding to avoid bank conflicts\n        __local float A_tile[TILE_SIZE][TILE_SIZE+1];\n        __local float B_tile[TILE_SIZE][TILE_SIZE+1];\n        \n        // Thread identifiers\n        const int tx = get_local_id(0);\n        const int ty = get_local_id(1);\n        \n        // Calculate global indices\n        const int row = get_group_id(1) * TILE_SIZE + ty;\n        const int col = get_group_id(0) * TILE_SIZE + tx;\n        \n        // Register to accumulate results - prevents repeated reads/writes\n        float sum = 0.0f;\n        \n        // Calculate number of tiles needed\n        const int num_tiles = (N + TILE_SIZE - 1) / TILE_SIZE;\n        \n        // Loop over all tiles\n        for (int t = 0; t < num_tiles; t++) {\n            // Load data into local memory with coalesced access patterns\n            if (row < N && t * TILE_SIZE + tx < N) {\n                A_tile[ty][tx] = A[row * N + t * TILE_SIZE + tx];\n            } else {\n                A_tile[ty][tx] = 0.0f;\n            }\n            \n            if (t * TILE_SIZE + ty < N && col < N) {\n                B_tile[ty][tx] = B[(t * TILE_SIZE + ty) * N + col];\n            } else {\n                B_tile[ty][tx] = 0.0f;\n            }\n            \n            // Ensure all threads have loaded their data\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Compute partial matrix multiplication with aggressive loop unrolling\n            #pragma unroll 8\n            for (int k = 0; k < TILE_SIZE; k++) {\n                sum += A_tile[ty][k] * B_tile[k][tx];\n            }\n            \n            // Ensure all computations are done before loading next tile\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Write the final result\n        if (row < N && col < N) {\n            C[row * N + col] = sum;\n        }\n    }\n    \n    // Version optimized for better ILP and memory access patterns\n    __kernel void matmul_tiled_optimized(__global const float* restrict A,\n                                        __global const float* restrict B,\n                                        __global float* restrict C,\n                                        const int N) {\n        // Use tile size of 16 for higher occupancy\n        #define OPT_TILE_SIZE 16\n        \n        // Local memory for tiles - padded to avoid bank conflicts\n        __local float A_tile[OPT_TILE_SIZE][OPT_TILE_SIZE+1];\n        __local float B_tile[OPT_TILE_SIZE][OPT_TILE_SIZE+1];\n        \n        // Thread identifiers\n        const int tx = get_local_id(0);\n        const int ty = get_local_id(1);\n        \n        // Block identifiers\n        const int bx = get_group_id(0);\n        const int by = get_group_id(1);\n        \n        // Calculate global indices\n        const int row = by * OPT_TILE_SIZE + ty;\n        const int col = bx * OPT_TILE_SIZE + tx;\n        \n        // Accumulate in registers\n        float sum = 0.0f;\n        \n        // Calculate number of tiles\n        const int num_tiles = (N + OPT_TILE_SIZE - 1) / OPT_TILE_SIZE;\n        \n        // Loop over tiles\n        for (int t = 0; t < num_tiles; t++) {\n            // Load data into shared memory\n            const int a_idx = row * N + t * OPT_TILE_SIZE + tx;\n            const int b_idx = (t * OPT_TILE_SIZE + ty) * N + col;\n            \n            // Boundary checks for A tile\n            if (row < N && t * OPT_TILE_SIZE + tx < N) {\n                A_tile[ty][tx] = A[a_idx];\n            } else {\n                A_tile[ty][tx] = 0.0f;\n            }\n            \n            // Boundary checks for B tile\n            if (t * OPT_TILE_SIZE + ty < N && col < N) {\n                B_tile[ty][tx] = B[b_idx];\n            } else {\n                B_tile[ty][tx] = 0.0f;\n            }\n            \n            // Synchronize\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Compute with maximized ILP through aggressive unrolling\n            // Use explicit unrolling for better control\n            float tmp = 0.0f;\n            #pragma unroll\n            for (int k = 0; k < OPT_TILE_SIZE; k++) {\n                tmp += A_tile[ty][k] * B_tile[k][tx];\n            }\n            sum += tmp;\n            \n            // Synchronize\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Write result\n        if (row < N && col < N) {\n            C[row * N + col] = sum;\n        }\n    }\n    \n    // Advanced version that processes 2x2 elements per thread\n    __kernel void matmul_tiled_2x2(__global const float* restrict A,\n                                  __global const float* restrict B,\n                                  __global float* restrict C,\n                                  const int N) {\n        #define BLOCK_SIZE 16\n        \n        // Each thread computes 2x2 output elements\n        __local float A_tile[BLOCK_SIZE*2][BLOCK_SIZE*2];\n        __local float B_tile[BLOCK_SIZE*2][BLOCK_SIZE*2];\n        \n        // Thread IDs\n        const int tx = get_local_id(0);\n        const int ty = get_local_id(1);\n        \n        // Base indices for 2x2 block\n        const int row = get_group_id(1) * (BLOCK_SIZE*2) + ty*2;\n        const int col = get_group_id(0) * (BLOCK_SIZE*2) + tx*2;\n        \n        // Registers for accumulation (2x2 block per thread)\n        float c00 = 0.0f;\n        float c01 = 0.0f;\n        float c10 = 0.0f;\n        float c11 = 0.0f;\n        \n        // Compute number of tiles needed\n        const int num_tiles = (N + BLOCK_SIZE*2 - 1) / (BLOCK_SIZE*2);\n        \n        // Each thread loads 4 elements per tile\n        for (int t = 0; t < num_tiles; t++) {\n            // Load 4 elements of A per thread\n            if (row < N && t*(BLOCK_SIZE*2) + tx*2 < N)\n                A_tile[ty*2][tx*2] = A[row*N + t*(BLOCK_SIZE*2) + tx*2];\n            else\n                A_tile[ty*2][tx*2] = 0.0f;\n                \n            if (row < N && t*(BLOCK_SIZE*2) + tx*2+1 < N)\n                A_tile[ty*2][tx*2+1] = A[row*N + t*(BLOCK_SIZE*2) + tx*2+1];\n            else\n                A_tile[ty*2][tx*2+1] = 0.0f;\n                \n            if (row+1 < N && t*(BLOCK_SIZE*2) + tx*2 < N)\n                A_tile[ty*2+1][tx*2] = A[(row+1)*N + t*(BLOCK_SIZE*2) + tx*2];\n            else\n                A_tile[ty*2+1][tx*2] = 0.0f;\n                \n            if (row+1 < N && t*(BLOCK_SIZE*2) + tx*2+1 < N)\n                A_tile[ty*2+1][tx*2+1] = A[(row+1)*N + t*(BLOCK_SIZE*2) + tx*2+1];\n            else\n                A_tile[ty*2+1][tx*2+1] = 0.0f;\n            \n            // Load 4 elements of B per thread\n            if (t*(BLOCK_SIZE*2) + ty*2 < N && col < N)\n                B_tile[ty*2][tx*2] = B[(t*(BLOCK_SIZE*2) + ty*2)*N + col];\n            else\n                B_tile[ty*2][tx*2] = 0.0f;\n                \n            if (t*(BLOCK_SIZE*2) + ty*2 < N && col+1 < N)\n                B_tile[ty*2][tx*2+1] = B[(t*(BLOCK_SIZE*2) + ty*2)*N + col+1];\n            else\n                B_tile[ty*2][tx*2+1] = 0.0f;\n                \n            if (t*(BLOCK_SIZE*2) + ty*2+1 < N && col < N)\n                B_tile[ty*2+1][tx*2] = B[(t*(BLOCK_SIZE*2) + ty*2+1)*N + col];\n            else\n                B_tile[ty*2+1][tx*2] = 0.0f;\n                \n            if (t*(BLOCK_SIZE*2) + ty*2+1 < N && col+1 < N)\n                B_tile[ty*2+1][tx*2+1] = B[(t*(BLOCK_SIZE*2) + ty*2+1)*N + col+1];\n            else\n                B_tile[ty*2+1][tx*2+1] = 0.0f;\n            \n            // Synchronize\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Compute 2x2 outputs for this tile\n            #pragma unroll 4\n            for (int k = 0; k < BLOCK_SIZE*2; k++) {\n                c00 += A_tile[ty*2][k] * B_tile[k][tx*2];\n                c01 += A_tile[ty*2][k] * B_tile[k][tx*2+1];\n                c10 += A_tile[ty*2+1][k] * B_tile[k][tx*2];\n                c11 += A_tile[ty*2+1][k] * B_tile[k][tx*2+1];\n            }\n            \n            // Synchronize before loading next tile\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Write 2x2 results\n        if (row < N && col < N)\n            C[row*N + col] = c00;\n        if (row < N && col+1 < N)\n            C[row*N + col+1] = c01;\n        if (row+1 < N && col < N)\n            C[(row+1)*N + col] = c10;\n        if (row+1 < N && col+1 < N)\n            C[(row+1)*N + col+1] = c11;\n    }\n    \"\"\"\n\n    # Matrix dimension\n    dim = 2048\n    num_iterations = 10\n\n    # Initialize OpenCL\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    \n    # Use the first platform and device\n    platform = platforms[0]\n    device = platform.get_devices()[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    \n    # Create context and command queue with profiling enabled\n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx, properties=cl.command_queue_properties.PROFILING_ENABLE)\n    \n    # Initialize data with random values\n    np.random.seed(42)  # For reproducibility\n    A_host = np.random.rand(dim, dim).astype(np.float32)\n    B_host = np.random.rand(dim, dim).astype(np.float32)\n    C_host = np.zeros((dim, dim), dtype=np.float32)\n    \n    # Create OpenCL buffers\n    mf = cl.mem_flags\n    A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A_host)\n    B_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B_host)\n    C_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=dim * dim * np.dtype(np.float32).itemsize)\n    \n    # Build the program with optimization flags\n    program = cl.Program(ctx, opencl_code).build(options=\"-cl-fast-relaxed-math -cl-mad-enable\")\n    \n    # Define kernel variants to test\n    kernel_variants = [\n        (\"matmul_tiled\", (32, 32)),\n        (\"matmul_tiled_optimized\", (16, 16)),\n        (\"matmul_tiled_2x2\", (16, 16))  # 16x16 threads computing 32x32 outputs\n    ]\n    \n    # Function to verify results\n    def verification_fn(result):\n        # Compute reference result\n        expected = np.matmul(A_host, B_host)\n        \n        # Check if results match (with tolerance for floating-point precision)\n        max_diff = np.max(np.abs(result - expected))\n        ok = np.allclose(result, expected, rtol=1e-5, atol=1e-5)\n        return ok, f\"Results {'match' if ok else 'DO NOT match'}! Max difference: {max_diff}\"\n    \n    # Test all kernel variants\n    best_kernel = None\n    best_time = float('inf')\n    best_block_size = None\n    best_grid_size = None\n    \n    for kernel_name, block_size in kernel_variants:\n        print(f\"\\nTesting kernel: {kernel_name}\")\n        \n        kernel_fn = getattr(program, kernel_name)\n        \n        # Calculate grid size based on block size\n        if kernel_name == \"matmul_tiled_2x2\":\n            # For 2x2 kernel, each thread handles 4 elements, so we need fewer threads\n            effective_block_size = (block_size[0]*2, block_size[1]*2)\n            grid_size = ((dim + effective_block_size[0] - 1) // effective_block_size[0] * block_size[0],\n                         (dim + effective_block_size[1] - 1) // effective_block_size[1] * block_size[1])\n        else:\n            # Standard grid calculation\n            grid_size = ((dim + block_size[0] - 1) // block_size[0] * block_size[0],\n                         (dim + block_size[1] - 1) // block_size[1] * block_size[1])\n        \n        # Warm-up runs\n        for _ in range(3):\n            event = kernel_fn(queue, grid_size, block_size, A_buf, B_buf, C_buf, np.int32(dim))\n            event.wait()\n        \n        # Timing loop\n        execution_times = []\n        for i in range(num_iterations):\n            # Ensure device is idle\n            queue.finish()\n            \n            # Execute kernel with profiling\n            event = kernel_fn(queue, grid_size, block_size, A_buf, B_buf, C_buf, np.int32(dim))\n            event.wait()\n            \n            # Calculate elapsed time in milliseconds using profiling\n            elapsed_time = (event.profile.end - event.profile.start) * 1e-6\n            execution_times.append(elapsed_time)\n            print(f\"Iteration {i+1}: {elapsed_time:.4f} ms\")\n        \n        # Calculate statistics\n        avg_time = sum(execution_times) / len(execution_times)\n        min_time = min(execution_times)\n        max_time = max(execution_times)\n        \n        # Verify results\n        result_host = np.empty_like(C_host)\n        cl.enqueue_copy(queue, result_host, C_buf)\n        is_correct, verification_msg = verification_fn(result_host)\n        \n        print(f\"Kernel: {kernel_name}\")\n        print(f\"Block size: {block_size}, Grid size: {grid_size}\")\n        print(f\"Avg Time: {avg_time:.4f} ms, Min: {min_time:.4f} ms, Max: {max_time:.4f} ms\")\n        print(f\"Verification: {verification_msg}\")\n        \n        # Update best kernel if this is faster and correct\n        if avg_time < best_time and is_correct:\n            best_kernel = kernel_name\n            best_time = avg_time\n            best_block_size = block_size\n            best_grid_size = grid_size\n            best_execution_times = execution_times\n            best_verification_msg = verification_msg\n            best_is_correct = is_correct\n    \n    if best_kernel:\n        print(f\"\\nBest kernel: {best_kernel}\")\n        print(f\"Block size: {best_block_size}, Grid size: {best_grid_size}\")\n        print(f\"Average time: {best_time:.4f} ms\")\n    else:\n        print(\"No successful kernel found!\")\n    \n    # Return timing results for the best kernel\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": best_time if best_kernel else None,\n        \"min_ms\": min(best_execution_times) if best_kernel else None,\n        \"max_ms\": max(best_execution_times) if best_kernel else None,\n        \"all_times_ms\": best_execution_times if best_kernel else None,\n        \"correct_result\": best_is_correct if best_kernel else False,\n        \"verification_feedback\": best_verification_msg if best_kernel else \"No successful kernel\",\n        \"block_size\": best_block_size,\n        \"grid_size\": best_grid_size,\n        \"best_kernel\": best_kernel\n    }\n    return timing_result",
  "timing_info": {
    "iterations": 10,
    "average_ms": 5.1000256,
    "min_ms": 4.8128,
    "max_ms": 5.44032,
    "all_times_ms": [
      4.822336,
      5.44032,
      4.814848,
      4.8128,
      5.169728,
      5.266432,
      4.9924479999999996,
      5.159744,
      5.270528,
      5.251072
    ],
    "correct_result": true,
    "verification_feedback": "Results match! Max difference: 0.00177001953125",
    "block_size": [
      16,
      16
    ],
    "grid_size": [
      1024,
      1024
    ],
    "best_kernel": "matmul_tiled_2x2"
  }
}