{
  "task": "Write OpenCL kernel that implements the torch model provided below import torch\nimport torch.nn as nn\n\n\nclass Model(nn.Module):\n    \"\"\"\n    Simple model that performs a single square matrix multiplication (C = A * B)\n    \"\"\"\n\n    def __init__(self):\n        super(Model, self).__init__()\n\n    def forward(self, A: torch.Tensor, B: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        Performs the matrix multiplication.\n\n        Args:\n            A (torch.Tensor): Input matrix A of shape (N, N).\n            B (torch.Tensor): Input matrix B of shape (N, N).\n\n        Returns:\n            torch.Tensor: Output matrix C of shape (N, N).\n        \"\"\"\n        return torch.matmul(A, B)\n\n\nN = 2048\n\n\ndef get_inputs():\n    A = torch.randn(N, N)\n    B = torch.randn(N, N)\n    return [A, B]\n\n\ndef get_init_inputs():\n    return []  # No special initialization inputs needed\n. Use the input specifications provided above for any validation and timings. A sample cuda kernel is provided here #include <torch/extension.h>\n\n#include <cuda.h>\n#include <cuda_runtime.h>\n#include <c10/cuda/CUDAException.h>\n\n#define TILE_SIZE 32\n#define VECTOR_SIZE 4  // float4 is 128-bits\n\n#define CHECK_CUDA(x) TORCH_CHECK(x.is_cuda(), #x \" must be a CUDA tensor\")\n#define CHECK_CONTIGUOUS(x) TORCH_CHECK(x.is_contiguous(), #x \" must be contiguous\")\n#define CHECK_INPUT(x) CHECK_CUDA(x); CHECK_CONTIGUOUS(x)\n#define CHECK_FLOAT(x) TORCH_CHECK(x.scalar_type() == torch::kFloat32, #x \" must be a float32 tensor\")\n\n__global__ void matmul_tiled_kernel(const float* __restrict__ A, const float* __restrict__ B, float* __restrict__ C, int N) {\n    __shared__ float As[TILE_SIZE][TILE_SIZE];\n    __shared__ float Bs[TILE_SIZE][TILE_SIZE];\n\n    int tx = threadIdx.x;\n    int ty = threadIdx.y;\n    int row = blockIdx.y * TILE_SIZE + ty;\n    int col = blockIdx.x * TILE_SIZE + tx;\n\n    float C_value = 0.0f;\n\n    // Ensure aligned access for vectorized loads\n    const float4* A_vec = reinterpret_cast<const float4*>(A);\n    const float4* B_vec = reinterpret_cast<const float4*>(B);\n\n    for (int m = 0; m < (N + TILE_SIZE - 1) / TILE_SIZE; ++m) {\n        // Vector loading for tile A\n        if (row < N && (m * TILE_SIZE + tx) < N) {\n            int base_idx = (row * N + m * TILE_SIZE + tx) / VECTOR_SIZE;\n            if ((m * TILE_SIZE + tx) % VECTOR_SIZE == 0 && (m * TILE_SIZE + tx + 3) < N) {\n                float4 tmp = __ldg(&A_vec[base_idx]);\n                As[ty][tx] = tmp.x;\n                if (tx + 1 < TILE_SIZE) As[ty][tx + 1] = tmp.y;\n                if (tx + 2 < TILE_SIZE) As[ty][tx + 2] = tmp.z;\n                if (tx + 3 < TILE_SIZE) As[ty][tx + 3] = tmp.w;\n            } else {\n                As[ty][tx] = __ldg(&A[row * N + m * TILE_SIZE + tx]);\n            }\n        } else {\n            As[ty][tx] = 0.0f;\n        }\n\n        // Vector loading for tile B\n        if ((m * TILE_SIZE + ty) < N && col < N) {\n            int base_idx = ((m * TILE_SIZE + ty) * N + col) / VECTOR_SIZE;\n            if (col % VECTOR_SIZE == 0 && (col + 3) < N) {\n                float4 tmp = __ldg(&B_vec[base_idx]);\n                Bs[ty][tx] = tmp.x;\n                if (tx + 1 < TILE_SIZE) Bs[ty][tx + 1] = tmp.y;\n                if (tx + 2 < TILE_SIZE) Bs[ty][tx + 2] = tmp.z;\n                if (tx + 3 < TILE_SIZE) Bs[ty][tx + 3] = tmp.w;\n            } else {\n                Bs[ty][tx] = __ldg(&B[(m * TILE_SIZE + ty) * N + col]);\n            }\n        } else {\n            Bs[ty][tx] = 0.0f;\n        }\n\n        __syncthreads();\n\n        #pragma unroll\n        for (int k = 0; k < TILE_SIZE; ++k) {\n            C_value += As[ty][k] * Bs[k][tx];\n        }\n\n        __syncthreads();\n    }\n\n    if (row < N && col < N) {\n        C[row * N + col] = C_value;\n    }\n}\n\ntorch::Tensor forward(torch::Tensor A, torch::Tensor B) {\n    CHECK_INPUT(A);\n    CHECK_INPUT(B);\n    CHECK_FLOAT(A);\n    CHECK_FLOAT(B);\n\n    TORCH_CHECK(A.dim() == 2 && A.size(0) == A.size(1), \"A must be a square matrix\");\n    TORCH_CHECK(B.dim() == 2 && B.size(0) == B.size(1), \"B must be a square matrix\");\n    TORCH_CHECK(A.size(0) == B.size(0), \"A and B must be of the same size\");\n\n    int64_t N = A.size(0);\n\n    auto C = torch::zeros({N, N}, A.options());\n\n    const float* A_data = A.data_ptr<float>();\n    const float* B_data = B.data_ptr<float>();\n    float* C_data = C.data_ptr<float>();\n\n    dim3 threadsPerBlock(TILE_SIZE, TILE_SIZE);\n    dim3 blocksPerGrid((N + TILE_SIZE - 1) / TILE_SIZE, (N + TILE_SIZE - 1) / TILE_SIZE);\n\n    matmul_tiled_kernel<<<blocksPerGrid, threadsPerBlock>>>(A_data, B_data, C_data, N);\n\n    C10_CUDA_CHECK(cudaGetLastError());\n\n    return C;\n}\n\nPYBIND11_MODULE(TORCH_EXTENSION_NAME, m) {\n    m.def(\"forward\", &forward, \"Matrix multiplication kernel (CUDA)\");\n}. The signature of the OpenCL kernel should match the signature of the cuda sample kernel.",
  "task_name": "aligned_vectorized_matmul_base_base",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # OpenCL kernel code\n    opencl_code = \"\"\"\n    __kernel void matmul_kernel(\n        __global const float* restrict A,\n        __global const float* restrict B,\n        __global float* restrict C,\n        const int N)\n    {\n        // Define tile size optimized for Adreno GPUs\n        const int TILE_SIZE = 16;\n        \n        // Local memory for tiles\n        __local float A_tile[TILE_SIZE][TILE_SIZE];\n        __local float B_tile[TILE_SIZE][TILE_SIZE];\n        \n        // Thread position\n        const int local_row = get_local_id(1);\n        const int local_col = get_local_id(0);\n        const int global_row = get_group_id(1) * TILE_SIZE + local_row;\n        const int global_col = get_group_id(0) * TILE_SIZE + local_col;\n        \n        // Accumulator for dot product\n        float sum = 0.0f;\n        \n        // Iterate over tiles\n        const int num_tiles = (N + TILE_SIZE - 1) / TILE_SIZE;\n        \n        for (int tile = 0; tile < num_tiles; tile++) {\n            // Load tiles into local memory\n            const int tileA_col = tile * TILE_SIZE + local_col;\n            const int tileB_row = tile * TILE_SIZE + local_row;\n            \n            // Load A tile with bounds checking\n            if (global_row < N && tileA_col < N) {\n                A_tile[local_row][local_col] = A[global_row * N + tileA_col];\n            } else {\n                A_tile[local_row][local_col] = 0.0f;\n            }\n            \n            // Load B tile with bounds checking\n            if (tileB_row < N && global_col < N) {\n                B_tile[local_row][local_col] = B[tileB_row * N + global_col];\n            } else {\n                B_tile[local_row][local_col] = 0.0f;\n            }\n            \n            // Ensure all loads are complete\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Compute dot product for this tile\n            #pragma unroll 8\n            for (int k = 0; k < TILE_SIZE; k++) {\n                sum += A_tile[local_row][k] * B_tile[k][local_col];\n            }\n            \n            // Ensure all computations are done before loading next tile\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Write result with bounds check\n        if (global_row < N && global_col < N) {\n            C[global_row * N + global_col] = sum;\n        }\n    }\n    \"\"\"\n\n    # Matrix dimension\n    N = 2048\n    num_iterations = 10\n\n    # Function to verify the results\n    def verification_fn(A_host, B_host, result_host):\n        expected = np.matmul(A_host, B_host)\n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n\n    # Initialize matrices with random data\n    A_host = np.random.rand(N, N).astype(np.float32)\n    B_host = np.random.rand(N, N).astype(np.float32)\n    C_host = np.zeros((N, N), dtype=np.float32)\n\n    # Setup OpenCL context, queue, and buffers\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    \n    platform = platforms[0]\n    devices = platform.get_devices()\n    if not devices:\n        raise RuntimeError(\"No OpenCL devices found on platform.\")\n    \n    device = devices[0]\n    context = cl.Context([device])\n    queue = cl.CommandQueue(context)\n\n    # Create buffers\n    mf = cl.mem_flags\n    A_buf = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A_host)\n    B_buf = cl.Buffer(context, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B_host)\n    C_buf = cl.Buffer(context, mf.WRITE_ONLY, C_host.nbytes)\n\n    # Build the program and create kernel\n    program = cl.Program(context, opencl_code).build()\n    matmul_kernel = program.matmul_kernel\n\n    # Set kernel arguments\n    matmul_kernel.set_args(A_buf, B_buf, C_buf, np.int32(N))\n\n    # Define global and local work sizes\n    local_size = (16, 16)  # Work-group size optimized for Adreno\n    global_size = (\n        ((N + local_size[0] - 1) // local_size[0]) * local_size[0],\n        ((N + local_size[1] - 1) // local_size[1]) * local_size[1]\n    )\n\n    # Run the kernel and measure execution time\n    execution_times = []\n    for i in range(num_iterations):\n        # Ensure all previous operations are complete\n        queue.finish()\n        \n        # Execute kernel and time it\n        start_time = time.time()\n        event = cl.enqueue_nd_range_kernel(queue, matmul_kernel, global_size, local_size)\n        event.wait()\n        queue.finish()\n        end_time = time.time()\n        \n        # Calculate elapsed time in milliseconds\n        elapsed_time = (end_time - start_time) * 1000\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n\n    # Copy the result back to host\n    cl.enqueue_copy(queue, C_host, C_buf)\n\n    # Verify results\n    is_correct, msg = verification_fn(A_host, B_host, C_host)\n\n    # Calculate statistics\n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n\n    # Return timing results\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"local_size\": local_size,\n        \"global_size\": global_size\n    }\n    \n    return timing_result",
  "timing_info": {
    "iterations": 10,
    "average_ms": 10.813021659851074,
    "min_ms": 10.471105575561523,
    "max_ms": 13.67950439453125,
    "all_times_ms": [
      13.67950439453125,
      10.489463806152344,
      10.474443435668945,
      10.65516471862793,
      10.473251342773438,
      10.471820831298828,
      10.471343994140625,
      10.471343994140625,
      10.472774505615234,
      10.471105575561523
    ],
    "correct_result": true,
    "verification_feedback": "Results match!",
    "local_size": [
      16,
      16
    ],
    "global_size": [
      2048,
      2048
    ]
  }
}