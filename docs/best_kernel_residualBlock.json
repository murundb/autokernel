{
  "task": "Write OpenCL kernel that implements a residual block as used in DNNs: given an input vector x of size 1024, two weight matrices W1 and W2 of size 1024x1024, and two bias vectors b1 and b2 of size 1024, compute result = ReLU(W2 * ReLU(W1 * x + b1) + b2 + x). The signature of the kernel should be __kernel void residualBlock(__global const float* restrict x, __global const float* restrict W1, __global const float* restrict b1, __global const float* restrict W2, __global const float* restrict b2, __global float* restrict result, const int dim).",
  "task_name": "residualBlock",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # Add the kernel code into a python string\n    opencl_code = \"\"\"\n    __kernel void residualBlock(\n        __global const float* restrict x,\n        __global const float* restrict W1,\n        __global const float* restrict b1,\n        __global const float* restrict W2,\n        __global const float* restrict b2,\n        __global float* restrict result,\n        const int dim)\n    {\n        // Use a tile size that balances local memory usage and reuse\n        #define TILE_SIZE 64\n        \n        // Local memory for tiles of input x and intermediate results\n        __local float x_shared[1024];\n        __local float intermediate[1024];\n        \n        // Thread identifiers\n        const int gid = get_global_id(0);\n        const int lid = get_local_id(0);\n        const int group_size = get_local_size(0);\n        const int group_id = get_group_id(0);\n        \n        // Prefetch x into shared memory with coalesced accesses\n        // Multiple threads cooperate to load x in chunks for better memory throughput\n        for (int i = lid; i < dim; i += group_size) {\n            x_shared[i] = x[i];\n        }\n        \n        // Ensure all threads have loaded their portion of x\n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // First matrix multiplication: compute intermediate = ReLU(W1 * x + b1)\n        // Each thread processes one or more rows based on workgroup size\n        for (int row = lid; row < dim; row += group_size) {\n            // Start with bias term\n            float sum = b1[row];\n            \n            // Process the row in tiles for better cache performance\n            for (int tile_start = 0; tile_start < dim; tile_start += TILE_SIZE) {\n                // Unroll by 16 within each tile for better instruction-level parallelism\n                // This reduces loop overhead and allows better instruction scheduling\n                int col = tile_start;\n                int end = min(tile_start + TILE_SIZE, dim);\n                \n                // Process 16 elements at a time when possible\n                for (; col <= end - 16; col += 16) {\n                    sum += W1[row * dim + col] * x_shared[col];\n                    sum += W1[row * dim + col + 1] * x_shared[col + 1];\n                    sum += W1[row * dim + col + 2] * x_shared[col + 2];\n                    sum += W1[row * dim + col + 3] * x_shared[col + 3];\n                    sum += W1[row * dim + col + 4] * x_shared[col + 4];\n                    sum += W1[row * dim + col + 5] * x_shared[col + 5];\n                    sum += W1[row * dim + col + 6] * x_shared[col + 6];\n                    sum += W1[row * dim + col + 7] * x_shared[col + 7];\n                    sum += W1[row * dim + col + 8] * x_shared[col + 8];\n                    sum += W1[row * dim + col + 9] * x_shared[col + 9];\n                    sum += W1[row * dim + col + 10] * x_shared[col + 10];\n                    sum += W1[row * dim + col + 11] * x_shared[col + 11];\n                    sum += W1[row * dim + col + 12] * x_shared[col + 12];\n                    sum += W1[row * dim + col + 13] * x_shared[col + 13];\n                    sum += W1[row * dim + col + 14] * x_shared[col + 14];\n                    sum += W1[row * dim + col + 15] * x_shared[col + 15];\n                }\n                \n                // Process 4 elements at a time for any remaining elements\n                for (; col <= end - 4; col += 4) {\n                    sum += W1[row * dim + col] * x_shared[col];\n                    sum += W1[row * dim + col + 1] * x_shared[col + 1];\n                    sum += W1[row * dim + col + 2] * x_shared[col + 2];\n                    sum += W1[row * dim + col + 3] * x_shared[col + 3];\n                }\n                \n                // Process any remaining elements individually\n                for (; col < end; col++) {\n                    sum += W1[row * dim + col] * x_shared[col];\n                }\n            }\n            \n            // Apply ReLU to the result and store in shared memory\n            intermediate[row] = max(0.0f, sum);\n        }\n        \n        // Ensure all intermediate results are computed before proceeding\n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // Second matrix multiplication only for threads that need to produce output\n        if (gid < dim) {\n            // Start with bias term\n            float sum = b2[gid];\n            \n            // Process in tiles for better cache performance\n            for (int tile_start = 0; tile_start < dim; tile_start += TILE_SIZE) {\n                // Unroll by 16 within each tile\n                int col = tile_start;\n                int end = min(tile_start + TILE_SIZE, dim);\n                \n                // Process 16 elements at a time when possible\n                for (; col <= end - 16; col += 16) {\n                    sum += W2[gid * dim + col] * intermediate[col];\n                    sum += W2[gid * dim + col + 1] * intermediate[col + 1];\n                    sum += W2[gid * dim + col + 2] * intermediate[col + 2];\n                    sum += W2[gid * dim + col + 3] * intermediate[col + 3];\n                    sum += W2[gid * dim + col + 4] * intermediate[col + 4];\n                    sum += W2[gid * dim + col + 5] * intermediate[col + 5];\n                    sum += W2[gid * dim + col + 6] * intermediate[col + 6];\n                    sum += W2[gid * dim + col + 7] * intermediate[col + 7];\n                    sum += W2[gid * dim + col + 8] * intermediate[col + 8];\n                    sum += W2[gid * dim + col + 9] * intermediate[col + 9];\n                    sum += W2[gid * dim + col + 10] * intermediate[col + 10];\n                    sum += W2[gid * dim + col + 11] * intermediate[col + 11];\n                    sum += W2[gid * dim + col + 12] * intermediate[col + 12];\n                    sum += W2[gid * dim + col + 13] * intermediate[col + 13];\n                    sum += W2[gid * dim + col + 14] * intermediate[col + 14];\n                    sum += W2[gid * dim + col + 15] * intermediate[col + 15];\n                }\n                \n                // Process 4 elements at a time for any remaining elements\n                for (; col <= end - 4; col += 4) {\n                    sum += W2[gid * dim + col] * intermediate[col];\n                    sum += W2[gid * dim + col + 1] * intermediate[col + 1];\n                    sum += W2[gid * dim + col + 2] * intermediate[col + 2];\n                    sum += W2[gid * dim + col + 3] * intermediate[col + 3];\n                }\n                \n                // Process any remaining elements individually\n                for (; col < end; col++) {\n                    sum += W2[gid * dim + col] * intermediate[col];\n                }\n            }\n            \n            // Add residual connection\n            sum += x_shared[gid];\n            \n            // Apply ReLU and write final result to global memory\n            result[gid] = max(0.0f, sum);\n        }\n    }\n    \"\"\"\n\n    # Set up dimensions and other parameters\n    dim = 1024\n    num_iterations = 10\n\n    # Function to verify results\n    def verification_fn():\n        # Copy result back to host\n        result_host = np.empty((dim,), dtype=np.float32)\n        cl.enqueue_copy(queue, result_host, result_buf)\n        \n        # Compute expected result on CPU\n        intermediate_cpu = np.maximum(0, np.dot(W1_host, x_host) + b1_host)  # ReLU(W1 * x + b1)\n        output_cpu = np.dot(W2_host, intermediate_cpu) + b2_host + x_host  # W2 * intermediate + b2 + x\n        expected = np.maximum(0, output_cpu)  # ReLU(output)\n        \n        # Check if results match\n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n\n    # Initialize data with random values\n    np.random.seed(42)\n    x_host = np.random.rand(dim).astype(np.float32)\n    W1_host = np.random.rand(dim, dim).astype(np.float32)\n    b1_host = np.random.rand(dim).astype(np.float32)\n    W2_host = np.random.rand(dim, dim).astype(np.float32)\n    b2_host = np.random.rand(dim).astype(np.float32)\n    \n    # Create OpenCL context, queue, and buffers\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    \n    platform = platforms[0]\n    devices = platform.get_devices(device_type=cl.device_type.GPU)\n    if not devices:\n        devices = platform.get_devices()\n    \n    device = devices[0]\n    print(f\"Using device: {device.name}\")\n    \n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n    \n    mf = cl.mem_flags\n    x_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=x_host)\n    W1_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=W1_host)\n    b1_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b1_host)\n    W2_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=W2_host)\n    b2_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b2_host)\n    result_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=dim * np.dtype(np.float32).itemsize)\n    \n    # Build the program and extract kernel\n    program = cl.Program(ctx, opencl_code).build()\n    kernel = program.residualBlock\n    \n    # Determine optimal work group size (use powers of 2 for NVIDIA GPUs)\n    # For this problem, 256 is often a good balance\n    local_size = 256\n    global_size = dim\n    \n    # Ensure global size is a multiple of local size\n    if global_size % local_size != 0:\n        global_size = ((global_size // local_size) + 1) * local_size\n    \n    # Run the kernel multiple times and collect timing information\n    execution_times = []\n    \n    # Warm-up run\n    kernel(queue, (global_size,), (local_size,), \n           x_buf, W1_buf, b1_buf, W2_buf, b2_buf, result_buf, np.int32(dim))\n    queue.finish()\n    \n    for i in range(num_iterations):\n        # Run the kernel and measure time\n        queue.finish()\n        start_time = time.time()\n        \n        kernel(queue, (global_size,), (local_size,), \n               x_buf, W1_buf, b1_buf, W2_buf, b2_buf, result_buf, np.int32(dim))\n        \n        queue.finish()\n        end_time = time.time()\n        \n        elapsed_time = (end_time - start_time) * 1000\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    # Calculate statistics\n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n    \n    # Verify results\n    is_correct, msg = verification_fn()\n    \n    # Prepare and return timing results\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"local_size\": local_size,\n        \"global_size\": global_size\n    }\n    \n    return timing_result",
  "timing_info": {
    "iterations": 10,
    "average_ms": 0.554656982421875,
    "min_ms": 0.5519390106201172,
    "max_ms": 0.5743503570556641,
    "all_times_ms": [
      0.5743503570556641,
      0.5538463592529297,
      0.553131103515625,
      0.5519390106201172,
      0.5521774291992188,
      0.5524158477783203,
      0.5519390106201172,
      0.5521774291992188,
      0.5524158477783203,
      0.5521774291992188
    ],
    "correct_result": true,
    "verification_feedback": "Results match!",
    "local_size": 256,
    "global_size": 1024
  }
}