{
  "task": "Write OpenCL kernel that implements a residual block as used in DNNs: given an input vector x of size 1024, two weight matrices W1 and W2 of size 1024x1024, and two bias vectors b1 and b2 of size 1024, compute result = ReLU(W2 * ReLU(W1 * x + b1) + b2 + x). The signature of the kernel should be __kernel void residualBlock(__global const float* restrict x, __global const float* restrict W1, __global const float* restrict b1, __global const float* restrict W2, __global const float* restrict b2, __global float* restrict result, const int dim).",
  "kernel_code": "def runner_setup():\n    import numpy as np\n    import pyopencl as cl\n    import time\n\n    # Kernel code\n    opencl_code = \"\"\"\n    __kernel void residualBlock(\n        __global const float* restrict x,\n        __global const float* restrict W1,\n        __global const float* restrict b1,\n        __global const float* restrict W2,\n        __global const float* restrict b2,\n        __global float* restrict result,\n        const int dim)\n    {\n        // Local memory for intermediate results\n        __local float relu_output[1024];\n        __local float x_local[1024];\n        \n        // Get indices\n        const int i = get_global_id(0);\n        const int lid = get_local_id(0);\n        \n        // Cache x in local memory\n        if (i < dim) {\n            x_local[lid] = x[i];\n        }\n        \n        // Ensure all work-items have read from global memory\n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // Compute W1 * x + b1 and apply ReLU\n        if (i < dim) {\n            float temp = b1[i];\n            const int row_offset = i * dim;\n            \n            // Matrix-vector multiplication W1 * x\n            // Unroll loop for better performance\n            int j = 0;\n            for (; j <= dim - 8; j += 8) {\n                temp += W1[row_offset + j] * x_local[j]\n                      + W1[row_offset + j + 1] * x_local[j + 1]\n                      + W1[row_offset + j + 2] * x_local[j + 2]\n                      + W1[row_offset + j + 3] * x_local[j + 3]\n                      + W1[row_offset + j + 4] * x_local[j + 4]\n                      + W1[row_offset + j + 5] * x_local[j + 5]\n                      + W1[row_offset + j + 6] * x_local[j + 6]\n                      + W1[row_offset + j + 7] * x_local[j + 7];\n            }\n            \n            // Process remaining elements\n            for (; j < dim; j++) {\n                temp += W1[row_offset + j] * x_local[j];\n            }\n            \n            // Apply ReLU and store in local memory\n            relu_output[lid] = max(temp, 0.0f);\n        }\n        \n        // Ensure all work-items have computed their portion of relu_output\n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // Compute W2 * relu_output + b2 + x and apply ReLU\n        if (i < dim) {\n            float temp2 = b2[i];\n            const int row_offset = i * dim;\n            \n            // Matrix-vector multiplication W2 * relu_output\n            // Unroll loop for better performance\n            int j = 0;\n            for (; j <= dim - 8; j += 8) {\n                temp2 += W2[row_offset + j] * relu_output[j]\n                       + W2[row_offset + j + 1] * relu_output[j + 1]\n                       + W2[row_offset + j + 2] * relu_output[j + 2]\n                       + W2[row_offset + j + 3] * relu_output[j + 3]\n                       + W2[row_offset + j + 4] * relu_output[j + 4]\n                       + W2[row_offset + j + 5] * relu_output[j + 5]\n                       + W2[row_offset + j + 6] * relu_output[j + 6]\n                       + W2[row_offset + j + 7] * relu_output[j + 7];\n            }\n            \n            // Process remaining elements\n            for (; j < dim; j++) {\n                temp2 += W2[row_offset + j] * relu_output[j];\n            }\n            \n            // Add residual connection and apply ReLU\n            result[i] = max(temp2 + x_local[lid], 0.0f);\n        }\n    }\n    \"\"\"\n\n    # Setup parameters\n    dim = 1024\n    num_iterations = 10\n    \n    # Initialize input data\n    x_host = np.random.rand(dim).astype(np.float32)\n    W1_host = np.random.rand(dim, dim).astype(np.float32)\n    b1_host = np.random.rand(dim).astype(np.float32)\n    W2_host = np.random.rand(dim, dim).astype(np.float32)\n    b2_host = np.random.rand(dim).astype(np.float32)\n    \n    # Create OpenCL context, command queue and buffers\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    \n    platform = platforms[0]\n    devices = platform.get_devices()\n    if not devices:\n        raise RuntimeError(\"No OpenCL devices found.\")\n    \n    device = devices[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    \n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n    \n    mf = cl.mem_flags\n    x_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=x_host)\n    W1_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=W1_host)\n    b1_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b1_host)\n    W2_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=W2_host)\n    b2_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=b2_host)\n    result_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=dim * np.dtype(np.float32).itemsize)\n    \n    # Compile the program and get kernel\n    program = cl.Program(ctx, opencl_code).build()\n    residual_block_kernel = program.residualBlock\n    \n    # Function to verify results\n    def verification_fn():\n        # Copy result back to host\n        result_host = np.empty(dim, dtype=np.float32)\n        cl.enqueue_copy(queue, result_host, result_buf)\n        \n        # Calculate expected result on CPU\n        temp1 = np.dot(W1_host, x_host) + b1_host\n        temp1 = np.maximum(temp1, 0)  # ReLU\n        temp2 = np.dot(W2_host, temp1) + b2_host + x_host\n        expected = np.maximum(temp2, 0)  # ReLU\n        \n        # Check if results match\n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n    \n    # Work group size - fits local memory constraints\n    local_size = 256\n    global_size = ((dim + local_size - 1) // local_size) * local_size\n    \n    # Benchmark the kernel\n    execution_times = []\n    for i in range(num_iterations):\n        # Set kernel arguments\n        args = (x_buf, W1_buf, b1_buf, W2_buf, b2_buf, result_buf, np.int32(dim))\n        \n        # Flush queues and measure execution time\n        queue.finish()\n        start_time = time.time()\n        \n        # Launch kernel\n        residual_block_kernel(queue, (global_size,), (local_size,), *args)\n        \n        # Wait for completion and record time\n        queue.finish()\n        end_time = time.time()\n        \n        elapsed_time = (end_time - start_time) * 1000  # Convert to ms\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    # Calculate statistics\n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n    \n    # Verify correctness\n    is_correct, msg = verification_fn()\n    \n    # Return timing results\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"block_size\": local_size,\n        \"grid_size\": global_size\n    }\n    \n    return timing_result",
  "timing_info": {
    "iterations": 10,
    "average_ms": 0.4789590835571289,
    "min_ms": 0.27251243591308594,
    "max_ms": 2.1240711212158203,
    "all_times_ms": [
      2.1240711212158203,
      0.31566619873046875,
      0.30159950256347656,
      0.2951622009277344,
      0.3001689910888672,
      0.28014183044433594,
      0.3261566162109375,
      0.27251243591308594,
      0.29087066650390625,
      0.28324127197265625
    ],
    "correct_result": false,
    "verification_feedback": "Results do NOT match!",
    "block_size": 256,
    "grid_size": 1024
  }
}