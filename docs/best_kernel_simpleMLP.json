{
  "task": "Write OpenCL kernel that implements a simple MLP: a linear layer (matrix-vector multiplication), followed by ReLU activation, followed by average pooling. The input is a vector of size 2048, the weight matrix is 2048x2048, and the output is a single float (the average of the ReLU outputs). The signature of the kernel should strictly be __kernel void simpleMLP(__global const float* restrict input, __global const float* restrict weights, __global float* restrict result, const int dim).",
  "task_name": "simpleMLP",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # Add the kernel code into a python string\n    opencl_code = \"\"\"\n    __kernel void simpleMLP(__global const float* restrict input,\n                         __global const float* restrict weights,\n                         __global float* restrict result,\n                         const int dim) {\n        // Get work dimensions and IDs\n        const int local_id = get_local_id(0);\n        const int local_size = get_local_size(0);\n        const int global_id = get_global_id(0);\n        const int global_size = get_global_size(0);\n        const int group_id = get_group_id(0);\n        \n        // Use local memory for input caching and reduction\n        __local float local_input[2048];\n        __local float local_sum[1024]; // Match max work group size of 1024\n        \n        // Cache the input vector in local memory collaboratively\n        for (int i = local_id; i < dim; i += local_size) {\n            local_input[i] = input[i];\n        }\n        \n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // Initialize private accumulator\n        float private_sum = 0.0f;\n        \n        // Each work-item processes multiple rows for better load distribution\n        for (int row = global_id; row < dim; row += global_size) {\n            // Process one matrix row\n            float dot_product = 0.0f;\n            \n            // Process columns in chunks of 4 for better vectorization on NVIDIA\n            int col = 0;\n            for (; col + 3 < dim; col += 4) {\n                float4 weight_chunk;\n                float4 input_chunk;\n                \n                // Load weights\n                const int row_offset = row * dim;\n                weight_chunk.x = weights[row_offset + col];\n                weight_chunk.y = weights[row_offset + col + 1];\n                weight_chunk.z = weights[row_offset + col + 2];\n                weight_chunk.w = weights[row_offset + col + 3];\n                \n                // Load inputs from local memory\n                input_chunk.x = local_input[col];\n                input_chunk.y = local_input[col + 1];\n                input_chunk.z = local_input[col + 2];\n                input_chunk.w = local_input[col + 3];\n                \n                // Manual dot product\n                dot_product += weight_chunk.x * input_chunk.x +\n                               weight_chunk.y * input_chunk.y +\n                               weight_chunk.z * input_chunk.z +\n                               weight_chunk.w * input_chunk.w;\n            }\n            \n            // Handle remaining elements\n            for (; col < dim; col++) {\n                dot_product += weights[row * dim + col] * local_input[col];\n            }\n            \n            // Apply ReLU activation\n            float activated = max(dot_product, 0.0f);\n            \n            // Add to private sum\n            private_sum += activated;\n        }\n        \n        // Store private sum in local memory\n        local_sum[local_id] = private_sum;\n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // Perform reduction using butterfly pattern\n        for (int stride = local_size >> 1; stride > 0; stride >>= 1) {\n            if (local_id < stride) {\n                local_sum[local_id] += local_sum[local_id + stride];\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Each work-group writes its partial sum to global memory\n        if (local_id == 0) {\n            result[group_id] = local_sum[0];\n        }\n    }\n    \n    __kernel void finalReduce(__global const float* restrict partial_sums,\n                          __global float* restrict result,\n                          const int num_groups,\n                          const int dim) {\n        const int lid = get_local_id(0);\n        const int local_size = get_local_size(0);\n        __local float local_sum[1024]; // Match max work group size\n        \n        // Initialize local sum\n        local_sum[lid] = 0.0f;\n        \n        // Load partial sums from global memory\n        for (int i = lid; i < num_groups; i += local_size) {\n            local_sum[lid] += partial_sums[i];\n        }\n        \n        barrier(CLK_LOCAL_MEM_FENCE);\n        \n        // Perform reduction\n        for (int stride = local_size >> 1; stride > 0; stride >>= 1) {\n            if (lid < stride) {\n                local_sum[lid] += local_sum[lid + stride];\n            }\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Write final result and compute average\n        if (lid == 0) {\n            result[0] = local_sum[0] / (float)dim;\n        }\n    }\n    \"\"\"\n    \n    # Parameters\n    dim = 2048\n    num_iterations = 5\n    \n    # Initialize data with random values\n    input_host = np.random.rand(dim).astype(np.float32)\n    weights_host = np.random.rand(dim, dim).astype(np.float32)\n    \n    # Create context, queue and program\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    \n    platform = platforms[0]\n    devices = platform.get_devices()\n    device = devices[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    \n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n    \n    # Create buffers\n    mf = cl.mem_flags\n    input_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=input_host)\n    weights_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=weights_host)\n    \n    # Set work sizes based on device capabilities\n    work_group_size = 256  # Good balance for compute-heavy kernels\n    max_compute_units = device.max_compute_units\n    num_groups = max_compute_units * 4  # Using multiple work-groups per compute unit\n    global_size = work_group_size * num_groups\n    \n    # Allocate buffers for partial results and final result\n    partial_results_buf = cl.Buffer(ctx, mf.READ_WRITE, size=num_groups * np.dtype(np.float32).itemsize)\n    result_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=np.dtype(np.float32).itemsize)\n    \n    # Build program\n    program = cl.Program(ctx, opencl_code).build()\n    mlp_kernel = program.simpleMLP\n    reduce_kernel = program.finalReduce\n    \n    # Function to calculate expected result for verification\n    def calculate_expected_result():\n        # Compute matrix-vector multiplication\n        dot_products = np.dot(weights_host, input_host)\n        # Apply ReLU\n        activated = np.maximum(dot_products, 0)\n        # Compute average\n        return np.mean(activated)\n    \n    # Function to verify the results\n    def verification_fn():\n        result_host = np.zeros(1, dtype=np.float32)\n        cl.enqueue_copy(queue, result_host, result_buf)\n        expected = calculate_expected_result()\n        ok = np.allclose(result_host[0], expected, rtol=1e-5, atol=1e-5)\n        return ok, f\"Results match: {result_host[0]} vs {expected}\" if ok else f\"Results do NOT match: {result_host[0]} vs {expected}\"\n    \n    execution_times = []\n    for i in range(num_iterations):\n        # Zero out buffers before each run\n        zeros = np.zeros(num_groups, dtype=np.float32)\n        cl.enqueue_copy(queue, partial_results_buf, zeros)\n        zero_result = np.zeros(1, dtype=np.float32)\n        cl.enqueue_copy(queue, result_buf, zero_result)\n        \n        queue.finish()\n        start_time = time.time()\n        \n        # First kernel: compute partial sums\n        mlp_kernel(queue, (global_size,), (work_group_size,), \n                  input_buf, weights_buf, partial_results_buf, np.int32(dim))\n        \n        # Second kernel: finalize sum and compute average\n        reduce_kernel(queue, (work_group_size,), (work_group_size,), \n                     partial_results_buf, result_buf, np.int32(num_groups), np.int32(dim))\n        \n        queue.finish()\n        end_time = time.time()\n        elapsed_time = (end_time - start_time) * 1000  # Convert to ms\n        \n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n    \n    is_correct, msg = verification_fn()\n    \n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"work_group_size\": work_group_size,\n        \"global_size\": global_size,\n        \"num_groups\": num_groups\n    }\n    return timing_result",
  "timing_info": {
    "iterations": 5,
    "average_ms": 0.9811878204345703,
    "min_ms": 0.6735324859619141,
    "max_ms": 2.2051334381103516,
    "all_times_ms": [
      2.2051334381103516,
      0.67901611328125,
      0.6742477416992188,
      0.6735324859619141,
      0.6740093231201172
    ],
    "correct_result": true,
    "verification_feedback": "Results match: 506.0168762207031 vs 506.01690673828125",
    "work_group_size": 256,
    "global_size": 47104,
    "num_groups": 184
  }
}