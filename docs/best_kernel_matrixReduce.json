{
  "task": "Write OpenCL kernel that reduces an 2048x2048 matrix to a single float by summing all its elements, optimized for Qualcomm Adreno architecture. The signature of the kernel should be __kernel void matrixReduce(__global const float* restrict A, __global float* restrict result, const int matrix_dim).",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # Add the kernel code into a python string\n    opencl_code = \"inline void atomic_add_global_float(__global float *addr, float val) {\\n    union {\\n        unsigned int u32;\\n        float f32;\\n    } next, expected, current;\\n    current.f32 = *addr;\\n    do {\\n        expected.f32 = current.f32;\\n        next.f32 = expected.f32 + val;\\n        current.u32 = atomic_cmpxchg((volatile __global unsigned int *)addr, expected.u32, next.u32);\\n    } while (current.u32 != expected.u32);\\n}\\n\\n__kernel void matrixReduce(__global const float* restrict A, \\n                         __global float* restrict result, \\n                         const int matrix_dim) {\\n    // Define local memory for reduction\\n    __local float localSum[256];\\n    \\n    // Get thread indices and dimensions\\n    const uint lid = get_local_id(0);  \\n    const uint gid = get_global_id(0);\\n    const uint localSize = get_local_size(0);\\n    const uint totalElements = matrix_dim * matrix_dim;\\n    const uint numWorkItems = get_global_size(0);\\n    \\n    // Initialize sum\\n    float sum = 0.0f;\\n    \\n    // Calculate chunk size and boundaries for this thread\\n    const uint itemsPerThread = (totalElements + numWorkItems - 1) / numWorkItems;\\n    const uint start = gid * itemsPerThread;\\n    const uint end = min(start + itemsPerThread, totalElements);\\n    \\n    // Accumulate values with unrolled loop for better performance\\n    uint i = start;\\n    float4 sum4 = (float4)(0.0f, 0.0f, 0.0f, 0.0f);\\n    \\n    // Process 4 elements at a time\\n    for (; i + 3 < end; i += 4) {\\n        sum4.x += A[i];\\n        sum4.y += A[i+1]; \\n        sum4.z += A[i+2];\\n        sum4.w += A[i+3];\\n    }\\n    \\n    // Accumulate the vector sum\\n    sum = sum4.x + sum4.y + sum4.z + sum4.w;\\n    \\n    // Handle remaining elements\\n    for (; i < end; i++) {\\n        sum += A[i];\\n    }\\n    \\n    // Store sum in local memory\\n    localSum[lid] = sum;\\n    \\n    // Wait for all threads to store their values\\n    barrier(CLK_LOCAL_MEM_FENCE);\\n    \\n    // Perform reduction in shared memory\\n    // Handle larger work group sizes\\n    if (localSize >= 512) {\\n        if (lid < 256) {\\n            localSum[lid] += localSum[lid + 256];\\n        }\\n        barrier(CLK_LOCAL_MEM_FENCE);\\n    }\\n    \\n    if (localSize >= 256) {\\n        if (lid < 128) {\\n            localSum[lid] += localSum[lid + 128];\\n        }\\n        barrier(CLK_LOCAL_MEM_FENCE);\\n    }\\n    \\n    if (localSize >= 128) {\\n        if (lid < 64) {\\n            localSum[lid] += localSum[lid + 64];\\n        }\\n        barrier(CLK_LOCAL_MEM_FENCE);\\n    }\\n    \\n    // For NVIDIA GPUs, threads in a warp are synchronized implicitly\\n    if (lid < 32) {\\n        // Use volatile to prevent compiler optimizations that would break warp synchronous programming\\n        volatile __local float* vLocalSum = localSum;\\n        \\n        if (localSize >= 64) vLocalSum[lid] += vLocalSum[lid + 32];\\n        if (localSize >= 32) vLocalSum[lid] += vLocalSum[lid + 16];\\n        if (localSize >= 16) vLocalSum[lid] += vLocalSum[lid + 8];\\n        if (localSize >= 8) vLocalSum[lid] += vLocalSum[lid + 4];\\n        if (localSize >= 4) vLocalSum[lid] += vLocalSum[lid + 2];\\n        if (localSize >= 2) vLocalSum[lid] += vLocalSum[lid + 1];\\n    }\\n    \\n    // First thread in the work group writes result using atomic add\\n    if (lid == 0) {\\n        atomic_add_global_float(result, localSum[0]);\\n    }\\n}\"\n\n    matrix_dim = 2048\n    num_iterations = 3\n\n    # Function to verify the results\n    def verification_fn():\n        result_host = np.zeros(1, dtype=np.float32)\n        cl.enqueue_copy(queue, result_host, result_buf)\n        expected = np.sum(A_host)\n        ok = np.isclose(result_host[0], expected, rtol=1e-5, atol=1e-5)\n        return ok, f\"Results match: {result_host[0]} vs expected {expected}\" if ok else f\"Results do NOT match: {result_host[0]} vs expected {expected}\"\n\n    # Initialize data with random values\n    A_host = np.random.rand(matrix_dim, matrix_dim).astype(np.float32)\n    \n    # Create OpenCL context, queue, etc.\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    platform = platforms[0]\n    device = platform.get_devices()[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n\n    # Create OpenCL buffers\n    mf = cl.mem_flags\n    A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A_host)\n    result_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=np.dtype(np.float32).itemsize)\n    \n    # Build the program\n    program = cl.Program(ctx, opencl_code).build()\n    \n    # Get kernel functions\n    matrix_reduce = program.matrixReduce\n    \n    # Define work group size (optimal for NVIDIA GPUs)\n    local_size = 256\n    global_size = ((matrix_dim * matrix_dim + local_size - 1) // local_size) * local_size\n    \n    execution_times = []\n    for i in range(num_iterations):\n        # Zero out result buffer\n        cl.enqueue_copy(queue, result_buf, np.zeros(1, dtype=np.float32))\n        \n        # Execute kernel\n        queue.finish()\n        start_time = time.time()\n        matrix_reduce(queue, (global_size,), (local_size,), A_buf, result_buf, np.int32(matrix_dim))\n        queue.finish()\n        end_time = time.time()\n        \n        elapsed_time = (end_time - start_time) * 1000\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n    \n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n\n    is_correct, msg = verification_fn()\n\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"local_size\": local_size,\n        \"global_size\": global_size\n    }\n    return timing_result",
  "timing_info": {
    "iterations": 3,
    "average_ms": 3.917137781778971,
    "min_ms": 3.221750259399414,
    "max_ms": 5.298852920532227,
    "all_times_ms": [
      5.298852920532227,
      3.221750259399414,
      3.2308101654052734
    ],
    "correct_result": "True",
    "verification_feedback": "Results match: 2095764.875 vs expected 2095764.0",
    "local_size": 256,
    "global_size": 4194304
  }
}