{
  "task": "Write OpenCL kernel that performs 4096x4096 matrix multiplication optimized for Qualcomm Adreno architecture. The signature of the kernel should be __kernel void matrixMultiply(__global const float* restrict A, __global const float* restrict B, __global float* restrict Result, const int matrix_dim).",
  "kernel_code": "def runner_setup():\n    import time\n    import numpy as np\n    import pyopencl as cl\n\n    # Add the kernel code into a python string\n    opencl_code = \"\"\"\n    __kernel void matrixMultiply(\n        __global const float* restrict A,\n        __global const float* restrict B,\n        __global float* restrict Result,\n        const int matrix_dim\n    ) {\n        // Tile size - 16x16 is a good compromise for Adreno\n        const int TILE_SIZE = 16;\n        \n        // Allocate local memory for tiles\n        __local float A_tile[TILE_SIZE][TILE_SIZE];\n        __local float B_tile[TILE_SIZE][TILE_SIZE];\n        \n        // Get work-item coordinates\n        const int row = get_global_id(0);\n        const int col = get_global_id(1);\n        const int local_row = get_local_id(0);\n        const int local_col = get_local_id(1);\n        \n        // Initialize accumulator\n        float sum = 0.0f;\n        \n        // Loop over tiles\n        const int num_tiles = (matrix_dim + TILE_SIZE - 1) / TILE_SIZE;\n        \n        for (int t = 0; t < num_tiles; t++) {\n            // Collaboratively load tiles into local memory\n            if (row < matrix_dim && (t * TILE_SIZE + local_col) < matrix_dim) {\n                A_tile[local_row][local_col] = A[row * matrix_dim + (t * TILE_SIZE + local_col)];\n            } else {\n                A_tile[local_row][local_col] = 0.0f;\n            }\n            \n            if ((t * TILE_SIZE + local_row) < matrix_dim && col < matrix_dim) {\n                B_tile[local_row][local_col] = B[(t * TILE_SIZE + local_row) * matrix_dim + col];\n            } else {\n                B_tile[local_row][local_col] = 0.0f;\n            }\n            \n            // Ensure all loads are complete\n            barrier(CLK_LOCAL_MEM_FENCE);\n            \n            // Compute dot product for this tile\n            for (int k = 0; k < TILE_SIZE; k++) {\n                sum += A_tile[local_row][k] * B_tile[k][local_col];\n            }\n            \n            // Ensure computation is complete before loading next tile\n            barrier(CLK_LOCAL_MEM_FENCE);\n        }\n        \n        // Write result to global memory\n        if (row < matrix_dim && col < matrix_dim) {\n            Result[row * matrix_dim + col] = sum;\n        }\n    }\n    \"\"\"\n\n    matrix_dim = 4096\n    num_iterations = 3\n\n    # Function to verify the results\n    def verification_fn():\n        result_host = np.empty((matrix_dim, matrix_dim), dtype=np.float32)\n        cl.enqueue_copy(queue, result_host, result_buf)\n        expected = np.dot(A_host, B_host)\n        ok = np.allclose(result_host, expected, rtol=1e-5, atol=1e-5)\n        return ok, \"Results match!\" if ok else \"Results do NOT match!\"\n\n    # Initialize data with random values\n    A_host = np.random.rand(matrix_dim, matrix_dim).astype(np.float32)\n    B_host = np.random.rand(matrix_dim, matrix_dim).astype(np.float32)\n    \n    # Create OpenCL buffers\n    mf = cl.mem_flags\n    platforms = cl.get_platforms()\n    if not platforms:\n        raise RuntimeError(\"No OpenCL platforms found.\")\n    platform = platforms[0]\n    device = platform.get_devices()[0]\n    print(f\"Using OpenCL device: {device.name}\")\n    ctx = cl.Context([device])\n    queue = cl.CommandQueue(ctx)\n\n    A_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=A_host)\n    B_buf = cl.Buffer(ctx, mf.READ_ONLY | mf.COPY_HOST_PTR, hostbuf=B_host)\n    result_buf = cl.Buffer(ctx, mf.WRITE_ONLY, size=matrix_dim * matrix_dim * np.dtype(np.float32).itemsize)\n    \n    # Build the kernel\n    program = cl.Program(ctx, opencl_code).build()\n    matrixMultiply = getattr(program, \"matrixMultiply\")\n\n    # Set local and global work sizes\n    local_size = (16, 16)  # 16x16 work group\n    global_size = (matrix_dim, matrix_dim)  # Cover entire matrix\n\n    execution_times = []\n    for i in range(num_iterations):\n        # Prepare kernel arguments\n        args = (A_buf, B_buf, result_buf, np.int32(matrix_dim))\n        \n        # Flush queue before timing\n        queue.finish()\n        start_time = time.time()\n        \n        # Launch kernel\n        matrixMultiply(queue, global_size, local_size, *args)\n        \n        # Wait for completion before timing\n        queue.finish()\n        end_time = time.time()\n        \n        elapsed_time = (end_time - start_time) * 1000\n        execution_times.append(elapsed_time)\n        print(f\"Iteration {i+1}: {elapsed_time:.2f} ms\")\n\n    avg_time = sum(execution_times) / len(execution_times)\n    min_time = min(execution_times)\n    max_time = max(execution_times)\n\n    is_correct, msg = verification_fn()\n\n    timing_result = {\n        \"iterations\": num_iterations,\n        \"average_ms\": avg_time,\n        \"min_ms\": min_time,\n        \"max_ms\": max_time,\n        \"all_times_ms\": execution_times,\n        \"correct_result\": is_correct,\n        \"verification_feedback\": msg,\n        \"local_size\": local_size,\n        \"global_size\": global_size\n    }\n    return timing_result\n",
  "timing_info": {
    "iterations": 3,
    "average_ms": 1759.8958015441895,
    "min_ms": 1737.4520301818848,
    "max_ms": 1800.8043766021729,
    "all_times_ms": [
      1800.8043766021729,
      1737.4520301818848,
      1741.4309978485107
    ],
    "correct_result": true,
    "verification_feedback": "Results match!",
    "local_size": [
      16,
      16
    ],
    "global_size": [
      4096,
      4096
    ]
  }
}